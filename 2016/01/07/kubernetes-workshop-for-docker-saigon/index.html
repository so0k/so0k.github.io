<!DOCTYPE html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]> <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]> <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>Kubernetes Workshop for Docker Saigon  &middot; Guides and Notes by Vincent</title>
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1">


<meta name="description" content="The first draft of my upcoming article on &#34;How To Provision a Kubernetes Cluster Using CoreOS&#34; is ready. This will be used as a guideline for a Kubernetes workshop I will lead for the http://docker-saigon.github.io community" />

<meta name="keywords" content="Kubernetes, Workshop, Docker, ">


<meta property="og:title" content="Kubernetes Workshop for Docker Saigon  &middot; Guides and Notes by Vincent ">
<meta property="og:site_name" content="Guides and Notes by Vincent"/>
<meta property="og:url" content="http://so0k.github.io/2016/01/07/kubernetes-workshop-for-docker-saigon/" />
<meta property="og:locale" content="en-us">


<meta property="og:type" content="article" />
<meta property="og:description" content="The first draft of my upcoming article on &#34;How To Provision a Kubernetes Cluster Using CoreOS&#34; is ready. This will be used as a guideline for a Kubernetes workshop I will lead for the http://docker-saigon.github.io community"/>
<meta property="og:article:published_time" content="2016-01-07T12:27:33&#43;07:00" />
<meta property="og:article:modified_time" content="2016-01-07T12:27:33&#43;07:00" />

  
    
<meta property="og:article:tag" content="Kubernetes">
    
<meta property="og:article:tag" content="Workshop">
    
<meta property="og:article:tag" content="Docker">
    
  

  
<meta name="twitter:card" content="summary" />
<meta name="twitter:site" content="@vincentdesmet" />
<meta name="twitter:creator" content="@vincentdesmet" />
<meta name="twitter:title" content="Kubernetes Workshop for Docker Saigon" />
<meta name="twitter:description" content="The first draft of my upcoming article on &#34;How To Provision a Kubernetes Cluster Using CoreOS&#34; is ready. This will be used as a guideline for a Kubernetes workshop I will lead for the http://docker-saigon.github.io community" />
<meta name="twitter:url" content="http://so0k.github.io/2016/01/07/kubernetes-workshop-for-docker-saigon/" />
<meta name="twitter:domain" content="http://so0k.github.io/">
  

<script type="application/ld+json">
  {
    "@context": "http://schema.org",
    "@type": "Article",
    "headline": "Kubernetes Workshop for Docker Saigon",
    "author": {
      "@type": "Person",
      "name": "http://profiles.google.com/+?rel=author"
    },
    "datePublished": "2016-01-07",
    "description": "The first draft of my upcoming article on \x22How To Provision a Kubernetes Cluster Using CoreOS\x22 is ready. This will be used as a guideline for a Kubernetes workshop I will lead for the http:\/\/docker-saigon.github.io community",
    "wordCount":  23223 
  }
</script>



<link rel="canonical" href="http://so0k.github.io/2016/01/07/kubernetes-workshop-for-docker-saigon/" />

<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://so0k.github.io/touch-icon-144-precomposed.png">
<link href="http://so0k.github.io/favicon.png" rel="icon">

<meta name="generator" content="Hugo 0.15" />

  <!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
<script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->



    <link rel="stylesheet" href="http://so0k.github.io/css/theme/default.css">


<link rel="stylesheet" href="http://so0k.github.io/css/font-awesome.min.css">
<link rel="stylesheet" href="http://so0k.github.io/css/style.css">


  <link rel="stylesheet" href="http://so0k.github.io/css/highlight/default.css">


</head>
<body>
    <header id="main-header">
  <nav class="navbar navbar-default navbar-fixed-top">
    <div class="container">
      <div class="navbar-header">
        <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
        </button>
        <a class="navbar-brand" href="http://so0k.github.io/">Guides and Notes by Vincent</a>
      </div>
      <div id="navbar" class="collapse navbar-collapse">
        <ul class="nav navbar-nav navbar-right">
          
          <li>

            <a href="http://so0k.github.io/page/about"
            >

            
                <i class="fa fa-info-circle"></i>
            
            
            About Me
            </a>
          </li>
          
          <li>

            <a href="http://so0k.github.io/post"
             title="Show list of posts">

            
                <i class="fa fa-files-o"></i>
            
            
            Posts
            </a>
          </li>
          
          <li>

            <a href="http://so0k.github.io/tags"
             title="Show list of tags">

            
                <i class="fa fa-tags"></i>
            
            
            Tags
            </a>
          </li>
          
        </ul>
      </div>
      
    </div>
  </nav>

</header>


<div class="container">
<div class="row">

  <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
  <div class="text-center">

    <h1>Kubernetes Workshop for Docker Saigon
</h1>

    <div class="metas">
<small>
  <i class="fa fa-calendar"></i>
  <time datetime="2016-01-07">7 Jan, 2016</time>
</small>


  <small>
    &middot; by Vincent De Smet
  
  &middot; Read in about 110 min
  &middot; (23223 Words)
</small>


<div class="margin-10">
  <i class="fa fa-tags"></i>
  
  <a href="http://so0k.github.io/tags/kubernetes" class="label label-primary">Kubernetes</a>
  
  <a href="http://so0k.github.io/tags/docker" class="label label-primary">Docker</a>
  
  <a href="http://so0k.github.io/tags/digitalocean" class="label label-primary">DigitalOcean</a>
  
  <a href="http://so0k.github.io/tags/docker-saigon" class="label label-primary">Docker-Saigon</a>
  


</div>
<br>
</div>

  </div>
</div>

  <div class="container content">
  

<p>This is the first draft of a guide I wrote to set up Kubernetes with CoreOS on DigitalOcean. To battle test this guide, I&rsquo;m holding a workshop for the <a href="http://docker-saigon.github.io">Docker-Saigon</a> Meetup Group. During the workshop, I will point users to this web page to follow along with the instructions.</p>

<p>Attendees will need to have:</p>

<ul>
<li>A laptop (Linux/OSX/Windows should all be fine)</li>
<li>A console ready to go with access to <code>git</code> &amp; <code>openssh</code></li>
<li>A <a href="https://www.digitalocean.com/?refcode=d6a3f4aecbdf">Digital Ocean</a> account ready (if you do not have an account, feel free to sign up with <a href="https://www.digitalocean.com/?refcode=d6a3f4aecbdf">my referral link</a>)</li>
<li>Decent Docker knowledge</li>
<li>Some CoreOS / Systemd knowledge</li>
</ul>

<p>This guide was written to be processed by the Digital Ocean Markdown parser, as such - Code Highlights may not render as expected. Refer to: <a href="http://do.co/formatting">http://do.co/formatting</a></p>

<h1 id="how-to-provision-a-kubernetes-cluster-using-coreos:a91459261407d5d36808cf519d4f7594">How To Provision a Kubernetes Cluster Using CoreOS</h1>

<h3 id="introduction:a91459261407d5d36808cf519d4f7594">Introduction</h3>

<p>Kubernetes is a system designed internally within Google to manage applications built within containers across a cluster of nodes. It handles the entire life cycle of a containerized application including deployment and scaling.</p>

<p>With AWS, Red Hat, Microsoft, IBM, Mirantis OpenStack, and VMware (and the list keeps growing) working to integrate Kubernetes into their platforms, going through this tutorial will provide you with a strong foundation and fundamental understanding of a framework that is here to stay.</p>

<p>In this tutorial, we will give step-by-step instructions on how to create a single-controller/multi-worker Kubernetes cluster on CoreOS hosted by Digital Ocean. This system will allow us to group related services together for deployment as a unit on a single host using what Kubernetes calls &laquo;<a href="https://coreos.com/kubernetes/docs/latest/pods.html">Pods</a>&raquo;. Kubernetes also provides health checking functionality, high availability, and efficient usage of resources through schedulers.</p>

<p>This tutorial was tested with Kubernetes v1.1.2. Keep in mind that this software changes frequently. To see your version, once it&rsquo;s installed, run:</p>

<pre><code class="language-command">kubectl version
</code></pre>

<h3 id="prerequisites-and-goals:a91459261407d5d36808cf519d4f7594">Prerequisites and goals</h3>

<p>We will provision each component of our Kubernetes cluster as part of this tutorial, no existing architecture is required. Experience with Docker is expected, experience with Systemd and CoreOS is a plus, but each concept is introduced and explained as part of this tutorial. If you are not familiar with CoreOS, it may be helpful to review <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-coreos-system-components">some basic information about the CoreOS system</a>.</p>

<p>After a high-level overview of the Kubernetes Architecture, we will configure our client machine to work with our Digital Ocean resources from the terminal using Doctl &amp; Jq. Once this is done we will be able to quickly and repeatedly provision our droplets with <code>cloud-config</code> files. This allows us to declaratively customize network configuration, Systemd units, and other OS-level items.</p>

<p>We will first provision an Etcd cluster to reliably provide storage of meta data across a cluster of machines. Etcd provides a great way to store configuration data reliably for Kubernetes. Thanks to the watch support provided by Etcd, coordinating components can be notified very quickly of changes. This component is crucial to our Kubernetes cluster.</p>

<p>With the help of our Etcd cluster, we will also configure <a href="https://coreos.com/flannel/docs/latest/flannel-config.html">Flannel</a>, a network fabric layer that provides each machine with an individual subnet for container communication. This satisfies a fundamental requirement for running a Kubernetes cluster. Docker will be configured to use this networking layer for its containers.</p>

<p>We will provision our Kubernetes controller Droplet and to ensure the security of our Kubernetes cluster, we will generate the required certificates for communication between Kubernetes components using <code>openssl</code> &amp; securely transfer these using <code>scp</code> to each Droplet. We will configure the command line client utility, <code>kubectl</code>, to work with our cluster from our client next.</p>

<p>Finally, we will provision worker nodes pointing to the controller nodes and deploy the internal cluster DNS through the DNS add-on. We will have a fully functional Kubernetes cluster allowing us to deploy our workloads and easily add worker nodes as required with the <code>cloud-config</code> files defined in this tutorial.</p>

<p>Working through this tutorial may take you a few hours, but it will give you a good understanding of the moving pieces of your cluster and set you up for success in the long run.</p>

<p>The structure and idea for this tutorial was taken from the <a href="https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/getting-started.md">Getting started with CoreOS and Kubernetes Guide</a> and updated with detailed step by step instructions for Digital Ocean. Let&rsquo;s get started.</p>

<h2 id="kubernetes-architectural-overview:a91459261407d5d36808cf519d4f7594">Kubernetes Architectural Overview</h2>

<p>In this section we will give an overview of the Kubernetes Architecture. For a more detailed look, refer to <a href="http://kubernetes.io/v1.1/docs/design/architecture.html">the official Kubernetes documentation</a>.</p>

<p>At a high level, we need to differentiate the services that run on every node, referred to as node agents (<code>kubelet</code>, &hellip; ), the controller services (APIs, scheduler, &hellip;) that compose the cluster-level control plane and the distributed storage solution (Etcd).</p>

<p>A crucial component on every node is the <strong>kubelet</strong>. The kubelet is responsible for what&rsquo;s running on each individual Droplet and making sure it keeps running. The kubelet controls the container runtime, in this tutorial <strong>Docker</strong> provides the container runtime and must also run on each node. Docker takes care of the details of downloading images and running containers. The kubelet registers nodes with the cluster, sends events and status updates and reports the resource utilization of the node.</p>

<p>To facilitate routing between containers as well as simplify service discovery, each node also runs the <strong>kube-proxy</strong>. The proxy is a simple network proxy and load balancer which can do simple TCP and UDP stream forwarding (round robin) across a set of back ends. The proxy is a crucial part for the Kubernetes services model. The proxy communicates with the controller services to keep up to date. See the <a href="https://github.com/kubernetes/kubernetes/wiki/Services-FAQ">Kubernetes&rsquo; services FAQ</a> for more details.</p>

<p>Worker node services are configured to be managed from the controller services. <!-- this is meant to highlight the difference between the same service deployed on worker nodes vs controller nodes. On worker nodes, the services register with the controller nodes, on controller nodes they are often bootstrapped... --></p>

<p>The first controller service we will highlight is the <strong>API server</strong>. The API server serves up the <a href="http://kubernetes.io/v1.1/docs/api.html">Kubernetes API</a> through a REST interface. It is intended to be a CRUD-y service, with most/all business logic implemented in separate components or in plug-ins. It&rsquo;s responsible for validating the requests and updating the corresponding objects in Etcd. The API server is void of state and will be the main component replicated and load balanced across controller nodes in a High Availability configuration.</p>

<p>The second controller service to highlight is the <strong>scheduler</strong>. The scheduler is responsible for assigning workloads to nodes in the cluster. This component uses the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_binding">binding</a> API to achieve this. The scheduler is pluggable and support for multiple cluster schedulers and even user-provided schedulers is expected, but not available yet when this tutorial was written.</p>

<p>All other cluster-level functions are performed by the <strong>controller manager</strong> component at the time of writing. This component embeds the core control loops shipped with Kubernetes. Each controller is a control loop that watches the shared state of the cluster through the API Server and makes changes attempting to move the current state towards the desired state. These controllers may eventually be split into separate components in future Kubernetes versions to make them independently pluggable.</p>

<p>As the scheduler and controller manager components modify cluster state, only one instance of each can run within the cluster. In High Availability configurations a process of master election is required for these components. We will explain and apply master election for these components as part of this tutorial, we will however only provision 1 controller node and no control plane load balancer. Setting up the control plane load balancers and appropriate TLS artifacts are left as an exercise for the reader.</p>

<p>Below is a high level diagram of these Kubernetes components in a High Availability set-up.</p>

<!-- created with http://asciiflow.com/ -->

<p><strong>Note:</strong> This is a temporary diagram which will be replaced with Digital Ocean Assets</p>

<pre><code> Etcd                             Controller Nodes                                           Worker Nodes
                                    +--------------------+                                     +--------------------+
                                    |                    |                                     |                    |
+--------------------+          +---+ API Server         &lt;---------+  +------------------------+ Kubelet            |
|                    |          |   |                    |         |  |                        |                    |
| Etcd  cluster      &lt;----------+   | Controller Manager*|         |  |                        | Docker             |
|                    |              |                    |         |  |                        |                    |
|                    |              | Scheduler*         |         |  |                        | Proxy              |
|                    |              |                    |         |  |                        |                    |
|                    |              | Kubelet            |         |  |                        |                    |
|                    |              |                    |         |  |                        |                    |
|                    |              | Docker             |         |  |                        |                    |
|                    |              |                    |       +-+--v---------------+        |                    |
|                    |              | Proxy              |       |                    |        |                    |
|                    |              |                    |       | Control Plane      |        |                    |
|                    |              +--------------------+       |                    |        +--------------------+
|                    |                                           | Load Balancer      |
+-^--^---------------+              +--------------------+       |                    |        +--------------------+
  |  |                              |                    |       |                    |        |                    |
  |  +------------------------------+ API Server         &lt;-------+                    &lt;--------+ Kubelet            |
  |                                 |                    |       |                    |        |                    |
  |                                 | Kubelet            |       |                    |        | Docker             |
  |                                 |                    |       |                    |        |                    |
  |                                 | Docker             |       |                    |        | Proxy              |
  |                                 |                    |       |                    |        |                    |
  |                                 | Proxy              |       |                    |        |                    |
  |                                 |                    |       +-+--^---------------+        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 +--------------------+         |  |                        +--------------------+
  |                                                                |  |
  |                                 +--------------------+         |  |                        +--------------------+
  |                                 |                    |         |  |                        |                    |
  +---------------------------------+ API Server         &lt;---------+  +------------------------+ Kubelet            |
                                    |                    |                                     |                    |
                                    | Kubelet            |                                     | Docker             |
                                    |                    |                                     |                    |
                                    | Docker             |                                     | Proxy              |
                                    |                    |                                     |                    |
                                    | Proxy              |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    +--------------------+                                     +--------------------+

</code></pre>

<p>Refer to the official diagram for a more detailed break down: <a href="http://kubernetes.io/v1.1/docs/design/architecture.png?raw=true">http://kubernetes.io/v1.1/docs/design/architecture.png?raw=true</a></p>

<h2 id="step-1-configuring-our-client-machine:a91459261407d5d36808cf519d4f7594">Step 1 — Configuring Our Client Machine.</h2>

<p>As the first step in this tutorial we will ensure our client machine is correctly configured to complete all subsequent steps.</p>

<p>The default folder for storing Kubernetes cluster certificates and config-related files is <code>$HOME/.kube/</code>. For this tutorial, we will store our cluster configuration and certificates in this folder, ensure the folder exists:</p>

<pre><code class="language-command">mkdir ~/.kube
</code></pre>

<p>We will be using the Digital Ocean Control TooL (<a href="https://github.com/digitalocean/doctl">Doctl</a>) as well as the Command-line JSON processor <a href="https://github.com/stedolan/jq">Jq</a> to manage our Digital Ocean resources from our terminal. This will allow us to quickly repeat commands and automate our Kubernetes cluster setup further down the line.</p>

<p>We will set up Doctl and Jq as well as introduce the basics on how to use these tools within this step.</p>

<p>At the end of this step a correctly configured client environment is expected, if you skip this step ensure first that you have:</p>

<ol>
<li>Configured your environment to create and destroy Droplets in a single Digital Ocean region from the terminal. Ensure you set the <code>$region</code> and <code>$DIGITALOCEAN_API_KEY</code> variables for the rest of this tutorial.</li>
<li>Created the SSH key for all Droplets in our cluster. Ensure the private key is loaded with your SSH agent and the public key is stored as a Digital Ocean resource named <code>k8s-key</code>.</li>
</ol>

<p>Follow the sub-steps to achieve this.</p>

<h3 id="setting-up-doctl:a91459261407d5d36808cf519d4f7594">Setting up Doctl</h3>

<p>To use Doctl from your terminal and follow the Kubernetes cluster config from this tutorial, you will need to generate a Personal Access Token with write permissions through the Digital Ocean Control Panel. Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-api-v2#how-to-generate-a-personal-access-token">this tutorial</a> for information on how to do this, continue with these steps once you have your Personal Access Token ready.</p>

<p>For all of the steps in this tutorial, we will assign our token to a variable called <code>DIGITALOCEAN_API_KEY</code>. For example, by running the following command in bash (replace the highlighted text with your own token):</p>

<pre><code class="language-command">export DIGITALOCEAN_API_KEY=77e027c7447f468068a7d4fea41e7149a75a94088082c66fcf555de3977f69d3
</code></pre>

<p>Review <a href="https://github.com/digitalocean/doctl/releases/latest">the latest Doctl release</a> and choose the right binary archive for your environment:</p>

<table>
<thead>
<tr>
<th>Operating System</th>
<th>Binary</th>
</tr>
</thead>

<tbody>
<tr>
<td>OSX</td>
<td>darwin-amd64-doctl.tar.bz2</td>
</tr>

<tr>
<td>Linux</td>
<td>linux-amd64-doctl.tar.bz2</td>
</tr>

<tr>
<td>Windows</td>
<td>windows-amd-64-doctl.zip</td>
</tr>
</tbody>
</table>

<p>For example, to download the archive for the <code>0.0.16</code> release (used in this tutorial) to your home directory on a Linux 64-bit host, run the following commands in your terminal:</p>

<pre><code class="language-command">curl -Lo ~/doctl.tar.bz2 https://github.com/digitalocean/doctl/releases/download/0.0.16/linux-amd64-doctl.tar.bz2
</code></pre>

<p>Next, we need to extract the downloaded archive. We will also need to add <code>doctl</code> to a location included in our <code>PATH</code> environment variable, <code>/usr/bin</code> or <code>/opt/bin</code> for example. The following command will extract <code>doctl</code> directly to <code>/usr/bin</code> making it available for all users on a Linux host. This command requires <code>sudo</code> rights:</p>

<pre><code class="language-super_user">tar xjf doctl.tar.bz2 -C /usr/bin
</code></pre>

<p>Finally, validate that <code>doctl</code> has been downloaded successfully by confirming the installed version:</p>

<pre><code class="language-command">doctl --version
</code></pre>

<p>If you followed the steps above, this should return:</p>

<pre><code>[secondary_label Output]
doctl version 0.0.16
</code></pre>

<h3 id="finding-help-about-doctl:a91459261407d5d36808cf519d4f7594">Finding help about Doctl</h3>

<p>An overview of Doctl and several usage examples are available on <a href="https://github.com/digitalocean/doctl">the Doctl GitHub repository</a>. Additionally, invoking Doctl without any arguments will print out usage instructions as well. Note that every Digital Ocean resource type has a corresponding Doctl command. Every command has subcommands to manage the resource as well as instructions available through the <code>help</code> subcommand or the <code>--help</code> flag.</p>

<p>For example, to review the available commands for <code>droplet</code> resources, run:</p>

<pre><code class="language-command">doctl droplet help
</code></pre>

<p>This should return:</p>

<pre><code>[secondary_label Output]
NAME:
   doctl droplet - Droplet commands. Lists by default.

USAGE:
   doctl droplet [global options] command [command options] [arguments...]

VERSION:
   0.0.16

COMMANDS:
   create, c            Create droplet.
   list, l              List droplets.
   find, f              &lt;Droplet name&gt; Find the first Droplet whose name matches the first argument.
   destroy, d           [--id | &lt;name&gt;] Destroy droplet.
   reboot               [--id | &lt;name&gt;] Reboot droplet.
   power_cycle          [--id | &lt;name&gt;] Powercycle droplet.
   shutdown             [--id | &lt;name&gt;] Shutdown droplet.
   poweroff, off        [--id | &lt;name&gt;] Power off droplet.
   poweron, on          [--id | &lt;name&gt;] Power on droplet.
   password_reset       [--id | &lt;name&gt;] Reset password for droplet.
   resize               [--id | &lt;name&gt;] Resize droplet.
   help, h              Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --help, -h   show help
</code></pre>

<p>Notice that the <code>droplet</code> command will <code>list</code> droplets by default.</p>

<p>To get more information about the <code>droplet</code> <code>create</code> command, run:</p>

<pre><code class="language-command">doctl droplet create --help
</code></pre>

<p>This should return:</p>

<pre><code>[secondary_label Output]
NAME:
   create - Create droplet.

USAGE:
   command create [command options] [arguments...]

OPTIONS:
   --domain, -d                         Domain name to append to the hostname. (e.g. server01.example.com)
   --add-region                         Append region to hostname. (e.g. server01.sfo1)
   --user-data, -u                      User data for creating server.
   --user-data-file, --uf               A path to a file for user data.
   --ssh-keys, -k                       Comma seperated list of SSH Key names. (e.g. --ssh-keys Work,Home)
   --size, -s &quot;512mb&quot;                   Size of Droplet.
   --region, -r &quot;nyc3&quot;                  Region of Droplet.
   --image, -i &quot;ubuntu-14-04-x64&quot;       Image slug of Droplet.
   --backups, -b                        Turn on backups.
   --ipv6, -6                           Turn on IPv6 networking.
   --private-networking, -p             Turn on private networking.
   --wait-for-active                    Don't return until the create has succeeded or failed.
</code></pre>

<h3 id="setting-up-your-ssh-keys-with-doctl:a91459261407d5d36808cf519d4f7594">Setting up your SSH Keys with Doctl</h3>

<p>Every CoreOS droplet that you will provision for your Kubernetes cluster, will need to have at least one SSH public key installed during its creation process. The key(s) will be installed to the <code>core</code> user&rsquo;s authorized keys file, and you will need the corresponding private key(s) to log in to your CoreOS server.</p>

<p>If you do not already have any SSH keys associated with your Digital Ocean account, do so now by following steps one and two of this tutorial: <a href="https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets">How To Use SSH Keys with Digital Ocean Droplets</a>. You may opt to use Doctl to add the new SSH keys to your account rather then copying the SSH Keys into the Digital Ocean control panel manually. This can be achieved by passing in your <code>DIGITALOCEAN_API_KEY</code> environment variable as the <code>--api-key</code> to Doctl and adding the public key of your newly created SSH key with the following command:</p>

<pre><code class="language-command">doctl --api-key $DIGITALOCEAN_API_KEY keys create &lt;key-name&gt; &lt;path-to-public-key&gt;
</code></pre>

<blockquote>
<p><strong>Note</strong>: Doctl will automatically try to use the <code>$DITIGALOCEAN_API_KEY</code> env variable as the <code>--api-key</code> if it exists and we do not need to explicitly pass it in every time. We will omit this in future Doctl commands.</p>
</blockquote>

<p>Add your private key to your SSH agent on your client machine, using <code>ssh-agent</code> as follows:</p>

<pre><code class="language-command">ssh-add &lt;path-to-private-key&gt;
</code></pre>

<p>or use the <code>-i &lt;path-to-private-key&gt;</code> flag each time connecting to your droplets over <code>ssh</code> if you do not have a running <code>ssh-agent</code>.</p>

<p>For example, in this tutorial we will store our key pair in our home directory as <code>~/.ssh/id_rsa</code> and upload the public key as <code>k8s-key</code> to our Digital Ocean account, combining all the above steps together, would look like this:</p>

<pre><code>[secondary_label Output]
# Generate the key pair
ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/demo/.ssh/id_rsa):
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/demo/.ssh/id_rsa.
Your public key has been saved in /home/demo/.ssh/id_rsa.pub.
The key fingerprint is:
4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@a
The key's randomart image is:
+--[ RSA 2048]----+
|          .oo.   |
|         .  o.E  |
|        + .  o   |
|     . = = .     |
|      = S = .    |
|     o + = +     |
|      . o + o .  |
|           . o   |
|                 |
+-----------------+
# Upload public key to Digital Ocean account as k8s-key
doctl keys create k8s-key /home/demo/.ssh/id_rsa.pub
# Add private key to SSH Agent
ssh-add ~/.ssh/id_rsa
</code></pre>

<h3 id="managing-droplets-with-doctl:a91459261407d5d36808cf519d4f7594">Managing Droplets with Doctl</h3>

<p>To verify that your account &amp; keys are setup correctly, we will create a new CoreOS Alpha droplet named <code>&quot;do-test&quot;</code> from the terminal.</p>

<!-- note that this will cause a charge? -->

<p>For the remainder of this tutorial, we will be creating all droplets within the same Digital Ocean region. Choose your region and store it into a variable called <code>$region</code>. Review the list of all available regions by running the <code>doctl region</code> command first.</p>

<pre><code class="language-command">doctl region
</code></pre>

<p>For example, we will use the Amsterdam 2 region for the rest of this tutorial. Choose the region most appropriate for your case:</p>

<pre><code class="language-command">export region=&quot;ams2&quot;
</code></pre>

<p>Now create the Droplet with the following command:</p>

<pre><code class="language-command">doctl droplet create \
	--image &quot;coreos-alpha&quot; \
	--size &quot;512mb&quot; \
	--region &quot;$region&quot; \
	--private-networking \
	--ssh-keys k8s-key \
	&quot;do-test&quot;
</code></pre>

<p>With the above command, we created a <code>&quot;512mb&quot;</code> droplet, in the region of our choice, requesting a <code>private_ip</code> and adding our ssh-key (<code>k8s-key</code>) to the droplet for remote access. Once the command completed, Doctl returns information about the new Digital Ocean resource that was just created.</p>

<p>First, confirm you can list all your droplets and their status with the following command:</p>

<pre><code class="language-command">doctl droplet list
</code></pre>

<p>This should output a list similar to below:</p>

<pre><code>[secondary_label Output]
ID          Name            IP Address              Status  Memory  Disk    Region
8684261     do-test.ams2    198.211.118.106         new     512MB   20GB    ams2
</code></pre>

<p>Note that, to speed up its usage, Doctl has several shortcuts. For example, the shortcut for the <code>droplet</code> command is <code>d</code>. Moreover, the default action for the <code>droplet</code> command is <code>list</code>, allowing us to re-write the above command as follows:</p>

<pre><code class="language-command">doctl d
</code></pre>

<p>Returning the same results as before. On Linux you can <code>watch</code> this list to capture when the droplet status changes from <code>new</code> to <code>active</code> (which will take the same amount of time it would take when provisioning through the web control panel).</p>

<p>Once the CoreOS droplet has fully been provisioned and its status changed from <code>new</code> to <code>active</code>, ensure your SSH Key was added correctly by connecting as the <code>core</code> user, run:</p>

<pre><code class="language-command">ssh core@198.211.118.106
</code></pre>

<p>Replace the IP highlighted above by the public IP of your droplet, as listed by the previous <code>doctl d</code> command. As this is the first time you connect to this server, you may be prompted to confirm the fingerprint of the server:</p>

<pre><code>[secondary_label Output]
The authenticity of host '198.211.118.106 (198.211.118.106)' can't be established.
ED25519 key fingerprint is SHA256:wp/zkg0UQifNYrxEsMVg2AEawqSVpRS+3mBAQ6TBNlU.
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>For more information about accessing your Droplet remotely, see <a href="https://www.digitalocean.com/community/tutorials/how-to-connect-to-your-droplet-with-ssh#ssh-login-as-root">How To Connect To Your Droplet with SSH</a>.</p>

<p>You should now be connected to a fully functional CoreOS droplet running in Digital Ocean.</p>

<p>Press <code>CTRL+D</code> or enter <code>exit</code> to log out. Keep the <code>do-test</code> droplet running to complete the exercises in the next section.</p>

<h3 id="working-with-doctl-responses:a91459261407d5d36808cf519d4f7594">Working with Doctl responses</h3>

<p>By default, Doctl will return yaml responses, but it is possible to change the format of the response with the <code>-f</code> flag. Using the <code>json</code> format will allow us to easily act on the data returned by Doctl through the Command-line JSON processor <a href="https://github.com/stedolan/jq">Jq</a>.</p>

<p>Jq comes installed on several Linux distributions (i.e. CoreOS). However, to download and setup Jq manually, review <a href="https://github.com/digitalocean/doctl/releases/latest">the latest releases</a> and choose the right release  for your environment:</p>

<table>
<thead>
<tr>
<th>Operating System</th>
<th>Binary</th>
</tr>
</thead>

<tbody>
<tr>
<td>OSX</td>
<td>jq-osx-amd64</td>
</tr>

<tr>
<td>Linux</td>
<td>jq-linux64</td>
</tr>

<tr>
<td>Windows</td>
<td>jq-win64.exe</td>
</tr>
</tbody>
</table>

<p>For example, to download the <code>1.5</code> release from a shell directly to your <code>/usr/bin/</code> directory on a Linux 64-bit host (which requires <code>sudo</code> rights), run:</p>

<pre><code class="language-super_user">curl -Lo /usr/bin/jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64
</code></pre>

<p>This will make the <code>jq</code> command available for all users.</p>

<p>Validate Jq has been downloaded successfully by confirming the installed version:</p>

<pre><code class="language-command">jq --version
</code></pre>

<p>If you followed the steps above, this should return:</p>

<pre><code>[secondary_label Output]
jq-1.5
</code></pre>

<p>Using the <code>-f json</code> argument for Doctl together with Jq allows us to, for example, extract the number of CPUs a droplet has:</p>

<pre><code class="language-command">doctl -f json d find do-test.$region | jq '.vcpus'
</code></pre>

<p>In the above command, the <code>find</code> command for droplets (shortcut <code>f</code>) returns all properties of a droplet matching the name provided, including the droplet&rsquo;s <code>vcpus</code> property. This <code>json</code> data is passed on to Jq with an argument to only return the <code>vcpus</code> property to us.</p>

<p>Another example of using Jq to manipulate the data returned by Doctl is given next, extracting the raw <code>public_key</code> for an existing Digital Ocean SSH Key, the key named <code>k8s-key</code> in our example:</p>

<pre><code class="language-command">doctl -f json keys f k8s-key | jq --raw-output '.public_key'
</code></pre>

<p>With Output similar to:</p>

<pre><code>[secondary_label Output]
ssh-rsa AAAAB3Nza... user@host
</code></pre>

<p>By default Jq will format strings as json strings, but using the <code>--raw-output</code> flag (shortcut <code>-r</code>), as can be seen above, will make Jq write strings directly to standard output. This is very useful for our scripts.</p>

<p>Finally, the real power of Jq becomes evident when we need to retrieve an array of network interfaces (<code>ipv4</code>) assigned to a droplet, filter the array based on a property <code>.type</code> with possible values <code>&quot;public&quot;</code> or <code>&quot;private&quot;</code> and extract the raw value of the <code>ip_address</code> property.</p>

<p>We&rsquo;ll break this down as follows. Notice first that the following command will return an array of all the IPv4 network interfaces assigned to a droplet:</p>

<pre><code class="language-command">doctl -f json d f do-test.$region | jq -r '.networks.v4[]'
</code></pre>

<p>Which will return a result similar to the following text block:</p>

<pre><code>[secondary_label Output]
{
  &quot;ip_address&quot;: &quot;10.129.73.216&quot;,
  &quot;netmask&quot;: &quot;255.255.0.0&quot;,
  &quot;gateway&quot;: &quot;10.129.0.1&quot;,
  &quot;type&quot;: &quot;private&quot;
}
{
  &quot;ip_address&quot;: &quot;198.211.118.106&quot;,
  &quot;netmask&quot;: &quot;255.255.192.0&quot;,
  &quot;gateway&quot;: &quot;198.211.128.1&quot;,
  &quot;type&quot;: &quot;public&quot;
}
</code></pre>

<p>Next, we direct Jq to apply a filter to the array of network interfaces based on the <code>type</code> property using the <code>select</code> statement and only return the <code>.ip_address</code> property of the filtered network interface:</p>

<pre><code class="language-command">doctl -f json d f do-test.$region | jq -r '.networks.v4[] | select(.type == &quot;private&quot;)  | .ip_address'
</code></pre>

<p>The above command effectively returns the private <code>ip_address</code> of our droplet directly to standard out. We will use this command often to store droplet IP addresses into environment variables. The output of the command may look like:</p>

<pre><code>[secondary_label Output]
10.129.73.216
</code></pre>

<p>Finally, destroy your <code>do-test</code> droplet with the following command:</p>

<pre><code class="language-command">doctl d d do-test.$region
</code></pre>

<p>Which will output:</p>

<pre><code>[secondary_label Output]
Droplet do-test.ams2 destroyed.
</code></pre>

<p>For a full explanation of all the features of Jq, kindly refer to <a href="https://stedolan.github.io/jq/manual/">the Jq manual</a>.</p>

<h3 id="using-doctl-to-configure-coreos-droplets-with-cloud-config:a91459261407d5d36808cf519d4f7594">Using Doctl to configure CoreOS Droplets with cloud-config</h3>

<p>For a short introduction to <code>cloud-config</code> files, kindly refer to the section on <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-coreos-cluster-on-digitalocean#write-a-cloud-config-file">writing cloud-config files</a> of the <a href="https://www.digitalocean.com/community/tutorial_series/getting-started-with-coreos-2">Getting Started with CoreOS</a> series. We will explain every aspect of <code>cloud-config</code> files we rely on as we write our own in this tutorial.</p>

<p>One of the most useful aspects of <code>cloud-config</code> files is that they allow you to define a list of arbitrary Systemd units to start after booting. To understand these <code>coreos.units</code> better, refer to the <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files">understanding Systemd units and unit files tutorial</a>. We will walk you through many Systemd unit examples within this tutorial.</p>

<p>We will heavily rely on these config files to manage the configuration of our droplets, giving us a way to consistently provision our Kubernetes clusters on Digital Ocean in the future. It is important however to note that <code>cloud-config</code> files are not intended as a replacement for configuration management tools such as Chef/Puppet/Ansible/Salt/TerraForm and we may benefit more adopting one of these tools in the long run.</p>

<p>Please ensure you use the <a href="https://coreos.com/validate">CoreOS validator</a> to validate any <code>cloud-config</code> file you write as part of this tutorial. Ensuring the config files are valid prior to creating the droplets will help avoid frustration and time loss. Also refer to the <a href="https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-issues-with-your-coreos-servers">general troubleshooting tutorial for CoreOS on Digital Ocean</a> when faced with CoreOS issues.</p>

<p>For this tutorial, we will be passing <code>cloud-config</code> files through the <code>--user-data-file</code> option (shortcut <code>--uf</code>) when creating droplets from the terminal with Doctl.</p>

<p>To see how this works, follow the below steps to create a Droplet with a custom <code>motd</code> and automatic reboots switched off, as an exercise.</p>

<p>First, create a <code>test.yaml</code> file in your working directory, with the content as follows.</p>

<pre><code>[label test.yaml]
#cloud-config

write_files:
  - path: &quot;/etc/motd&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      Good news, everyone!
coreos:
  update:
    group: alpha
    reboot-strategy: off
</code></pre>

<p>The <code>write_files</code> <a href="https://coreos.com/os/docs/latest/cloud-config.html#write_files">directive</a> defines a set of files to create on the local filesystem. For each file, we specify the absolute location on disk through the <code>path</code> key and the data to be write through the <code>content</code> key. The <code>coreos.update.*</code> <a href="https://coreos.com/os/docs/latest/cloud-config.html#update">parameters</a> manipulate settings related to how CoreOS instances are updated, setting the <code>reboot-strategy</code> to <code>off</code> will instruct the CoreOS reboot manager (Locksmith) to disable rebooting after updates are applied.</p>

<p>Create a new droplet named <code>ccfg-test</code>, using this <code>test.yaml</code> file with the following command (this command will take about a minute to complete, please be patient):</p>

<pre><code class="language-command">doctl d c --wait-for-active \
	-i &quot;CoreOS-alpha&quot; \
	-s 512mb \
	-r &quot;$region&quot; \
	-p \
	-k k8s-key \
	-uf test.yaml ccfg-test
</code></pre>

<p>We are using the <code>d</code> shortcut to manage our <code>droplet</code> resources and <code>c</code> as a shortcut for <code>create</code>. The <code>--wait-for-active</code> flag will ensure Doctl waits for the droplet to become active before returning control to our terminal, which is why we had to wait.</p>

<p>Once Doctl returned control, you may need to give your Droplet some more time to boot and load the SSH Daemon before attempting to connect.</p>

<p>Try to connect via the public ip of this droplet with the following one-liner.</p>

<pre><code class="language-command">ssh core@`doctl d | grep ccfg-test | awk '{print $3}'`
</code></pre>

<p>In this case we are using the droplet listing command piped into <code>grep</code> to filter down to the droplet we just created and we capture the third column, which is the public IP, using <code>awk</code>. The result should be similar to below, confirming our <code>motd</code> was written correctly once we confirm the authenticity of our new droplet:</p>

<pre><code>[secondary_label Output]
The authenticity of host '37.139.21.41 (37.139.21.41)' can't be established.
ED25519 key fingerprint is SHA256:VtdI6P5sRqvQC0dGWE1ffLYTq1yBIWoRFdWc6qcm+04.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '37.139.21.41' (ED25519) to the list of known hosts.
Good news, everyone!
</code></pre>

<p>If you are prompted for a password, ensure your SSH Agent has the private key associated with your <code>k8s-key</code> loaded and try again.</p>

<p>If you happened to destroy a droplet directly prior to creating the one that you are connecting to, you may see a warning like this:</p>

<pre><code>[secondary_label Output]
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
...
</code></pre>

<p>If this is the case, your new droplet probably has the same IP address as the old, destroyed droplet, but it has a different host SSH key. This is fine, and you can remove the warning, by deleting the old droplet&rsquo;s host key from your system, with this command (replacing <code>$SERVER_IP_ADDRESS</code> with your droplet public ip):</p>

<pre><code class="language-command">ssh-keygen -R $SERVER_IP_ADDRESS
</code></pre>

<p>Now try connecting to your server again.</p>

<p>Finally, destroy this test droplet with a command similar to below:</p>

<pre><code class="language-command">doctl d d ccfg-test.$region
</code></pre>

<p><strong>Note</strong>: At the time of writing, user provided <code>cloud-config</code> files can not be modified once a droplet has been created. To change the <code>cloud-config</code> the droplets need to be re-created. Take this into consideration when writing cloud-config files which limit ssh access to certain user accounts as these may be reset after every reboot.</p>

<p>With the above commands in our toolbox, we are ready to start a highly automated Kubernetes configuration on Digital Ocean.</p>

<h2 id="step-2-provisioning-the-data-storage-back-end:a91459261407d5d36808cf519d4f7594">Step 2 — Provisioning The Data Storage Back End.</h2>

<p>No matter if you are using Swarm with Docker overlay networks or Kubernetes, a data storage back end is required for the infrastructure meta data.</p>

<p>Kubernetes uses Etcd for data storage and for cluster consensus between different software components. Etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. It is open-source and available on <a href="https://github.com/coreos/etcd">GitHub</a>. We will introduce the minimum concepts necessary to set up Etcd for our Kubernetes cluster, full Etcd documentation is available <a href="https://coreos.com/etcd/docs/latest/">here</a>.</p>

<p>Your Etcd cluster will be heavily utilized since all objects are stored within and every scheduling decision is recorded. It is recommended that you run a multi-droplet cluster to gain maximum performance and reliability of this important part of your Kubernetes cluster. Of your Kubernetes components, you should only give the <code>kube-apiserver</code> component read/write access to Etcd. You do not want the Etcd cluster used by Kubernetes exposed to every node in your cluster (or worse, to the Internet at large), because access to Etcd is equivalent to root in your cluster.</p>

<!-- TODO: We should share Etcd with flannel..., it means Etcd is exposed on every node... -->

<p>For development &amp; testing environments, a single droplet running Etcd will suffice.</p>

<p>For production environments it is highly recommended that Etcd is ran as a dedicated cluster separately from Kubernetes components. Use the <a href="https://coreos.com/os/docs/latest/cluster-architectures.html">CoreOS cluster architecture overview</a> as well as the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md">official Etcd clustering guide</a> to bootstrap a new Etcd cluster on Digital Ocean. If you do not have an existing Etcd cluster, you can bootstrap a fresh Etcd cluster on Digital Ocean either by:</p>

<ul>
<li>Using the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md#public-etcd-discovery-service">public Etcd discovery service</a> or</li>
<li>Using <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md#dns-discovery">DNS discovery</a></li>
</ul>

<p>Additionally, refer to the official Etcd guides on <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/security.md">securing your Etcd cluster</a> and to get a <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md">full overview of Etcd configuration flags</a></p>

<p>In this tutorial, instead of being slowed down and distracted by generating new discovery URLs and bootstrapping Etcd, it&rsquo;s easier to start a single Etcd node. Since the full Etcd daemon isn&rsquo;t running on all of the machines, we&rsquo;ll gain a little bit of extra CPU and RAM to play with. However, for ease of configuration of all the cluster services, we will run a local Etcd in <strong>proxy mode</strong> on every Worker node (this daemon will listen on localhost and proxy all requests to the Etcd node). This allows us to configure every cluster component with the local Etcd proxy.</p>

<p>If you already have an Etcd cluster and wish to skip this step, ensure that you have set the <code>$ETCD_PEER</code> environment variable to your Etcd cluster before proceeding with the rest of this tutorial.</p>

<h3 id="deploying-with-a-single-etcd-node:a91459261407d5d36808cf519d4f7594">Deploying with a single Etcd node</h3>

<p>Since we&rsquo;re only using a single Etcd node, there is no need to include a discovery token. There isn&rsquo;t any high availability for Etcd in this configuration, but that&rsquo;s assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address.</p>

<p>Etcd is configurable through command-line flags and environment variables. To start Etcd automatically using custom settings with Systemd, we may store manually created partial Systemd unit drop-ins under: <code>/etc/systemd/system/etcd2.service.d/</code>.</p>

<p>Alternatively, we may use the <a href="https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/cloud-config.md#etcd2"><code>coreos.etcd2.*</code></a> parameters in our <code>cloud-config</code> file to let CoreOS automatically generate the Etcd drop-ins on startup for us.</p>

<blockquote>
<p><strong>Note</strong>: <code>cloud-config</code> generated drop-ins are stored under <code>/run/systemd/system/etcd2.service.d/</code>.</p>
</blockquote>

<pre><code>#cloud-config

coreos:
  etcd2:
    name: &quot;etcd-01&quot;
    advertise-client-urls: &quot;http://$private_ipv4:2379&quot;
    listen-client-urls: &quot;http://$private_ipv4:2379, http://127.0.0.1:2379&quot;
    listen-peer-urls: &quot;http://$private_ipv4:2380, http://127.0.0.1:2380&quot;
    #bootstrapping
    initial-cluster: &quot;etcd-01=http://$private_ipv4:2380&quot;
    initial-advertise-peer-urls: &quot;http://$private_ipv4:2380&quot;
</code></pre>

<p>As we will use a single region for our Kubernetes cluster, we configure our Etcd instance to listen for incoming requests on the <code>private_ip</code> and <code>localhost</code> only, this may give us a little protection from the public Internet - but not from other droplets within the same region. For a production setup, it is recommended to follow the official Etcd guides on <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/security.md">securing your Etcd cluster</a>.</p>

<p>We set the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-listen-client-urls"><code>-listen-client-urls</code></a> flag to listen for client traffic and <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-listen-peer-urls"><code>-listen-peer-urls</code></a> flag to listen for peer traffic coming from Etcd proxies running on other cluster nodes. We use the <code>$private_ipv4</code> substitution variable made available by the Digital Ocean <a href="https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-issues-with-your-coreos-servers#checking-for-access-to-the-metadata-service">metadata service</a> in our <code>cloud-config</code> files. We use the <a href="http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=etcd">IANA-assigned</a> Etcd ports <code>2379</code> for client traffic and <code>2380</code> for peer traffic.</p>

<blockquote>
<p><strong>Note</strong>: Several Etcd applications, such as <a href="https://github.com/skynetservices/skydns">SkyDNS</a>, still rely on Etcd&rsquo;s legacy port <code>4001</code>. We did not configure Etcd to listen on this port, but you may need to do this to support older Etcd applications in your infrastructure.</p>
</blockquote>

<p>Our Etcd node will advertise itself with it&rsquo;s <code>private_ip</code> to clients as we define <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-advertise-client-urls"><code>-advertise-client-urls</code></a> to overwrite the default of <code>localhost</code>. this is important to avoid loops for our <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/proxy.md">Etcd proxy</a> running on our worker nodes. We are also required to configure a <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-name">name</a> for our Etcd instance to overwrite the default name for static bootstrapping. To bootstrap our single node Etcd cluster we directly provide the <code>initial</code> clustering flags <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-initial-cluster"><code>-initial-cluster</code></a> and <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-initial-advertise-peer-urls"><code>-initial-advertise-peer-urls</code></a> as we do not rely on cluster discovery.</p>

<p>Next, we tell Systemd to start our Etcd service on boot by providing a unit definition for the <code>etcd2</code> service in the same <code>cloud-config</code> file as well and as this component is crucial and we only have a single node, we turn off the CoreOS <code>reboot-strategy</code> which is <code>on</code> by default.</p>

<p>Combining all of the above, our <code>cloud-config</code> file for our Etcd Droplet should look as follows:</p>

<pre><code class="language-line_numbers">[label etcd-01.yaml]

#cloud-config

coreos:
  etcd2:
    name: &quot;etcd-01&quot;
    advertise-client-urls: &quot;http://$private_ipv4:2379&quot;
    listen-client-urls: &quot;http://$private_ipv4:2379, http://127.0.0.1:2379&quot;
    listen-peer-urls: &quot;http://$private_ipv4:2380, http://127.0.0.1:2380&quot;
    #bootstrapping
    initial-cluster: &quot;etcd-01=http://$private_ipv4:2380&quot;
    initial-advertise-peer-urls: &quot;http://$private_ipv4:2380&quot;
  units:
    - name: &quot;etcd2.service&quot;
      command: &quot;start&quot;
  update:
    group: alpha
    reboot-strategy: off
</code></pre>

<p><a href="https://coreos.com/validate">Validate</a> your cloud-config file, then create your <code>etcd-01</code> droplet with the following command. :</p>

<pre><code class="language-command">doctl d c --wait-for-active \
	-i &quot;CoreOS-alpha&quot; \
	-s 512mb \
	-r &quot;$region&quot; \
	-p \
	-k k8s-key \
	-uf etcd-01.yaml etcd-01
</code></pre>

<p>Again we are waiting for the droplet creation to be completed before proceeding. When ready, give the Droplet some time to start the SSH daemon, then connect::</p>

<pre><code class="language-command">ssh core@`doctl d | grep etcd-01 | awk '{print $3}'`
</code></pre>

<p>Confirm Etcd is running:</p>

<pre><code class="language-command">systemctl status etcd2
</code></pre>

<p>This should return output similar to:</p>

<pre><code>[secondary_label Output]
● etcd2.service - etcd2
   Loaded: loaded (/usr/lib64/systemd/system/etcd2.service; disabled; vendor preset: disabled)
  Drop-In: /run/systemd/system/etcd2.service.d
           └─20-cloudinit.conf
   Active: active (running) since Sat 2015-11-11 23:19:13 UTC; 6min ago
 Main PID: 841 (etcd2)
   CGroup: /system.slice/etcd2.service
           └─841 /usr/bin/etcd2

Nov 11 23:19:13 etcd-01.ams2 systemd[1]: Started etcd2.
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: added local member ce2a822cea30bfca [http://10.129.69.201:2379] to cluster 7e27652122e8b2ae
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca is starting a new election at term 1
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca became candidate at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca received vote from ce2a822cea30bfca at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca became leader at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: raft.node: ce2a822cea30bfca elected leader ce2a822cea30bfca at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: setting up the initial cluster version to 2.2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: set the initial cluster version to 2.2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: published {Name:etcd-01 ClientURLs:[http://10.129.69.201:2379]} to cluster 7e27652122e8b2ae
</code></pre>

<p>Confirm the cluster is healthy:</p>

<pre><code class="language-command">etcdctl cluster-health
</code></pre>

<p>This should return output similar to:</p>

<pre><code>[secondary_label Output]
member ce2a822cea30bfca is healthy: got healthy result from http://10.129.69.201:2379
cluster is healthy
</code></pre>

<p>Close the connection to the droplet and note down the Etcd endpoint your kubernetes will use, <code>http://10.129.69.201:2379</code> for clients and <code>http://10.129.69.201:2380</code> for peers, in the example above.</p>

<p>if we were to script this assignment, using Jq we can extract the <code>private_ip</code> property of the droplet and format the result as required:</p>

<pre><code class="language-command">export ETCD_PEER=`doctl -f json d f etcd-01.$region | jq -r '.networks.v4[] | select(.type == &quot;private&quot;)  | &quot;http://\(.ip_address):2380&quot;'`
</code></pre>

<p>Refer to <a href="#working-with-doctl-responses">Working with Doctl responses</a> of Step 1 in this tutorial for a full explanation of the above command and confirm:</p>

<pre><code class="language-command">echo $ETCD_PEER
</code></pre>

<p>This should return output similar to:</p>

<pre><code>[secondary_label Output]
http://10.129.69.201:2380
</code></pre>

<p>For our cluster nodes, Etcd will run in proxy mode pointing to our Etcd Droplet <code>private_ip</code>. The relevant part of the cluster nodes <code>cloud-config</code> for Etcd proxies will look like this:</p>

<pre><code>[label cloud-config-*.yaml - cluster etcd proxy config snippet]
...
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: &quot;etcd-01=ETCD_PEER&quot;
  units:
    - name: &quot;etcd2.service&quot;
      command: &quot;start&quot;
...
</code></pre>

<p>Where we will script the substitution of the <code>ETCD_PEER</code> placeholder with the following <code>sed</code> command, prior to creating the Droplets:</p>

<pre><code class="language-command">sed -i -e &quot;s|ETCD_PEER|${ETCD_PEER}|g;&quot; cloud-config-*.yaml
</code></pre>

<blockquote>
<p><strong>Note</strong>: Because our variable value includes forward slashes, we are using <code>sed</code> with the pipeline &laquo;|&raquo; character as separator for the &laquo;s&raquo; command instead of the more common forward slash. Whichever character follows the &laquo;s&raquo; command is used as the separator by <code>sed</code>.</p>
</blockquote>

<p>We will proceed with reviewing the networking requirements for Kubernetes and how we achieve them on Digital Ocean in the next section.</p>

<h2 id="step-3-configuring-the-network-fabric-layer:a91459261407d5d36808cf519d4f7594">Step 3 — Configuring The Network Fabric Layer.</h2>

<p>As explained in the introduction of this tutorial, Kubernetes has the fundamental networking requirement of ensuring that all containers are routable without network translation or port brokering on the hosts. In other words, this means every Droplet is required to have its own IP range within the cluster. To achieve this on Digital Ocean, Flannel will be used to provide an overlay network across multiple Droplets and configure Docker to use this networking layer for its containers.</p>

<p>Flannel runs an agent, <code>flanneld</code>, on each host which is responsible for allocating a subnet lease out of a pre-configured address space. When enabling Flannel on CoreOS, <a href="https://coreos.com/flannel/docs/latest/flannel-config.html#under-the-hood">CoreOS will ensure</a> Docker is automatically configured to use the Flannel overlay network for its containers.</p>

<p>Flannel uses Etcd to store the network configuration, allocated subnets, and auxiliary data (such as host&rsquo;s IP). The Etcd storage back end for Flannel may be ran separately in the cluster. To reduce the complexity in this tutorial however, we will configure Flannel to share the external Etcd cluster with Kubernetes which is acceptable for Testing and Development only. By default, Flannel looks up its configuration under the <code>/coreos.com/network/config</code> key within Etcd. To run Flannel on each node in a consistent way, we are required to publish the Flannel configuration to Etcd under this key.</p>

<p>At the bare minimum, the configuration must provide Flannel an IP range (subnet) that it should use for the overlay network. The IP subnet used by Flannel should not overlap with the public and private IP ranges used by the Digital Ocean Droplets, <code>10.2.0.0/16</code> is the IP range used in this tutorial. This /16 range will be assigned for the entire overlay network and used by containers and Pods across the cluster Droplets. By default, Flannel will allocate a /24 to each Droplet. This default, along with the minimum and maximum subnet IP addresses is <a href="https://coreos.com/flannel/docs/latest/flannel-config.html#publishing-config-to-etcd">overridable in config</a>.</p>

<p>The forwarding of packets by Flannel is achieved using one of several strategies that are known as back ends. In this tutorial we will configure Flannel to use the <code>vxlan</code> back end which is built on the performant in-kernel VXLAN tunneling protocol to encapsulate the packets for the overlay network.</p>

<p>If we were to use the <a href="https://github.com/coreos/etcd/tree/master/etcdctl"><code>etcdctl</code></a> utility, which is shipped with CoreOS, directly from the terminal of any of our Droplets to publish this configuration, it would look like this:</p>

<pre><code class="language-command">etcdctl set /coreos.com/network/config '{&quot;Network&quot;:&quot;10.2.0.0/16&quot;, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}
</code></pre>

<p>With the above command, <code>etcdctl</code> uses the localhost, which in our hosts will be an Etcd daemon running in proxy mode forwarding the configuration to our Etcd cluster.</p>

<p>To facilitate the cluster bootstrap, we will put this command into a drop-in for the <code>flanneld.service</code> as part of our Kubernetes controller&rsquo;s <code>cloud-config</code>. You can create this <code>cloud-config-controller.yaml</code> file and add the snippets as we go through them in this tutorial, however we will re-order the snippets in the final file and provide a full listing of the file as part of this tutorial.</p>

<pre><code>[label cloud-config-controller.yaml - flannel drop-in snippet]
...
  units:
    - name: flanneld.service
      command: start
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{&quot;Network&quot;:&quot;10.2.0.0/16&quot;, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}'
...
</code></pre>

<blockquote>
<p><strong>Note</strong>: any services that run Docker containers must come after the <code>flanneld.service</code> and should include <code>Requires=flanneld.service</code>, <code>After=flanneld.service</code>, and <code>Restart=always|on-failure</code> directives. These directives are necessary because <code>flanneld.service</code> may fail due to Etcd not being available yet. It will keep restarting and it is important for Docker based services to also keep trying until Flannel is up.</p>
</blockquote>

<p>In order for Flannel to manage the pod network in the cluster, Docker needs to be configured to use it. All we need to do is require that <code>flanneld</code> is running prior to Docker starting.</p>

<p>We&rsquo;re going to do this with a Systemd drop-in, which is a method for appending or overriding parameters of a Systemd unit. In this case we&rsquo;re appending two dependency rules:</p>

<pre><code>[label cloud-config-controller - docker config snippet]
...
  units:
    - name: docker.service
      command: start
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
...
</code></pre>

<h2 id="step-4-understanding-where-to-get-the-kubernetes-artifacts:a91459261407d5d36808cf519d4f7594">Step 4 — Understanding Where To Get The Kubernetes Artifacts</h2>

<p>At the time of writing, the <a href="http://kubernetes.io/v1.1/docs/getting-started-guides/scratch.html#downloading-and-extracting-kubernetes-binaries">Official</a> guidelines require the Kubernetes binaries and Docker images wrapping those binaries to be downloaded as part of the full Kubernetes release archive located under the <a href="https://github.com/kubernetes/kubernetes/releases">Kubernetes</a> repository on GitHub.</p>

<p>At the same time, all Kubernetes artifacts are also stored on the <code>kubernetes-release</code> bucket on Google cloud storage for every release.</p>

<p>To confirm the current stable release of Kubernetes run:</p>

<pre><code class="language-command">curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt
</code></pre>

<p>This returned v1.1.2 at the time of writing.</p>

<p>To list all Kubernetes release binaries stored in the Google cloud storage bucket for a particular release, you can either use the python 2.7 based <a href="https://cloud.google.com/storage/docs/gsutil">Gsutil</a> from the Google SDK as follows:</p>

<pre><code class="language-command">gsutil ls -R gs://kubernetes-release/release/v1.1.2/bin/linux/amd64
</code></pre>

<p>Or without Python, directly talking to the <a href="https://cloud.google.com/storage/docs/json_api/v1/buckets/list">Google cloud platform API</a> using <code>curl</code> &amp; <code>jq</code> (<a href="https://stedolan.github.io/jq/manual/">Jq ref</a>):</p>

<pre><code class="language-command"> curl -sL  https://www.googleapis.com/storage/v1/b/kubernetes-release/o?prefix='release/v1.1.2/bin/linux/amd64' | jq '&quot;https://storage.googleapis.com/kubernetes-release/\(.items[].name)&quot;'
</code></pre>

<p>A combined binary is provided as the <a href="https://releases.k8s.io/release-1.1/cmd/hyperkube">Hyperkube</a>. This Hyperkube is an all-in-one binary, allowing you to run any Kubernetes component as a subcommand of the <code>hyperkube</code> command. The Hyperkube is also made available within a Docker image. The Dockerfile of this container can be reviewed <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/cluster/images/hyperkube/Dockerfile">here</a>.</p>

<p>The plan for the Kubernetes release process is to publish the Kubernetes images on the Google Container Registry, under the <code>google_containers</code> repository: <code>gcr.io/google_containers/hyperkube:$TAG</code>, where TAG is the latest stable release tag (i.e.: <code>v1.1.2</code>).</p>

<p>For example, we would obtain the Hyperkube image with the following command:</p>

<pre><code class="language-command">docker pull gcr.io/google_containers/hyperkube:v1.1.2
</code></pre>

<blockquote>
<p><strong>Note</strong>: At the time of writing, Kubernetes images were not yet being pushed to the Google Container Registry as part of the release process. Any available images were pushed as a one-off. Refer to <a href="https://github.com/kubernetes/kubernetes/issues/11751">the following support ticket</a> for an updated status of the release process.</p>
</blockquote>

<p>Moreover, as the Hyperkube combines all binaries, is based on <code>debian:jessie</code> and includes additional packages such <code>iptables</code> (required by the <code>kube-proxy</code>), its size is considerable:</p>

<pre><code>[secondary_label Output]
gcr.io/google_containers/hyperkube    v1.1.2    aa1283b0c02d    2 weeks ago    254.3 MB
</code></pre>

<p>As a result, for this tutorial, we will run the <code>kube-proxy</code> binary outside of a container, the same way we run the <code>kubelet</code> or any system daemon. For the <code>kube-apiserver</code>, <code>kube-controller-manager</code> and <code>kube-scheduler</code> it is recommended to run these as containers and we will take a closer look at the available Docker images now.</p>

<p>As can be seen in the listings earlier, tarred repositories for Docker images wrapping the Kubernetes binaries are also available:</p>

<table>
<thead>
<tr>
<th>binary_name</th>
<th>base image</th>
<th>size</th>
</tr>
</thead>

<tbody>
<tr>
<td>kube-apiserver</td>
<td>busybox</td>
<td>47.97 MB</td>
</tr>

<tr>
<td>kube-controller-manager</td>
<td>busybox</td>
<td>40.12 MB</td>
</tr>

<tr>
<td>kube-scheduler</td>
<td>busybox</td>
<td>21.44 MB</td>
</tr>
</tbody>
</table>

<p>Assuming you have access to a Docker daemon, we can <code>curl</code> and <code>load</code> these Docker images with the following commands.</p>

<pre><code class="language-command">curl -sLo ./kube-apiserver.tar https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver.tar
docker load -i ./kube-apiserver.tar
</code></pre>

<p>We have loaded the <code>kube-apiserver</code> image in this example.</p>

<p>The Kubernetes build script tags these images as <code>gcr.io/google_containers/$binary_name:$md5_sum</code>. To easily run a container from this image, or push the image to a private registry for bootstrapping, we may re-tag the images with the following commands:</p>

<pre><code class="language-command">#get md5 via docker_tag
docker_tag=&quot;$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver.docker_tag)&quot;
#re-tag
docker tag -f &quot;gcr.io/google_containers/kube-apiserver:${docker_tag}&quot; &quot;kube-apiserver:1.1.2&quot;
</code></pre>

<p>In the above command we first get the md5_sum from the cloud storage bucket and use it to re-tag the image.</p>

<!-- do we really need to mention the Kuar images? They are mentioned in every Workshop from Kelsey and used a lot... may be just to clarify they are not official and their presence is not guaranteed.. -->

<p><strong>A note about the Kubernetes Up and Running images</strong>:</p>

<p>Non-official images are also made available by Kelsey Hightower for his Kubernetes Up And Running book through his <code>kuar</code> repository (backed by the Kubernetes Up And Running <a href="https://cloud.google.com/container-registry/docs/#using_an_existing_google_cloud_storage_bucket">Google cloud storage bucket</a>), we may list the contents of that bucket using the <code>repositories/library</code> key, as follows:</p>

<pre><code class="language-command">gsutil ls gs://kuar/repositories/library/
</code></pre>

<p>Or without Python, using <code>curl</code>, <code>jq</code> and <code>grep</code> to grab all v1.1.2 Kubernetes tagged images:</p>

<pre><code class="language-command">curl -sL  https://www.googleapis.com/storage/v1/b/kuar/o?prefix='repositories/library/kube' | jq .items[].name | grep tag_1.1.2
</code></pre>

<p>The <code>kuar</code> images are more easily ran with a single Docker command using the registry endpoint: <code>b.gcr.io/kuar/$binary_name:$version</code>, for example: to run the <code>kube-apiserver</code>:</p>

<pre><code class="language-command">docker run b.gcr.io/kuar/kube-apiserver:1.1.2
</code></pre>

<p>This is much easier than manually <code>curl</code>-ing, <code>load</code>-ing, re-<code>tag</code>-ing and <code>run</code>-ing the images, but keep in mind that these are not the official Kubernetes images and their availability is not guaranteed.</p>

<h2 id="step-5-initializing-the-kubernetes-cluster-pki:a91459261407d5d36808cf519d4f7594">Step 5 — Initializing The Kubernetes Cluster PKI</h2>

<p>In this step we will initialize the root Certificate Authority used by our Cluster.</p>

<p>In this tutorial we will configure the Kubernetes API Server to use client certificate authentication to enable encryption and prevent traffic interception and man-in-the-middle attacks. This means it is necessary to have a Certificate Authority (CA) which will be trusted as the root authority for the cluster and use it to generate the proper credentials. The necessary assets can also be generated from an existing Public Key Infrastructure (PKI), if already available.</p>

<p>For this tutorial we will use Self-Signed certificates. Every certificate is created by submitting Certificate Signing Requests (CSRs) to a CA. A CSR contains information identifying whom the certificate request is for, including the public key associated with the private key of the requesting party. The CA will sign the CSR, effectively returning what is from then on referred to as &laquo;the certificate&raquo;.</p>

<p>For a detailed overview of OpenSSL, refer to the <a href="https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs">OpenSSL Essentials guide</a> on Digital Ocean.</p>

<h3 id="initialize-cluster-root-ca:a91459261407d5d36808cf519d4f7594">Initialize Cluster Root CA</h3>

<p>Generate the private key for your root certificate into the default <code>$HOME/.kube</code> folder, which we should have created as part of our client machine setup, with the following OpenSSL command to generate a 2048 bit RSA private key:</p>

<pre><code class="language-command">openssl genrsa -out ~/.kube/ca-key.pem 2048
</code></pre>

<p>This <code>ca-key.pem</code> private key will be used to generate the self-signed <code>ca.pem</code> certificate which will be trusted by all your Kubernetes components, as well as every Worker node and Administrator key pair. This key needs to be closely guarded and kept in a secure location for future use.</p>

<p>Next, use the private key to generate the self-signed root <code>ca.pem</code> certificate with the following openssl command:</p>

<pre><code class="language-command">openssl req -x509 -new -nodes -key ~/.kube/ca-key.pem -days 10000 -out ~/.kube/ca.pem -subj &quot;/CN=kube-ca&quot;
</code></pre>

<p>The <code>ca.pem</code> certificate will be used as the root certificate to verify the authenticity of certificates by every component within your Kubernetes cluster, you will copy this file to the controller and worker Droplets as well as the Administrator clients.</p>

<p>Confirm your Root CA assets exist in the expected location:</p>

<pre><code class="language-command">ls -1 ~/.kube/ca*.pem
</code></pre>

<p>Output similar to:</p>

<pre><code>[secondary_label Output]
/home/demo/.kube/ca.pem  
/home/demo/.kube/ca-key.pem
</code></pre>

<p>We now have all the necessary ingredients to generate certificates for all of our cluster components. We will come back to <code>openssl</code> to generate each as required.</p>

<h2 id="step-6-provisioning-the-kubernetes-controller-droplets:a91459261407d5d36808cf519d4f7594">Step 6 — Provisioning The Kubernetes Controller Droplets</h2>

<p>In this step we will create a single Kubernetes controller Droplet.</p>

<p>Most of the Kubernetes controller configuration will be done through <code>cloud-config</code>, aside from placing the TLS assets on disk. The <code>cloud-config</code> we create will take into account the possibility to have load-balanced controller nodes for High Availability in the future. How this affects our configuration will be discussed in detail in the <a href="#controller-services-set-up-master-election">Controller Services set up: Master Election</a> section.</p>

<blockquote>
<p><strong>Note</strong>: The TLS assets shouldn&rsquo;t be stored in the <code>cloud-config</code> for enhanced security. If you do prefer to transfer the TLS assets as part of the <code>cloud-config</code> refer to <a href="https://coreos.com/os/docs/latest/customizing-docker.html#cloud-config5">this CoreOS tutorial</a> as an example of storing TLS assets within the <code>cloud-config</code> file.</p>
</blockquote>

<p>We will now introduce every Kubernetes component and its configuration to incrementally add to our controller Droplet&rsquo;s <code>cloud-config</code> file and when completed, we will create the controller Droplet with Doctl from our client machine. Once the Droplet is ready we will generate and transfer the TLS assets.</p>

<h3 id="a-deep-dive-into-the-kubelet:a91459261407d5d36808cf519d4f7594">A Deep Dive Into The Kubelet</h3>

<p>As seen in the Architectural overview section, Kubernetes is made up of several components. One such fundamental component is the <a href="http://kubernetes.io/v1.1/docs/admin/kubelet.html"><code>kubelet</code></a>. The <code>kubelet</code> is responsible for what&rsquo;s running on each individual Droplet within your cluster. You can think of it as a process watcher like <code>systemd</code>, but focused on running containers. It has one job: given a set of containers to run, make sure they are all running.</p>

<p>The unit of execution Kubernetes works with is the <a href="http://kubernetes.io/v1.1/docs/user-guide/pods.html">Pod</a>, not the individual container. A Pod is a collection of containers and volumes sharing the same execution environment. The containers within a Pod share a single IP, in our case this IP is provided by Docker within the Flannel overlay network. Pods are defined by a JSON or YAML file called a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod">Pod manifest</a>.</p>

<!-- this is duplicated from the Kubernetes overview part -->

<p>Within a Kubernetes cluster, the <code>kubelet</code> functions as a local agent that watches for Pod specs via the Kubernetes API server. The <code>kubelet</code> is also responsible for registering a node with a Kubernetes cluster, sending events and pod status, and reporting resource utilization.</p>

<p>While the <code>kubelet</code> plays an important role in a Kubernetes cluster, it also works well in standalone mode - outside of a Kubernetes cluster. With the <code>kubelet</code> running in standalone mode we will be able to use containers to distribute our binaries, monitor container resource utilization through the built-in support for <a href="https://github.com/google/cadvisor/blob/master/README.md">cAdvisor</a> and establish resource limits for the daemon services. The <code>kubelet</code> provides a convenient interface for managing containers on a local system, allowing us to update our controller services by updating the containers without rebuilding our unit files. To achieve this, the <code>kubelet</code> supports the configuration of a manifest directory, which is monitored for pod manifests every 20 seconds by default.</p>

<p>We will use our controller&rsquo;s <code>cloud-config</code> to configure &amp; start the <code>kube-kubelet.service</code> in standalone mode on our controller Droplet. We will run the <code>kube-proxy</code> service in the same way. Next, we will deploy all the Kubernetes cluster control services using a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod">Pod manifest</a> placed in the <code>manifest</code> directory of the controller as soon as the TLS assets become available. The kubelet will start and make sure all containers within the Pod keep running, just as if the Pod was submitted via the API. The cool trick here is that we don&rsquo;t have an API running yet, but the Pod will function in the exact same way, which simplifies troubleshooting later on.</p>

<!-- TODO: Should we always curl the kubelet for consistency? -->

<blockquote>
<p><strong>Note</strong>: Please note that only CoreOS Alpha or Beta images come with the Kubernetes kubelet. The Stable channel has never contained a version which included the <code>kubelet</code>. If a Droplet was booted from the Beta or Alpha channels and then moved to the Stable channel, it will lose the <code>kubelet</code> when it updates to the stable release.</p>
</blockquote>

<p>At the time of writing, the CoreOS Alpha image on Digital Ocean has the following version:</p>

<pre><code>[secondary_label Output]
$ cat /etc/lsb-release
DISTRIB_ID=CoreOS
DISTRIB_RELEASE=891.0.0
</code></pre>

<p>And the Kubelet bundled with this is:</p>

<pre><code>[secondary_label Output]
$ kubelet --version=true
Kubernetes v1.1.2+3085895
</code></pre>

<p>If you are required to use the CoreOS stable channel or need a different Kubelet version, you may <code>curl</code> the <code>kubelet</code> binary as part of the <code>cloud-config</code> using the paths identified in <a href="#step-4-understanding-where-to-get-the-kubernetes-artifacts">Step 4 — Understanding Where To Get The Kubernetes Artifacts</a> of this tutorial.</p>

<pre><code>[label cloud-config-* - kubelet snippet]
...
ExecStartPre=/usr/bin/curl -sLo /opt/bin/kubelet -z /opt/bin/kubelet https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubelet
ExecStartPre=/usr/bin/chmod +x /opt/bin/kubelet
...
</code></pre>

<blockquote>
<p><strong>Note</strong>: The <code>-z</code> option of <code>curl</code> will only download newer files based on a date expression or, as used here - given an existing local file - only if the date of the remote file is newer than the date of the local file. This will generate a warning if the local file does not exist, as shown below.</p>
</blockquote>

<pre><code>[secondary_label Output]
Warning: Illegal date format for -z/--timecond (and not a file name).
Warning: Disabling time condition. See curl_getdate(3) for valid date syntax.
</code></pre>

<p>Wherever we <code>curl</code> binaries with the <code>-z</code> option as part of a Systemd unit, these warnings will show in the journal and can safely be ignored.</p>

<h3 id="running-the-kubelet-in-standalone-mode:a91459261407d5d36808cf519d4f7594">Running the Kubelet in standalone mode</h3>

<p>The parameters we will pass on to the kubelet are as follows, we will break these down one by one next:</p>

<pre><code>kubelet \
  --api-servers=http://127.0.0.1:8080 \
  --register-node=false \
  --allow-privileged=true \
  --config=/etc/kubernetes/manifests \
  --hostname-override=$public_ipv4 \
  --cluster-dns=10.3.0.10 \
  --cluster-domain=cluster.local
</code></pre>

<p>The <code>kubelet</code> will communicate with the API server through <code>localhost</code> as we specify this with the <code>--api-servers</code> flag, but it will not register our controller node for cluster work as we set the <code>--register-node=false</code> flag, this ensures our controller Pods will not be affected by Pods scheduled by users within the cluster. As mentioned in the Kubelet deep dive section, to run the <code>kubelet</code> in standalone mode, we need to point it to a manifest directory. We set the <code>kubelet</code> manifest directory via the <code>--config</code> flag, which will be <code>/etc/kubernetes/manifests</code> in our setup. To facilitate the routing between Droplets, we also override the hostname with the Droplet public IP through the <code>--hostname-override</code> flag.</p>

<p>CoreOS Linux ships with reasonable defaults for the kubelet, which have been optimized for security and ease of use. However, we are going to loosen the security restrictions in order to enable support for privileged containers through the <code>--allow-privileged=true</code> flag.</p>

<p><strong>Service Discovery and Kubernetes Services</strong></p>

<p>To enable service discovery within the Kubernetes cluster, we need to provide our <code>kubelet</code> with the service IP for the cluster DNS component as well as the DNS domain. The <code>kubelet</code> will pass this on as the DNS server and DNS search suffix to each container running within the cluster. In this tutorial we will deploy DNS as a service within our Kubernetes cluster through <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns">the cluster DNS add-on</a>, for which Kubernetes uses cluster Virtual IPs (VIPs). Routing to these VIPs is handled by the Kubernetes proxy components and VIPs are not required to be routable between nodes.</p>

<p>We configure Kubernetes to use the <code>10.3.0.0/24</code> IP range for all services. Each service will be assigned a cluster IP in this range. This range must not overlap with any IP ranges assigned to Pods as configured in our Flannel overlay network, or the Digital Ocean public and private IP ranges. The API server will take the first IP in that range (<code>10.3.0.1</code>) by itself and we will configure the DNS service to take the static IP of <code>10.3.0.10</code>. Modify these values to mirror your own configuration.</p>

<p>We must pass on this DNS service IP to the <code>kubelet</code> via the <code>--cluster-dns</code> flag and the DNS domain via the <code>--cluster-domain</code> flag.</p>

<p>If the <code>kubelet</code> is bundled with CoreOS (Alpha/Beta), it is stored on <code>/usr/bin/kubelet</code>, if you manually download (Stable) to another path (<code>/opt/bin/kubelet</code> for example), make sure to update the paths in the snippet below. Prior to starting the Kubelet service, we will also ensure the <code>manifests</code> and <code>ssl</code> directories exist on the host using the <code>ExecStartPre</code> directive, preceded by &laquo;-&raquo; which indicates to Systemd that failure of the command will be tolerated.</p>

<p>We combine all the information above in the <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files">systemd service unit</a> file for running the kubelet. We add a dependency on the <code>docker.service</code> and make sure the unit restarts on failure. This is only a snippet of our controller droplet&rsquo;s full <code>cloud-config</code>:</p>

<pre><code>[label cloud-config-controller.yaml kubelet - snippet]
...
  units:
    - name: &quot;kube-kubelet.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service
        After=docker.service
        [Service]
        ExecStartPre=-/bin/bash -c &quot;mkdir -p /etc/kubernetes/{manifests,ssl}&quot;
        ExecStart=/usr/bin/kubelet \
        --api-servers=http://127.0.0.1:8080 \
        --register-node=false \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local
        Restart=always
        RestartSec=10
...
</code></pre>

<p>With this configuration, all state-less controller services will be managed through the Pod manifests dropped into the <code>kubelet</code>&rsquo;s manifest folder (<code>/etc/kubernetes/manifests</code>). After configuring the <code>kube-proxy</code> service next, we will go through the structure of a Pod manifest and the Pod manifest section for each controller service. We will finalize the controller configuration section with an overview of the full Kubernetes controller Pod manifests and the full controller <code>cloud-config</code> file.</p>

<h3 id="running-the-kubernetes-proxy-service:a91459261407d5d36808cf519d4f7594">Running the Kubernetes Proxy Service</h3>

<p>All nodes should run <code>kube-proxy</code> (Running <code>kube-proxy</code> on a &laquo;controller&raquo; node is not strictly required, but being consistent is easier.) The proxy is responsible for directing traffic destined for specific services and pods to the correct location. The proxy communicates with the API server periodically to keep up to date.</p>

<!-- TODO: change "option" to "command" -->

<p>Unlike the <code>kubelet</code>, the <code>kube-proxy</code> binary is currently not shipped with any CoreOS release. The URL to download the binary is described in <a href="#step-4-understanding-where-to-get-the-kubernetes-artifacts">Step 4 — Understanding Where To Get The Kubernetes Artifacts</a> of this tutorial. We will <code>curl</code> the binary from this URL prior to starting the service by providing the following <code>ExecStartPre</code> directives within the <code>[Service]</code> section of our Systemd unit:</p>

<pre><code>ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
</code></pre>

<p>We will also delay the <code>kube-proxy</code> daemon from starting until the <code>kube-apiserver</code> service has started with the following <code>ExecStartPre</code> option:</p>

<pre><code>ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
</code></pre>

<p>Both the controller and worker nodes in your cluster will run the proxy. The following <code>kube-proxy</code> parameters will be defined in our systemd service unit:</p>

<ul>
<li><code>--master=http://127.0.0.1:8080</code>: The address of the Kubernetes API server for our Kubernetes Controller node. In the section below, we will configure our <code>kube-apiserver</code> to bind to the network of the host and be reachable on the loopback interface.</li>
<li><code>--proxy-mode=iptables</code>: The proxy mode for our <code>kube-proxy</code>. At the time of writing the following two options are valid: <code>userspace</code> (older, stable) or <code>iptables</code> (experimental). If the <code>iptables</code> mode is selected, but the system&rsquo;s kernel or iptables versions are insufficient, this always falls back to the <code>userspace</code> proxy.</li>
<li><code>--hostname-override=$public_ipv4</code>: to facilitate routing without DNS resolution.</li>
</ul>

<pre><code>[label cloud-config-controller.yaml kube-proxy - snippet]

  units:
    - name: kube-proxy.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        # wait for kube-apiserver to be up and ready
        ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
        ExecStart=/opt/bin/kube-proxy \
        --master=http://127.0.0.1:8080 \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        TimeoutStartSec=10
        Restart=always
        RestartSec=10
</code></pre>

<p><strong>Note</strong>: At the time of writing, the <code>kube-proxy</code> binary is 18.3MB while the docker wrapped image based on Debian with the Iptables package installed is over 180MB, downloading the <code>kube-proxy</code> binary takes less than 10 seconds and is therefore the method used in this tutorial as opposed to running the proxy in a privileged Hyperkube container.</p>

<blockquote>
<p><strong>Note</strong>: By setting the <code>TimeoutStartSec</code> to 10, Systemd will fail the kube-proxy if it hasn&rsquo;t started after 10 seconds, but it will be restarted after the specified <code>RestartSec</code> timeout. We may notice these failures in the journal until the <code>kube-apiserver</code> has started. Until we are certain the <code>kube-apiserver</code> has started, these warning may be ignored.</p>
</blockquote>

<p>For the full overview of all <code>kube-proxy</code> arguments, refer to <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/kube-proxy.md">the official documentation</a>.</p>

<h3 id="pre-loading-kubernetes-containers:a91459261407d5d36808cf519d4f7594">Pre-Loading Kubernetes containers</h3>

<p>As seen in <a href="#step-4-understanding-where-to-get-the-kubernetes-artifacts">Step 4 — Understanding Where To Get The Kubernetes Artifacts</a> of this tutorial, we may pull the combined <code>hyperkube</code> image (<code>gcr.io/google_containers/hyperkube:v1.1.2</code>), use the unofficial Kubernetes Up and Running Images (<code>b.gcr.io/kuar/$binary:$version</code>) or <code>curl</code> and <code>load</code> each image individually.</p>

<p>As explained earlier, we will <code>curl</code> and <code>load</code> each image individually, see <a href="http://kubernetes.io/v1.1/docs/user-guide/images.html#pre-pulling-images">Pre-pulling images</a> for details on how pre-pulled images may affect the Kubelet setup. The following script combines the commands described in Step 4:</p>

<pre><code class="language-line_numbers">[label pull-kube-images.sh]

#!/bin/bash
tag=1.1.2
docker_wrapped_binaries=(
   &quot;kube-apiserver&quot;
   &quot;kube-controller-manager&quot;
   &quot;kube-scheduler&quot;
   #&quot;kube-proxy&quot;
)
temp_dir=&quot;$(mktemp -d -t 'kube-server-XXXX')&quot;

for binary in &quot;${docker_wrapped_binaries[@]}&quot;; do
  docker_tag=&quot;$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)&quot;
  echo &quot;downloading ${binary} ${docker_tag}&quot;
  curl -Lo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
  echo &quot;loading docker image&quot;
  docker load -i ${temp_dir}/${binary}.tar
  echo &quot;tagging docker image as ${binary} ${tag}&quot;
  docker tag -f &quot;gcr.io/google_containers/${binary}:${docker_tag}&quot; &quot;${binary}:${tag}&quot;
done

echo &quot;cleaning up temp dir&quot;
rm -rf &quot;${temp_dir}&quot;
</code></pre>

<p>You may modify this script to push these images to a local registry to simplify the provisioning of your cluster nodes, assuming a local Docker registry is available. In this case, our cluster nodes will easily pull the containers from the local registry and we do not need to include this script as part of our controller Droplet configuration. For this tutorial, we assume such registry is not available and embed the above script into the <code>cloud-config</code> file of every controller node.</p>

<p>After embedding this script in a Systemd unit, stripping the detailed output and specifying that the network needs to be up as well as the docker service needs to be successfully loaded, our <code>cloud-config</code> file will have the following snippet to pre-pull the Kubernetes images:</p>

<pre><code>[label cloud-config-controller.yaml pull-kube-images - snippet]
...
write-files:
  - path: /opt/bin/pull-kube-images.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      tag=1.1.2
      docker_wrapped_binaries=(
        &quot;kube-apiserver&quot;
        &quot;kube-controller-manager&quot;
        &quot;kube-scheduler&quot;
      )
      temp_dir=&quot;$(mktemp -d -t 'kube-server-XXXX')&quot;
      for binary in &quot;${docker_wrapped_binaries[@]}&quot;; do
        docker_tag=&quot;$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)&quot;
        curl -sLo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
        docker load -i ${temp_dir}/${binary}.tar
        docker tag -f &quot;gcr.io/google_containers/${binary}:${docker_tag}&quot; &quot;${binary}:${tag}&quot;
      done;
      rm -rf &quot;${temp_dir}&quot;;
      exit $?
...
coreos:
  units:
    - name: pull-kube-images.service
      command: start
      content: |
        [Unit]
        Description=Pull and load all Docker wrapped Kubernetes binaries
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target docker.service
        After=network-online.target docker.service
        [Service]
        ExecStart=/opt/bin/pull-kube-images.sh
        RemainAfterExit=yes
        Type=oneshot
...
</code></pre>

<p>In the above <code>cloud-config</code> snippet, we use the <code>write_files</code> directive to store the script on disk and make it executable as well as define a <code>oneshot</code> service to run that script. All our controller services will depend on the successful completion of this service. Oneshot services are flagged as successful by Systemd even after exiting.</p>

<h3 id="introduction-to-kubernetes-manifest-files:a91459261407d5d36808cf519d4f7594">Introduction to Kubernetes Manifest files</h3>

<p>Our kubelet will be used to manage our controller services within containers based on manifest files. In this section we will have a closer look at the structure of these files, you may refer to this section to understand manifest files better.</p>

<p>Kubernetes manifests can be written using YAML or JSON, but only YAML provides the ability to add comments. All of the manifests accepted and returned by the server have a schema, identified by the <code>kind</code> and <code>apiVersion</code> fields. These fields are required for proper decoding of the object.</p>

<p>The <code>kind</code> field takes a string that identifies the schema of an object, in our case we are writing a manifest to create Pod objects, as such we write <code>kind: Pod</code> in our Pod manifest.</p>

<p>The <code>apiVersion</code> field takes a string that identifies the API group &amp; version of the schema of an object. API groups will enable the Kubernetes API to be broken down into modular groups which can then be enabled/disabled individually, versioned separately as well as provide 3rd parties the ability to develop Kubernetes plug-ins without naming conflicts. At the time of writing there are only 2 API groups:</p>

<ul>
<li>The &laquo;core&raquo; group, which currently consists of the original monolithic Kubernetes v1 API. This API group is simply omitted and specified only by it&rsquo;s version, for example: <code>apiVersion: v1</code></li>
<li>The &laquo;extensions&raquo; group, which is the first API group introduced with v1.1. The <code>extensions</code> API group is still in <code>v1beta1</code> at the time of writing, as such this API group is specified as <code>apiVersion: extensions/v1beta1</code>. Resources within the <code>extensions</code> API group can be enabled or disabled through the <code>--runtime-config</code> flag passed on to the apiserver. For example, to disable <code>HorizontalPodAutoscalers</code> and <code>Jobs</code> we may set <code>--runtime-config=extensions/v1beta1/horizontalpodautoscalers=false,extensions/v1beta1/jobs=false</code>.</li>
</ul>

<p>For a more detailed explanation of the Kubernetes API, refer to <a href="http://kubernetes.io/v1.1/docs/api.html">the API documentation</a>.</p>

<p>Once the schema has been specified, Pod manifests mainly consist of the following key structures:</p>

<ul>
<li>A <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_objectmeta">metadata</a> structure for describing the pod and its labels</li>
<li>A <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec">spec</a> structure for describing volumes, and a list of containers that will run in the Pod.</li>
</ul>

<p>The <code>name</code> and <code>namespace</code> of the <code>metadata</code> structure are generally user provided. The <a href="http://kubernetes.io/v1.1/docs/user-guide/identifiers.html#names">name</a> has to be unique within the namespace specified. An empty <a href="http://kubernetes.io/v1.1/docs/user-guide/namespaces.html">namespace</a> is equivalent to the <code>default</code> namespace. In our case, we will scope our Pods related to the Kubernetes system environment to the <code>kube-system</code> namespace. We will combine all stateless controller service containers within one Pod and call it the <code>kube-controller</code> Pod.</p>

<p>Every Pod <code>spec</code> structure must at least have a list of <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_container">containers</a> with a minimum of 1 container. Each container in a pod must have a unique <code>name</code>. For example, the API service container may be named identical to it&rsquo;s binary name: <code>kube-apiserver</code>. Next, we may specify the <code>image</code> for the container, the <code>command</code> ran within the container (equivalent to the Docker image <code>entrypoint</code> array), the <code>args</code> passed on to the container process (equivalent to Docker image <code>cmd</code> array), the <code>volumeMounts</code>, &hellip;</p>

<p>A special requirement for our controller service containers is that they need to use the host&rsquo;s network namespace. This can be achieved by setting the <code>hostNetwork: true</code> for the <code>spec</code> structure of our controller Pod manifest.</p>

<p>Thus, this is how our Pod manifest for the controller services starts:</p>

<pre><code>[label kube-controller.yaml - header]

apiVersion: v1
kind: Pod
metadata:
  name: kube-controller
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: &quot;kube-apiserver&quot;
  ...
</code></pre>

<h3 id="controller-services-set-up-master-election:a91459261407d5d36808cf519d4f7594">Controller Services set up: Master Election</h3>

<p>By using a single Kubernetes controller Droplet, we have a single point of failure in our infrastructure. To ensure high availability we will need to run multiple controller nodes at some point. Every stateless Kubernetes component, such as the <code>kube-apiserver</code>, can be scaled across multiple controller nodes without concern. However, there are components which modify the state of the cluster, such as the <code>kube-controller-manager</code> and the <code>kube-scheduler</code>. Of these components, only one instance may modify the state at a time. To achieve this, we need to have a way to ensure only 1 instance for each of these components is running which is done by setting up master election per component.</p>

<p>At the time of writing, master election is not integrated within the <code>kube-controller-manager</code> and <code>kube-scheduler</code>, but is planned to be added in the future. Until then, a powerful and generic master election utility called the <a href="https://github.com/kubernetes/contrib/tree/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master">Podmaster</a> is recommended.</p>

<p>The Podmaster is a small (8MB) utility written in Go that uses Etcd&rsquo;s atomic <code>CompareAndSwap</code> functionality to implement master election. The first controller node to reach the Etcd cluster wins the race and becomes the master node for that service, marking itself as such with an expiring key identifying the service. The Podmaster will then periodically extend its service key. If a Podmaster finds the service key it monitors has <a href="https://github.com/kubernetes/contrib/blob/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master/podmaster.go#L89">expired</a>, it attempts to take over by setting itself as the new master for that service. If it is the current master, the Podmaster copies the manifest of its service into the manifests directory of its host, ensuring a single instance of the service is always running within the cluster. If the Podmaster finds it is no longer the master, it removes the manifest file from the manifests directory of its host, ensuring the kubelet will no longer run the service on that controller node.</p>

<p>A Podmaster instance may run for each service requiring master election, each instance takes the key identifying the service as well as a source manifest file and a destination manifest file. The Podmaster itself will run inside a container and a <a href="https://github.com/kubernetes/contrib/blob/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master/Dockerfile">Docker image</a> wrapping the Podmaster can be pulled from the Google Container Registry under the <code>gcr.io/google_containers/podmaster</code> repository. At the time of writing there is only 1 tag: <code>1.1</code>.</p>

<p>Even though we are only creating one controller node in this tutorial, we will set up the master election for the controller manager and scheduler service by storing their manifest files under the <code>/srv/kubernetes/manifests</code> path and letting Podmaster instances copy the manifest files to the <code>/etc/kubernetes/manifests</code> path on the elected master node.</p>

<p>In a single-controller deployment, the Podmaster will simply ensure that the <code>kube-scheduler</code> and <code>kube-controller-manager</code> run on the current node. In a multi-controller deployment, the Podmaster will be responsible for ensuring no additional instances are started, unless a machine dies, in which case the Podmaster will ensure new instances are started on one of the other controller nodes.</p>

<p>As our Podmasters depend on Kubernetes volumes, we will see the full Podmaster configurations after defining the Kubernetes volumes and <code>kube-apiserver</code> Pod manifests.</p>

<h3 id="controller-services-set-up-kubernetes-volumes:a91459261407d5d36808cf519d4f7594">Controller Services set up: Kubernetes Volumes</h3>

<p>At its core, a Kubernetes volume is just a directory, possibly with some data in it, which is accessible to the containers in a Pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used. Each volume type is backed by a Kubernetes volume plug-in.</p>

<p>For our controller services, we ensure the Pod is tied to our controller node, and we will use <code>HostPath</code> type volumes. <code>HostPath</code> type volumes represent a pre-existing file or directory on the host machine that is directly exposed to the container. They are generally used for system agents or other privileged things that are allowed to see the host machine.</p>

<p>We will place our API server certificates, once generated, in the following pre-defined path on the host:</p>

<ul>
<li>File: <code>/etc/kubernetes/ssl/ca.pem</code></li>
<li>File: <code>/etc/kubernetes/ssl/apiserver.pem</code></li>
<li>File: <code>/etc/kubernetes/ssl/apiserver-key.pem</code></li>
</ul>

<p>The address of the controller node is required to generate these API Server certificates. On Digital Ocean, this address is not known in advance. Therefore, we will generate the certificates and securely copy them after we provision our controller Droplet in a separate step, but we will prepare our <code>cloud-config</code> and the volumes defined in our Pod manifests to expect these certificates into these pre-defined host paths.</p>

<p>Every <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_volume">volume</a> requires a name which is unique within the Pod. The name is how we reference the volumes when we mount them into the Pod containers.</p>

<p>We define the following volume collection as part of our controller Pod manifest:</p>

<ol>
<li>a <code>HostPath</code> volume to provision the Kubernetes TLS Credentials from the parent directory <code>/etc/kubernetes/ssl</code>.</li>
<li>a <code>HostPath</code> volume for the list of &laquo;well-known&raquo; ca certificates - which, under CoreOS, is located under the read-only <code>/usr/share/ca-certificates</code> path.</li>
<li>a <code>HostPath</code> volume to provision as a source for manifest files for master election for the Podmaster <code>/srv/kubernetes/manifests</code></li>
<li>a <code>HostPath</code> volume for the Podmaster to access the host manifest folder <code>/etc/kubernetes/manifests</code>, where it will store the destination manifest files.</li>
</ol>

<pre><code>[label kube-controller.yaml - volumes]
spec:
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
    - hostPath:
        path: /srv/kubernetes/manifests
      name: manifest-src
    - hostPath:
        path: /etc/kubernetes/manifests
      name: manifest-dst
</code></pre>

<h3 id="controller-services-set-up-the-kube-apiserver:a91459261407d5d36808cf519d4f7594">Controller Services set up: The kube-apiserver</h3>

<p>The first controller service we will configure in our controller Pod manifest is the API server. The API server is where most of the magic happens. It is stateless by design and takes in API requests, processes them and stores the result in Etcd if needed, and then returns the result of the request. the API server will run on every controller Droplet and will be stored directly under the kubelet manifest folder.</p>

<p>In this tutorial we are using individual Docker images wrapping each Kubernetes binary, in our Pod manifest we specify this binary as the entrypoint for the containers through the <code>command</code> array together with all of its arguments.</p>

<p>Below is the <code>kube-apiserver</code> container spec for our controller Pod, we will go through each argument in detail right after:</p>

<pre><code>[label kube-controller.yaml - api-server container ]

  containers:
    - name: &quot;kube-apiserver&quot;
      image: &quot;kube-apiserver:1.1.2&quot;
      command: 
        - &quot;kube-apiserver&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--bind-address=0.0.0.0&quot;
        - &quot;--secure_port=443&quot;
        - &quot;--advertise-address=$public_ipv4&quot;
        - &quot;--service-cluster-ip-range=10.3.0.0/24&quot;
        - &quot;--service-node-port-range=30000-37000&quot;
        - &quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;
        - &quot;--allow-privileged=true&quot;
        - &quot;--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem&quot;
        - &quot;--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
        - &quot;--client-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
        - &quot;--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
      ports:
        - containerPort: 443
          hostPort: 443
          name: https
        - containerPort: 8080
          hostPort: 8080
          name: local
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
</code></pre>

<p>As highlighted in the <a href="#introduction-to-kubernetes-manifest-files">Kubernetes manifest files</a> section of this tutorial, our <code>kube-controller</code> Pod uses the host&rsquo;s network namespace and each container running within the Pod can reach the host services, such as the Etcd proxy, over localhost.</p>

<ul>
<li><p><code>--etcd-servers=[]</code>: By design, the <code>kube-apiserver</code> component is the only Kubernetes component communicating with Etcd. We specify the location of the Etcd cluster through the <code>--etcd-servers=[]</code> flag. This flag takes a comma separated list of etcd servers to watch. In this tutorial we bind an Etcd proxy for the cluster to the loopback interface of each Droplet, thus the Etcd cluster can be reached through <code>http://127.0.0.1:2379</code>. Also note that by default, Kubernetes objects are stored under the <code>/registry</code> key in Etcd. We could prefix this path by also setting the <code>--etcd-prefix=&quot;/foo&quot;</code> flag, but wont do this for this tutorial.</p></li>

<li><p><code>--bind-address=0.0.0.0</code>: The IP address on which the API server listens for requests. We explicitely configure our API server to listen on all interfaces of the host.</p></li>

<li><p><code>--secure-port=443</code>: To enable HTTPS with authentication and authorization we need to set this flag.</p></li>
</ul>

<!-- TODO: use $private_ipv4? -->

<ul>
<li><p><code>--advertise-address=$public_ipv4</code>: The IP address on which to advertise the apiserver to members of the cluster. This address must be reachable by the rest of the cluster. If blank, the <code>--bind-address</code> will be used, which would not work in our set up.</p></li>

<li><p><code>--service-cluster-ip-range=10.3.0.0/24</code>: A required CIDR notation IP range from which to assign service cluster IPs. See the <a href="#running-the-kubelet-in-standalone-mode">Running the kubelet in standalone mode</a> section for more details on how this is used within Kubernetes, we use <code>10.3.0.0/24</code> for Kubernetes Services within this Tutorial. Modify these values to mirror your own configuration.</p></li>

<li><p><code>--service-node-port-range=30000-37000</code>: A port range to reserve for services with NodePort visibility. If we do not specify this range we will not be able to run some of the Kubernetes <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_servicespec">service</a> examples using <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_serviceport">nodePort</a>.</p></li>

<li><p><code>--admission-control=[]</code>: In Kubernetes, API requests need to pass through a chain of <a href="http://kubernetes.io/v1.1/docs/admin/admission-controllers.html">admission controllers</a> after authentication and authorization but prior to being accepted and executed. Admission controllers are chained plug-ins, many advanced features in Kubernetes require an admission control plug-in to be enabled in order to properly support the feature. As a result, a Kubernetes API server that is not properly configured with the right set of admission control plug-ins is an incomplete server and will not support all the features you expect. The recommended set of admission controllers for Kubernetes 1.0 is <code>NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota</code>. We would like to highlight the <code>NamespaceLifecycle</code> plug-in which ensures that API requests in a non-existant Namespace are rejected. Due to this, we will be required to manually create the <code>kube-system</code> namespace used by our controller services once our <code>kube-apiserver</code> is available or our other nodes won&rsquo;t be able to discover them.</p></li>

<li><p><code>--allow-privileged=true</code>: We have to explicitely allow privileged containers to run in our cluster.</p></li>

<li><p><code>--tls-cert-file=&quot;etc/kubernetes/ssl/apiserver.pem&quot;</code>: The certificate used for SSL/TLS connections to the API Server. We will generate The apiserver certificate containing host identities (DNS name, IP, ..) and securely copy it to our controller Droplet in a separate step. If HTTPS serving is enabled, and <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> are not provided, a self-signed certificate and key are generated for the public address and saved to <code>/var/run/kubernetes</code>. If you intend to use this approach, ensure to provide a volume for <code>/var/run/kubernetes/</code> as well.</p></li>

<li><p><code>--tls-private-key-file=&quot;/etc/kubernetes/ssl/apiserver-key.pem&quot;</code>: The API Server private key matching the <code>--tls-cert-file</code> we generated.</p></li>

<li><p><code>--client-ca-file=&quot;/etc/kubernetes/ssl/ca.pem&quot;</code>: The trusted certificate authority, Kubernetes will check all incoming HTTPs request for a client certificate signed by this trusted CA. Any request presenting a client certificate signed by one of the authorities in the <code>client-ca-file</code> is authenticated with an identity corresponding to the CommonName of the client certificate.</p></li>

<li><p><code>--service-account-key-file=&quot;/etc/kubernetes/ssl/apiserver-key.pem&quot;</code>: used to verify <a href="http://kubernetes.io/v1.1/docs/user-guide/service-accounts.html">ServiceAccount</a> tokens. We explicitely set this to the same private key as our <code>--tls-private-key-file</code> flag. If - unspecified, <code>--tls-private-key-file</code> is also used.</p></li>
</ul>

<p>Refer to the full <a href="http://kubernetes.io/v1.1/docs/admin/kube-apiserver.html">kube-apiserver reference</a> for a full overview of all API server flags.</p>

<h3 id="controller-services-set-up-the-kube-system-namespace:a91459261407d5d36808cf519d4f7594">Controller Services set up: The kube-system namespace</h3>

<p>As soon as the <code>kube-apiserver</code> is available we need to create the <code>kube-system</code> namespace used by our controller services or our cluster nodes won&rsquo;t be able to discover them. In this section we define the Systemd unit responsible for this.</p>

<p>We wait until the <code>kube-apiserver</code> service has started, the same way as our <code>kube-proxy</code> service was configured to wait:</p>

<pre><code>ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl -s http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
</code></pre>

<p>The command to create the namespace using the Kubernetes API is:</p>

<pre><code class="language-command">curl -XPOST -d'{&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Namespace&quot;,&quot;metadata&quot;:{&quot;name&quot;:&quot;kube-system&quot;}}' &quot;http://127.0.0.1:8080/api/v1/namespaces&quot;
</code></pre>

<p>We are passing in a the Manifest file as a JSON string in this case.</p>

<p>Putting the command into a oneshot Systemd unit which depends on a successful start of the kubelet service, gives us the following unit definition:</p>

<pre><code>[label cloud-config-controller.yaml create kube-system namespace - snippet]

coreos:
  units:
    - name: &quot;create-kube-system-ns.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Create the kube-system namespace
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl -s http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
        ExecStart=/usr/bin/curl -XPOST -d'{&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Namespace&quot;,&quot;metadata&quot;:{&quot;name&quot;:&quot;kube-system&quot;}}' &quot;http://127.0.0.1:8080/api/v1/namespaces&quot;
        RemainAfterExit=yes
        Type=oneshot
</code></pre>

<h3 id="controller-services-set-up-the-kube-podmaster:a91459261407d5d36808cf519d4f7594">Controller Services set up: The kube-podmaster</h3>

<p>In this section we add our Podmaster containers to our <code>kube-controller</code> Pod manifest. As mentioned in the <a href="#controller-services-set-up-master-election">Controller Services: Master Election</a> section of this tutorial, the <code>kube-scheduler</code> and <code>kube-controller-manager</code> services require master election. We will create 1 Podmaster container for each component requiring master election and define the Pod manifests in the following sections.</p>

<p>We will go into the contents of the <code>kube-scheduler.yaml</code> Pod manifest and <code>kube-controller-manager.yaml</code> Pod manifest after finalizing this <code>kube-controller.yaml</code> Pod manifest.</p>

<p>As the <code>kube-controller</code> Pod shares the host network, our Podmaster containers can reach the Etcd cluster via the localhost Etcd proxy. To ease the setup, we overwrite the hostname the Podmaster stores in the master reservation with the Droplet public IP by setting the <code>--whoami</code> flag. The Droplet IP is always routable without the need for DNS services. We mount the <code>manifest-src</code> volume as a read only volume within the Podmaster containers. The <code>manifest-dst</code> volume is the path monitored by the Kubelet and needs to be writable by the Podmaster.</p>

<p>Here is the Podmaster container managing the master election for the <code>kube-scheduler</code> service</p>

<!-- TODO: should this be $private_ipv4?-->

<pre><code>[label kube-controller.yaml - scheduler-elector - snippet ]

  containers:
    - name: &quot;scheduler-elector&quot;
      image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
      args:
        - &quot;--whoami=$public_ipv4&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--key=scheduler&quot;
        - &quot;--source-file=/src/manifests/kube-scheduler.yaml&quot;
        - &quot;--dest-file=/dst/manifests/kube-scheduler.yaml&quot;
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
</code></pre>

<p>For the <code>kube-scheduler</code> our Podmaster sets the value of the <code>scheduler</code> key in Etcd to record which controller Droplet is the master. We point this Podmaster to the <code>kube-scheduler</code> Pod manifest source and destination files.</p>

<p>For the <code>kube-controller-manager</code> the master elector looks almost identical, apart from the key, source and destination manifest files. The key used for the <code>kube-controller-manager</code> is <code>controller</code> and the <code>kube-controller-manager.yaml</code> Pod manifest file is used instead.</p>

<!-- TODO: should this be $private_ipv4?-->

<pre><code>[label kube-controller.yaml - controller-manager-elector - snippet ]

  containers
    - name: &quot;controller-manager-elector&quot;
      image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
      args:
        - &quot;--whoami=$public_ipv4&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--key=controller&quot;
        - &quot;--source-file=/src/manifests/kube-controller-manager.yaml&quot;
        - &quot;--dest-file=/dst/manifests/kube-controller-manager.yaml&quot;
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
</code></pre>

<h3 id="combining-kube-controller-pod-manifest-snippets:a91459261407d5d36808cf519d4f7594">Combining kube-controller Pod manifest snippets</h3>

<p>Combining all the <code>kube-controller.yaml</code> snippets above into a single kube-controller Pod manifest:</p>

<pre><code class="language-line_numbers">[label kube-controller.yaml]

apiVersion: v1
kind: Pod
metadata:
  name: kube-controller
  namespace: kube-system
spec:
  hostNetwork: true
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
    - hostPath:
        path: /srv/kubernetes/manifests
      name: manifest-src
    - hostPath:
        path: /etc/kubernetes/manifests
      name: manifest-dst
  containers:
    - name: &quot;kube-apiserver&quot;
      image: &quot;kube-apiserver:1.1.2&quot;
      command: 
        - &quot;kube-apiserver&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--bind-address=0.0.0.0&quot;
        - &quot;--secure_port=443&quot;
        - &quot;--advertise-address=$public_ipv4&quot;
        - &quot;--service-cluster-ip-range=10.3.0.0/24&quot;
        - &quot;--service-node-port-range=30000-37000&quot;
        - &quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;
        - &quot;--allow-privileged=true&quot;
        - &quot;--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem&quot;
        - &quot;--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
        - &quot;--client-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
        - &quot;--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
      ports:
        - containerPort: 443
          hostPort: 443
          name: https
        - containerPort: 8080
          hostPort: 8080
          name: local
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
    - name: &quot;scheduler-elector&quot;
      image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
      args:
        - &quot;--whoami=$public_ipv4&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--key=scheduler&quot;
        - &quot;--source-file=/src/manifests/kube-scheduler.yaml&quot;
        - &quot;--dest-file=/dst/manifests/kube-scheduler.yaml&quot;
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
    - name: &quot;controller-manager-elector&quot;
      image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
      args:
        - &quot;--whoami=$public_ipv4&quot;
        - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
        - &quot;--key=controller&quot;
        - &quot;--source-file=/src/manifests/kube-controller-manager.yaml&quot;
        - &quot;--dest-file=/dst/manifests/kube-controller-manager.yaml&quot;
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
</code></pre>

<h3 id="the-kube-controller-pod-pre-conditions:a91459261407d5d36808cf519d4f7594">The kube-controller Pod Pre-conditions</h3>

<p>The <code>kube-apiserver</code> requires the TLS assets to be in place, if these are not in place the container will die after starting. The <code>kubelet</code> will create a new container every 5 minutes until the container stays up. To keep the error logs and dead containers minimal during first boot, we prefer to hold off on putting the <code>kube-controller</code> Pod manifest in the kubelet manifest directory until the <code>kube-apiserver</code> TLS assets are available. We will use the <code>write_files</code> directive to create the <code>kube-controller</code> Pod manifest under the <code>/srv/kubernetes/manifests/</code> Path until then.</p>

<p>We will use a Systemd unit to monitor the <code>/etc/kubernetes/ssl</code> path and copy the <code>kube-controller</code> manifest file to the kubelet manifest directory as soon as the TLS assets are detected.</p>

<p>The following loop sleeps until all 3 TLS assets required on the controller node, are available:</p>

<pre><code class="language-command">until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo &quot;waiting for TLS assets...&quot;;sleep 5; done
</code></pre>

<p>Putting this into a oneshot Systemd unit which starts as soon as the kubelet is ready, gives us the following unit definition:</p>

<pre><code>[label cloud-config-controller.yaml start controller pod - snippet]
...
coreos:
  units:
    - name: &quot;tls-ready.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/bin/bash -c &quot;until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \&quot;waiting for TLS assets...\&quot;;sleep 5; done&quot;
        ExecStart=/usr/bin/cp /srv/kubernetes/manifests/kube-controller.yaml /etc/kubernetes/manifests/
...
</code></pre>

<p>We will now proceed with defining the master elected <code>kube-scheduler</code> and <code>kube-controller-manager</code> Pod manifests which will also be stored under the <code>/srv/kubernetes/manifests</code> path.</p>

<h3 id="controller-services-set-up-the-kube-controller-manager-pod-manifest:a91459261407d5d36808cf519d4f7594">Controller Services set up: The kube-controller-manager Pod manifest</h3>

<p>The controller manager embeds the core control loops within Kubernetes such as the replication controller, endpoints controller, namespace controller and serviceaccount controller. In short, a control loop watches the shared state of the cluster through the <code>kube-apiserver</code> and makes changes attempting to move the current state towards the desired state.</p>

<p>For example, if you increased the replica count for a replication controller, the controller manager would generate a scale up event, which would cause a new Pod to get scheduled in the cluster. The controller manager communicates with the API to submit these events.</p>

<p>We start writing this Pod manifest in exactly the same way as our <code>kube-controller</code> Pod manifest, but with it&rsquo;s own unique name in the <code>kube-system</code> namespace:</p>

<pre><code>[label /srv/kubernetes/manifests/kube-controller-manager.yaml - header - snippet]

apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
    - name: &quot;kube-controller-manager&quot;
...
</code></pre>

<p>This Pod also shares the network with the host (<code>hostNetwork: true</code>), allowing the containers running within to access the <code>kube-apiserver</code> through localhost as well as exposing themselves to the kubelet over localhost.</p>

<p>We define volumes for the ssl certificates and list off &laquo;well-known&raquo; ca certificates stored on the host so we can mount these into the Pod containers:</p>

<!-- TODO: Do we need the "well-known" certificates mount? It seems it is not used?? -->

<pre><code>[label /srv/kubernetes/manifests/kube-controller-manager.yaml - volumes - snippet]

spec:
  ...
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
</code></pre>

<p>Our <code>kube-controller-manager</code> is called with the following arguments:</p>

<pre><code>kube-controller-manager \
  --master=http://127.0.0.1:8080 \
  --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
  --root-ca-file=/etc/kubernetes/ssl/ca.pem
</code></pre>

<p>We provide the address of the <code>kube-apiserver</code> via the <code>--master=http://127.0.0.1:8080</code> flag. We provide the private key (to sign service account tokens) and our Kubernetes cluster root CA certificate for inclusion in service account tokens via the <code>--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem</code> and <code>--root-ca-file=/etc/kubernetes/ssl/ca.pem</code> flags respectively.</p>

<p>We are also adding a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_probe">livenessProbe</a> to our Pod manifest. This is a diagnostic performed periodically by the kubelet on a container. The LivenessProbe hints to the kubelet when a container is <a href="http://kubernetes.io/v1.1/docs/user-guide/pod-states.html#container-probes">unhealthy</a>. If the LivenessProbe fails, the kubelet will kill the container and the container will be subjected to its <code>RestartPolicy</code>. If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>. The default state of Liveness before the initial delay is <code>Success</code>. The state of Liveness for a container when no probe is provided is assumed to be <code>Success</code>.</p>

<p>The <code>httpGet</code> handler used in our livenessProbe performs an HTTP Get against the provided IP address on a specified port and path expecting on success that the response has a status code greater than or equal to 200 and less than 400. Note the default port used by <code>kube-controller-manager</code> is always <code>10252</code> and the Kubernetes <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/pkg/healthz">&laquo;healtz&raquo; package</a> registers a handler on the &lsquo;/healthz&rsquo; path , that serves 200s.</p>

<p>This gives us the following container spec for our <code>kube-controller-manager</code> container:</p>

<pre><code>[label /srv/kubernetes/manifests/kube-controller-manager.yaml - containers - snippet ]

spec:
  ...
  containers:
    - name: &quot;kube-controller-manager&quot;
      image: &quot;kube-controller-manager:1.1.2&quot;
      command: 
        - &quot;kube-controller-manager&quot;
        - &quot;--master=http://127.0.0.1:8080&quot;
        - &quot;--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
        - &quot;--root-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10252
        initialDelaySeconds: 15
        timeoutSeconds: 1
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
</code></pre>

<p>Refer to <a href="http://kubernetes.io/v1.1/docs/admin/kube-controller-manager.html">the official kube-controller-manager reference</a> for a full overview of all arguments.</p>

<p>Combining the above snippets together, the full <code>kube-controller-manager.yaml</code> Pod manifest file will look as follows:</p>

<pre><code class="language-line_numbers">[label /srv/kubernetes/manifests/kube-controller-manager.yaml]

apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  hostNetwork: true
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
  containers:
    - name: &quot;kube-controller-manager&quot;
      image: &quot;kube-controller-manager:1.1.2&quot;
      command: 
        - &quot;kube-controller-manager&quot;
        - &quot;--master=http://127.0.0.1:8080&quot;
        - &quot;--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
        - &quot;--root-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10252
        initialDelaySeconds: 15
        timeoutSeconds: 1
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
</code></pre>

<h3 id="controller-services-set-up-the-kube-scheduler-pod-manifest:a91459261407d5d36808cf519d4f7594">Controller Services set up: The kube-scheduler Pod manifest</h3>

<p>The scheduler is the last major piece of our control services. It monitors the API for unscheduled pods, finds them a machine to run on, and communicates the decision back to the API.</p>

<p>The full <code>kube-scheduler.yaml</code> Pod manifest file introduces no new concepts, does liveness probes on the scheduler default port of <code>10251</code>, doesn&rsquo;t require any volumes and looks as follows:</p>

<pre><code class="language-line_numbers">[label /srv/kubernetes/manifests/kube-scheduler.yaml]

apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
    - name: &quot;kube-scheduler&quot;
      image: &quot;kube-scheduler:1.1.2&quot;
      command:
        - &quot;kube-scheduler&quot;
        - &quot;--master=http://127.0.0.1:8080&quot;
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: 10251
        initialDelaySeconds: 15
        timeoutSeconds: 1
</code></pre>

<p>Refer to <a href="http://kubernetes.io/v1.1/docs/admin/kube-scheduler.html">the official kube-scheduler reference</a> for a full overview of all arguments.</p>

<h3 id="embedding-all-pod-manifests-into-the-controller-cloud-config:a91459261407d5d36808cf519d4f7594">Embedding all Pod manifests into the Controller cloud-config</h3>

<p>We will embed the Pod manifest files constructed above into our controller <code>cloud-config</code> file and store each manifest under the following paths:</p>

<ol>
<li>/srv/kubernetes/manifests/kube-scheduler.yaml</li>
<li>/srv/kubernetes/manifests/kube-controller-manager.yaml</li>
<li>/srv/kubernetes/manifests/kube-controller.yaml</li>
</ol>

<p>This is achieved through the <code>write-files</code> directive highlighted earlier.</p>

<pre><code class="language-line_numbers">[label cloud-config-controller.yaml pod-manifests - snippet]
#cloud-config

write-files:
  - path: &quot;/srv/kubernetes/manifests/kube-scheduler.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
          - name: &quot;kube-scheduler&quot;
            image: &quot;kube-scheduler:1.1.2&quot;
            command:
              - &quot;kube-scheduler&quot;
              - &quot;--master=http://127.0.0.1:8080&quot;
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 1
  - path: &quot;/srv/kubernetes/manifests/kube-controller-manager.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
        containers:
          - name: &quot;kube-controller-manager&quot;
            image: &quot;kube-controller-manager:1.1.2&quot;
            command:
              - &quot;kube-controller-manager&quot;
              - &quot;--master=http://127.0.0.1:8080&quot;
              - &quot;--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
              - &quot;--root-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 1
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
  - path: &quot;/srv/kubernetes/manifests/kube-controller.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller
        namespace: kube-system
      spec:
        hostNetwork: true
        volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /srv/kubernetes/manifests
            name: manifest-src
          - hostPath:
              path: /etc/kubernetes/manifests
            name: manifest-dst
        containers:
          - name: &quot;kube-apiserver&quot;
            image: &quot;kube-apiserver:1.1.2&quot;
            command: 
              - &quot;kube-apiserver&quot;
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--bind-address=0.0.0.0&quot;
              - &quot;--secure_port=443&quot;
              - &quot;--advertise-address=$public_ipv4&quot;
              - &quot;--service-cluster-ip-range=10.3.0.0/24&quot;
              - &quot;--service-node-port-range=30000-37000&quot;
              - &quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;
              - &quot;--allow-privileged=true&quot;
              - &quot;--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem&quot;
              - &quot;--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
              - &quot;--client-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
              - &quot;--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
            ports:
              - containerPort: 443
                hostPort: 443
                name: https
              - containerPort: 8080
                hostPort: 8080
                name: local
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
          - name: &quot;scheduler-elector&quot;
            image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
            args:
              - &quot;--whoami=$public_ipv4&quot;
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--key=scheduler&quot;
              - &quot;--source-file=/src/manifests/kube-scheduler.yaml&quot;
              - &quot;--dest-file=/dst/manifests/kube-scheduler.yaml&quot;
            volumeMounts:
              - mountPath: /src/manifests
                name: manifest-src
                readOnly: true
              - mountPath: /dst/manifests
                name: manifest-dst
          - name: &quot;controller-manager-elector&quot;
            image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
            args:
              - &quot;--whoami=$public_ipv4&quot;
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--key=controller&quot;
              - &quot;--source-file=/src/manifests/kube-controller-manager.yaml&quot;
              - &quot;--dest-file=/dst/manifests/kube-controller-manager.yaml&quot;
            volumeMounts:
              - mountPath: /src/manifests
                name: manifest-src
                readOnly: true
              - mountPath: /dst/manifests
                name: manifest-dst

</code></pre>

<h3 id="the-final-controller-cloud-config-with-all-coreos-units:a91459261407d5d36808cf519d4f7594">The Final Controller cloud-config with all CoreOS Units</h3>

<p>To finally create the controller Droplet, we will combine all above <code>cloud-config</code> snippets into a single <code>cloud-config</code> file:</p>

<ol>
<li><code>write-files</code> snippets:

<ol>
<li><code>/opt/bin/pull-kube-images.sh</code> script to pre-load the Kubernetes docker images</li>
<li><code>/srv/kubernetes/manifests/kube-scheduler.yaml</code> Pod manifest source for the <code>kube-scheduler</code></li>
<li><code>/srv/kubernetes/manifests/kube-controller-manager.yaml</code> Pod manifest source for the <code>kube-controller-manager</code></li>
<li><code>/etc/kubernetes/manifests/kube-controller.yaml</code> Pod manifest to start the <code>kube-apiserver</code>, <code>controller-manager-elector</code> and <code>scheduler-elector</code></li>
</ol></li>
<li><code>etcd2.service</code> snippet to start a local Etcd proxy, notice the <code>ETCD_PEER</code> placeholder.</li>
<li><code>flanneld.service</code> snippet to start the overlay network daemon with a drop-in to configure the network subnet</li>
<li><code>docker.service</code> drop-in snippet to add flannel dependency</li>
<li><code>kubelet.service</code> snippet running the kubelet in standalone mode</li>
<li><code>kube-proxy.service</code> snippet running the <code>kube-proxy</code> service</li>
<li><code>kube-pull-images.service</code> snippet running the script to pre-load the Kubernetes docker images</li>
<li><code>create-kube-system.service</code> snippet creating the <code>kube-system</code> namespace as soon as the API server is available</li>
</ol>

<p>Several of these services depend on the TLS assets, which we generate as soon as the IP addresses are known for our Droplet.</p>

<p>In a multi-controller set-up, every controller node may be created using this <code>cloud-config</code>, although the <code>flanneld</code> drop-in and <code>create-kube-system.service</code> unit only need to be ran once within the cluster and are not required on subsequent controller nodes.</p>

<!-- TODO: test if including the flannel config commands twice cause the other controller nodes to fail? -->

<p>As we are running a single controller node, we are also turning off CoreOS updates and reboots in our <code>cloud-config</code>.</p>

<pre><code class="language-line_numbers">[label cloud-config-controller.yaml]
#cloud-config

write-files:
  - path: /opt/bin/pull-kube-images.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      tag=1.1.2
      docker_wrapped_binaries=(
        &quot;kube-apiserver&quot;
        &quot;kube-controller-manager&quot;
        &quot;kube-scheduler&quot;
      )
      temp_dir=&quot;$(mktemp -d -t 'kube-server-XXXX')&quot;
      for binary in &quot;${docker_wrapped_binaries[@]}&quot;; do
        docker_tag=&quot;$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)&quot;
        curl -sLo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
        docker load -i ${temp_dir}/${binary}.tar
        docker tag -f &quot;gcr.io/google_containers/${binary}:${docker_tag}&quot; &quot;${binary}:${tag}&quot;
      done;
      rm -rf &quot;${temp_dir}&quot;;
      exit $?
  - path: &quot;/srv/kubernetes/manifests/kube-scheduler.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
          - name: &quot;kube-scheduler&quot;
            image: &quot;kube-scheduler:1.1.2&quot;
            command:
              - &quot;kube-scheduler&quot;
              - &quot;--master=http://127.0.0.1:8080&quot;
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10251
              initialDelaySeconds: 15
              timeoutSeconds: 1
  - path: &quot;/srv/kubernetes/manifests/kube-controller-manager.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        hostNetwork: true
        volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
        containers:
          - name: &quot;kube-controller-manager&quot;
            image: &quot;kube-controller-manager:1.1.2&quot;
            command:
              - &quot;kube-controller-manager&quot;
              - &quot;--master=http://127.0.0.1:8080&quot;
              - &quot;--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
              - &quot;--root-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
            livenessProbe:
              httpGet:
                host: 127.0.0.1
                path: /healthz
                port: 10252
              initialDelaySeconds: 15
              timeoutSeconds: 1
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
  - path: &quot;/srv/kubernetes/manifests/kube-controller.yaml&quot;
    permissions: &quot;0644&quot;
    owner: &quot;root&quot;
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller
        namespace: kube-system
      spec:
        hostNetwork: true
        volumes:
          - hostPath:
              path: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /srv/kubernetes/manifests
            name: manifest-src
          - hostPath:
              path: /etc/kubernetes/manifests
            name: manifest-dst
        containers:
          - name: &quot;kube-apiserver&quot;
            image: &quot;kube-apiserver:1.1.2&quot;
            command:
              - &quot;kube-apiserver&quot; 
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--bind-address=0.0.0.0&quot;
              - &quot;--secure_port=443&quot;
              - &quot;--advertise-address=$public_ipv4&quot;
              - &quot;--service-cluster-ip-range=10.3.0.0/24&quot;
              - &quot;--service-node-port-range=30000-37000&quot;
              - &quot;--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota&quot;
              - &quot;--allow-privileged=true&quot;
              - &quot;--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem&quot;
              - &quot;--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
              - &quot;--client-ca-file=/etc/kubernetes/ssl/ca.pem&quot;
              - &quot;--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem&quot;
            ports:
              - containerPort: 443
                hostPort: 443
                name: https
              - containerPort: 8080
                hostPort: 8080
                name: local
            volumeMounts:
              - mountPath: /etc/kubernetes/ssl
                name: ssl-certs-kubernetes
                readOnly: true
              - mountPath: /etc/ssl/certs
                name: ssl-certs-host
                readOnly: true
          - name: &quot;scheduler-elector&quot;
            image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
            args:
              - &quot;--whoami=$public_ipv4&quot;
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--key=scheduler&quot;
              - &quot;--source-file=/src/manifests/kube-scheduler.yaml&quot;
              - &quot;--dest-file=/dst/manifests/kube-scheduler.yaml&quot;
            volumeMounts:
              - mountPath: /src/manifests
                name: manifest-src
                readOnly: true
              - mountPath: /dst/manifests
                name: manifest-dst
          - name: &quot;controller-manager-elector&quot;
            image: &quot;gcr.io/google_containers/podmaster:1.1&quot;
            args:
              - &quot;--whoami=$public_ipv4&quot;
              - &quot;--etcd-servers=http://127.0.0.1:2379&quot;
              - &quot;--key=controller&quot;
              - &quot;--source-file=/src/manifests/kube-controller-manager.yaml&quot;
              - &quot;--dest-file=/dst/manifests/kube-controller-manager.yaml&quot;
            volumeMounts:
              - mountPath: /src/manifests
                name: manifest-src
                readOnly: true
              - mountPath: /dst/manifests
                name: manifest-dst
coreos:
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: &quot;etcd-01=ETCD_PEER&quot;
  units:
    - name: &quot;etcd2.service&quot;
      command: &quot;start&quot;
    - name: &quot;flanneld.service&quot;
      command: &quot;start&quot;
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{&quot;Network&quot;:&quot;10.2.0.0/16&quot;, &quot;Backend&quot;: {&quot;Type&quot;: &quot;vxlan&quot;}}'
    - name: &quot;docker.service&quot;
      command: &quot;start&quot;
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
    - name: &quot;pull-kube-images.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Pull and load all Docker wrapped Kubernetes binaries
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target docker.service
        After=network-online.target docker.service
        [Service]
        ExecStart=/opt/bin/pull-kube-images.sh
        RemainAfterExit=yes
        Type=oneshot
    - name: &quot;kube-proxy.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        # wait for kube-apiserver to be up and ready
        ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl -s http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
        ExecStart=/opt/bin/kube-proxy \
        --master=http://127.0.0.1:8080 \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        TimeoutStartSec=10
        Restart=always
        RestartSec=10
    - name: &quot;kube-kubelet.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service
        After=docker.service
        [Service]
        ExecStartPre=-/bin/bash -c &quot;mkdir -p /etc/kubernetes/{manifests,ssl}&quot;
        ExecStart=/usr/bin/kubelet \
        --api-servers=http://127.0.0.1:8080 \
        --register-node=false \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local
        Restart=always
        RestartSec=10
    - name: &quot;tls-ready.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/bin/bash -c &quot;until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \&quot;waiting for TLS assets...\&quot;;sleep 5; done&quot;
        ExecStart=/usr/bin/cp /srv/kubernetes/manifests/kube-controller.yaml /etc/kubernetes/manifests/
    - name: &quot;create-kube-system-ns.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Create the kube-system namespace
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        ExecStartPre=/bin/bash -c &quot;until /usr/bin/curl -s http://127.0.0.1:8080; do echo \&quot;waiting for API server to come online...\&quot;; sleep 3; done&quot;
        ExecStart=/usr/bin/curl -XPOST -d'{&quot;apiVersion&quot;:&quot;v1&quot;,&quot;kind&quot;:&quot;Namespace&quot;,&quot;metadata&quot;:{&quot;name&quot;:&quot;kube-system&quot;}}' &quot;http://127.0.0.1:8080/api/v1/namespaces&quot;
        RemainAfterExit=yes
        Type=oneshot
  update:
    group: alpha
    reboot-strategy: off
</code></pre>

<p><a href="https://coreos.com/validate">Validate</a> your cloud-config file, then create your <code>kube-controller-01</code> droplet with the following Doctl command. :</p>

<p>Ensure your <code>ETCD_PEER</code> environment variable is still set from the <a href="#step-2-provisioning-the-data-storage-back-end">Step 2 — Provisioning The Data Storage Back End</a> section of this tutorial:</p>

<pre><code class="language-command">$ echo $ETCD_PEER
http://10.129.69.201:2380
</code></pre>

<p>If not - set it to the <code>private_ip</code> of your single node ECTD cluster:</p>

<pre><code class="language-command">export ETCD_PEER=`doctl -f json d f etcd-01.$region | jq -r '.networks.v4[] | select(.type == &quot;private&quot;)  | &quot;http://\(.ip_address):2380&quot;'`
</code></pre>

<p>Substitute the <code>ETCD_PEER</code> placeholder from above <code>cloud-config-controller.yaml</code> template file with the following command:</p>

<pre><code class="language-command">sed -e &quot;s|ETCD_PEER|${ETCD_PEER}|g;&quot; cloud-config-controller.yaml &gt; kube-controller.yaml
</code></pre>

<p>And send the command to create the droplet:</p>

<pre><code class="language-command">doctl d c --wait-for-active \
    -i &quot;CoreOS-alpha&quot; \
    -s 512mb \
    -r &quot;$region&quot; \
    -p \
    -k k8s-key \
    -uf kube-controller.yaml kube-controller-01
</code></pre>

<!-- TODO: Droplet size? -->

<p><strong>Note</strong>: running <code>free -m</code> on a <code>512mb</code> Droplet shows only 12mb free memory after all controller services have started, it may be better to use a <code>1024mb</code> droplet to fully test Kubernetes.</p>

<p>We are waiting for the droplet to be flagged as active before proceeding. Once the Doctl command completes, the Droplet configuration is returned. As it usually takes more time for the Droplet to return its public and private ip addresses, we need to re-query the Droplet configuration. We will cache the json string returned in the <code>$CONTROLLER_JSON</code> environment variable for subsequent commands:</p>

<pre><code class="language-command">CONTROLLER_JSON=`doctl -f 'json' d f kube-controller-01.$region`
</code></pre>

<p>We parse the private and public IPs out as explained in the <a href="#working-with-doctl-responses">Working with doctl responses</a> section of this tutorial.</p>

<!--

Combined:
```command
read CONTROLLER_PUBLIC_IP2 CONTROLLER_PRIVATE_IP2 <<<$(echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == "public"), select(.type == "private") | .ip_address')
```
-->

<pre><code class="language-command">CONTROLLER_PUBLIC_IP=`echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == &quot;public&quot;) | .ip_address'`
CONTROLLER_PRIVATE_IP=`echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == &quot;private&quot;) | .ip_address'`
</code></pre>

<p>Confirm values were populated correctly:</p>

<pre><code class="language-command">echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
</code></pre>

<p>You may monitor the initialization process driven by <code>cloud-config</code> by connecting to the Droplet:</p>

<pre><code class="language-command">ssh core@$CONTROLLER_PUBLIC_IP
</code></pre>

<p>and follow the <code>oem-cloudinit</code> service running the <code>cloud-config</code>:</p>

<pre><code class="language-command">journalctl -u oem-cloudinit -f
</code></pre>

<p>Once the <code>oem-cloudinit</code> service has reached the &laquo;tls-ready.service&raquo; it will wait for our actions. <code>CTRL+C</code> and confirm Etcd proxy is running:</p>

<pre><code class="language-command">systemctl status etcd2
</code></pre>

<p>Confirm Flannel service started</p>

<pre><code class="language-command">systemctl status flanneld
</code></pre>

<p>If Flannel started, confirm it was able to retrieve its configuration from Etcd:</p>

<pre><code class="language-command">cat /run/flannel/subnet.env
</code></pre>

<p>The Docker daemon options for the overlay network generated by Flannel are stored under <code>/run/flannel_docker_opts.env</code>:</p>

<pre><code class="language-command">cat /run/flannel_docker_opts.env
</code></pre>

<p>Confirm all services are running:</p>

<pre><code class="language-command">systemctl status tls-ready
systemctl status docker
systemctl status pull-kube-images
</code></pre>

<p>confirm the docker images loaded the kube images have all been loaded by running</p>

<pre><code class="language-command">docker images | grep kube
</code></pre>

<p>confirm all files have been written to disk:</p>

<pre><code class="language-command">ls -l /opt/bin/
ls -l /srv/kubernetes/manifests/
</code></pre>

<p>Monitor when the kubelet will launch the containers (which will happen as soon as we copy the TLS assets)</p>

<pre><code class="language-command">watch -n 1 'docker ps --format=&quot;table {{.Image}}\t{{.ID}}\t{{.Status}}\t{{.Ports}}&quot; -a' 
</code></pre>

<p>If the <code>oem-cloudinit</code> failed, review the <code>cloud-config</code> stored by the Digital Ocean Metadata Service:</p>

<pre><code class="language-command">curl -sL 169.254.169.254/metadata/v1/user-data | less
</code></pre>

<p>If you find a mistake in the <code>cloud-config</code>, your only option is to delete and re-create the Droplet.</p>

<h3 id="generating-and-transferring-the-kube-apiserver-tls-assets:a91459261407d5d36808cf519d4f7594">Generating and Transferring the kube-apiserver TLS Assets</h3>

<p>The address of the controller node is required for the API Server certificate. In most cases this will be the publicly routable IP or hostname of the controller cluster. Worker nodes must be able to reach the controller node(s) via this address on port 443. Additionally, external clients (such as an administrator using kubectl) will also need access, since this will run the Kubernetes API endpoint.</p>

<p>If you will be running a highly-available control-plane consisting of multiple controller nodes, then the host name for the certificate will ideally be pointing at a network load balancer that sits in front of the controller nodes. Alternatively, a DNS name can be configured which will resolve to the controller node IPs. In either case, the certificate which is generated next, needs to have the correct CommonName and/or SubjectAlternateNames.</p>

<p>Ensure you have populated the <code>$CONTROLLER_PUBLIC_IP</code>, <code>$region</code> and <code>$CONTROLLER_PRIVATE_IP</code> variables:</p>

<pre><code class="language-command">echo $region &amp;&amp; echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
</code></pre>

<p>which should show output similar to:</p>

<pre><code>[secondary_label Output]
$ echo $region &amp;&amp; echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
ams2
188.166.252.4
10.130.158.66
</code></pre>

<p>The API Server will take the first IP in the Kubernetes Service IP range. In this tutorial we are using the <code>10.3.0.1/24</code> IP range for the cluster services (See <a href="#running-the-kubelet-in-standalone-mode">Running the Kubelet in standalone mode</a>). The IP used by the <code>apiserver</code> service within Kubernetes is thus <code>10.3.0.1</code> and needs to be included in the API server certificate. If you are using a different Service IP range, update the value in the configuration file below.</p>

<p>Now we are ready to prepare the openssl config file (see <a href="https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/openssl.md">CoreOS OpenSSL tutorial</a>).</p>

<pre><code class="language-command">cat &gt; openssl.cnf &lt;&lt;EOF
[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
DNS.1 = kube-controller-01.$region
IP.1 = 10.3.0.1
IP.2 = $CONTROLLER_PUBLIC_IP
IP.3 = $CONTROLLER_PRIVATE_IP
EOF
</code></pre>

<!-- TODO: change apiserver openssl.cnf

use
``` 
$ENV::CONTROLLER_PUBLIC_IP 
```

instead of cat <<EOF...?

refer to [worker config](https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/openssl.md#openssl-config-1)

-->

<p>Generate the API server private key (<code>apiserver-key.pem</code>) which is needed to create the signing request:</p>

<pre><code class="language-command">openssl genrsa -out ~/.kube/apiserver-key.pem 2048
</code></pre>

<p>Generate the Certificate Signing Request (CSR):</p>

<pre><code class="language-command">openssl req -new -key ~/.kube/apiserver-key.pem -out apiserver.csr -subj &quot;/CN=kube-apiserver&quot; -config openssl.cnf
</code></pre>

<p>And finally, use the Certificate Authority to generate the signed API Server certificate (<code>apiserver.pem</code>):</p>

<pre><code class="language-command">openssl x509 -req -in apiserver.csr \
 -CA &quot;$HOME/.kube/ca.pem&quot; \
 -CAkey &quot;$HOME/.kube/ca-key.pem&quot; \
 -CAcreateserial \
 -out &quot;$HOME/.kube/apiserver.pem&quot; \
 -days 365 \
 -extensions v3_req \
 -extfile openssl.cnf
</code></pre>

<!-- 

We no longer need the CSR, delete the `apiserver.csr`:

```command
rm apiserver.csr
```

-->

<blockquote>
<p><strong>Note</strong>: the above command does not work on <code>git-for-windows</code> due to windows path conversions, it is recommended to copy the <code>apiserver.csr</code> and <code>openssl.cnf</code> to <code>~/.kube/</code> and just run the command from within the <code>~/.kube/</code> directory (without the <code>&quot;$HOME/.kube/&quot;</code> parts)</p>
</blockquote>

<p>Copy the necessary certificates to the controller node. The <code>core</code> user does not have write permissions to <code>/etc/kubernetes/ssl</code> directly, thus we store the files in the home directory first.</p>

<pre><code class="language-command">scp ~/.kube/apiserver-key.pem ~/.kube/apiserver.pem ~/.kube/ca.pem core@$CONTROLLER_PUBLIC_IP:~
</code></pre>

<p>Move the certificates from the Home directory to the <code>/etc/kubernetes/ssl</code> path and fix the permissions by executing the following commands over ssh:</p>

<pre><code class="language-command">ssh core@$CONTROLLER_PUBLIC_IP &lt;&lt;EOF
sudo mkdir -p /etc/kubernetes/ssl/
sudo mv ~core/*.pem /etc/kubernetes/ssl/
sudo chown root:root /etc/kubernetes/ssl/*.pem
sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
EOF
</code></pre>

<p><strong>Troubleshooting</strong>: Review the certificate contents with the following command:</p>

<pre><code class="language-command">openssl x509 -text -noout -in apiserver.pem
</code></pre>

<p>As soon as the certificates are available it will take just a few minutes for all the Controller services to start running.</p>

<p>The <code>kube-proxy</code> service will start as soon as the <code>kube-apiserver</code> is available. As we specify the <code>iptables</code> mode, it will try to flush the <code>userpace</code> settings from <code>iptables</code> - which don&rsquo;t exist - this will show up in the log files, but can be ignored</p>

<pre><code>[secondary_label Output]
$ journalctl -u kube-proxy -f
...
Error flushing userspace chain: error flushing chain &quot;KUBE-NODEPORT-HOST&quot;: exit status 1: iptables: No chain/target/match by that name.
Error flushing userspace chain: error flushing chain &quot;KUBE-NODEPORT-CONTAINER&quot;: exit status 1: iptables: No chain/target/match by that name.
</code></pre>

<p>If you started the <code>docker ps -a</code> <code>watch</code> on the controller, you should notice all containers being created by the kubelet.</p>

<p>We can confirm the <code>apiserver</code> authenticates itself with the certificate we provided and requires client authentication using <code>curl</code>. As the self-signed root CA used by the cluster is not trusted by our client, we need to pass it in:</p>

<pre><code class="language-command">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces --cacert ~/.kube/ca.pem -v
</code></pre>

<p>The <code>-v</code> flag allows us to see the verbose log of communication between our client and the apiserver. As we did not present our client certificate, the server responds with <code>unauthorized</code>.</p>

<pre><code>[secondary_label Output]
...
* successfully set certificate verify locations:
*   CAfile: /home/demo/.kube/ca.pem
  CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
...
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-SHA
* ALPN, server accepted to use http/1.1
* Server certificate:
*        subject: CN=kube-apiserver
*        start date: Dec 15 08:15:44 2015 GMT
*        expire date: Dec 14 08:15:44 2016 GMT
*        subjectAltName: 188.166.252.4 matched
*        issuer: CN=kube-ca
*        SSL certificate verify ok.
...
&lt; HTTP/1.1 401 Unauthorized
&lt; Content-Type: text/plain; charset=utf-8
&lt; Date: Wed, 16 Dec 2015 00:05:36 GMT
&lt; Content-Length: 13
&lt;
Unauthorized
...
</code></pre>

<p>We will need to authenticate by presenting a client certificate signed by our Kubernetes root CA, we will generate an admin certificate in the Administrator set up section of this tutorial.</p>

<p>At this stage we can configure our client to communicate with our Kubernetes Controller, although we do not have any worker nodes and won&rsquo;t be able to start the workload yet, this will ensure our configuration is working so far.</p>

<p><strong>Note</strong>: We may re-use the same Controller cloud-config files to spin-up a cluster of Controller Droplets with a load balancer in front of it. In this case, our apiserver certificate should have included all necessary IP addresses (such as the Load Balancer IP) for proper TLS authentication.</p>

<h2 id="step-8-setting-up-the-kubernetes-cluster-administrator:a91459261407d5d36808cf519d4f7594">Step 8 — Setting Up The Kubernetes Cluster Administrator</h2>

<h3 id="generate-the-cluster-administrator-keypair:a91459261407d5d36808cf519d4f7594">Generate the Cluster Administrator Keypair</h3>

<p>Every administrator, needs to have a private key, which we generate using openssl as follows:</p>

<pre><code class="language-command">openssl genrsa -out ~/.kube/admin-key.pem 2048
</code></pre>

<p>Using his private key, the administrator needs to create a Certificate Signing Request (CSR):</p>

<pre><code class="language-command">openssl req -new -key ~/.kube/admin-key.pem -out admin.csr -subj &quot;/CN=kube-admin&quot;
</code></pre>

<p>To be authorized to connect to the Kubernetes apiserver, this <code>admin.csr</code> needs to be sent to and processed by the Kubernetes Cluster root CA to generate  the signed <code>admin.pem</code> certificate:</p>

<pre><code class="language-command">openssl x509 -req -in admin.csr -CA ~/.kube/ca.pem -CAkey ~/.kube/ca-key.pem -CAcreateserial -out ~/.kube/admin.pem -days 365
</code></pre>

<p>From now on, the administrator can use his <code>admin-key.pem</code> and signed certificate <code>admin.pem</code> to connect to the Kubernetes cluster.</p>

<!--

TODO: also remove the apiserver csr...

We no longer need the `admin.csr` file:

```command
rm admin.csr
```

-->

<p>Test the freshly signed admin certificate by passing it in to the <code>curl</code> command we used earlier:</p>

<pre><code class="language-command">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces --cacert ~/.kube/ca.pem --cert ~/.kube/admin.pem --key ~/.kube/admin-key.pem
</code></pre>

<p>Now authenticated, this should return a <code>json</code> response containing all namespaces within our cluster. You can use Jq to simplify the output:</p>

<pre><code class="language-command">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces \
 --cacert ~/.kube/ca.pem --cert ~/.kube/admin.pem --key ~/.kube/admin-key.pem \
 | jq .items[].metadata.name
</code></pre>

<p>Instead of talking to the API directly, we will download and configure the command line tool <code>kubectl</code>.</p>

<h3 id="download-kubectl:a91459261407d5d36808cf519d4f7594">Download Kubectl</h3>

<p>As highlighted in the <a href="#step-4-understanding-where-to-get-the-kubernetes-artifacts">Step 4 — Understanding Where To Get The Kubernetes Artifacts</a> section of this tutorial, the <code>kubectl</code> binary can be downloaded from the Google cloud storage bucket.</p>

<p>For 64bit Linux clients:</p>

<pre><code class="language-command">sudo curl -Lo /opt/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubectl
sudo chmod +x /opt/bin/kubectl
</code></pre>

<p>For 64bit OSX clients:</p>

<pre><code class="language-command">sudo curl -Lo /usr/local/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/darwin/amd64/kubectl
sudo chmod +x /usr/local/bin/kubectl
</code></pre>

<p>For 64bit Windows clients (for this tutorial, tested using <a href="https://git-for-windows.github.io">git-for-windows</a>) bash:</p>

<pre><code class="language-command">curl -Lo /usr/bin/kubectl.exe https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/windows/amd64/kubectl.exe
</code></pre>

<p>Before we can use <code>kubectl</code>, we need to understand how configuration is managed within Kubernetes. This section is also important for our worker node configuration as we will use these concepts to simplify the worker setup.</p>

<h3 id="introduction-to-kubeconfig-files:a91459261407d5d36808cf519d4f7594">Introduction to Kubeconfig files</h3>

<!-- Environment variables to manage the environments, who would have thought... -->

<p>Several tools, Docker for example, rely on command-line flags and environment variables to configure the environment:</p>

<pre><code>[label docker environment variables]
DOCKER_HOST=tcp://192.168.99.101:2376
DOCKER_CERT_PATH=/home/demo/.docker/machines/.client
DOCKER_TLS_VERIFY=1
DOCKER_MACHINE_NAME=dev
</code></pre>

<p>When users have to work with multiple environments which require a different configuration however, managing several environment variables to define a single configuration becomes cumbersome, even more so when the combination of clusters and users allow for many different configurations as is the case with Kubernetes.</p>

<p>Docker opted to facilitate environment management by creating the <code>docker-machine env</code> command. This tool generates the necessary shell commands allowing users to easily switch the server their client talks to. The commands generated by <code>docker-machine</code> in turn need to support each shell (bash/fish/cmd/PowerShell/..) users may be using and ideally also auto-detect the shell in use.</p>

<p>For Kubernetes, kubeconfig files were created instead to store the environment definitions such as authentication and connection details as well as provide a mechanism to easily switch between multiple clusters and multiple user credentials. Kubernetes components were written to read the configuration from these config files including functionality for merging multiple configurations based on certain rules.</p>

<p>On one side, kubeconfig files store connection information for clusters in an associative array of <code>name-&gt;cluster</code> entries. A <code>cluster</code> entry consists of information such as the <code>server</code> to connect to, the <code>api-version</code> of the cluster and the <code>certificate-authority</code> for the cluster or a flag to skip verification of the authority which signed the server certificate (<code>insecure-skip-tls-verify</code>).</p>

<p>On the other side, kubeconfig files also store user credentials in a second associative array of <code>name-&gt;user</code> entries. A <code>user</code> entry defines <code>user</code> authentication mechanisms which may be:</p>

<ol>
<li>Authentication through a client certificate,</li>
<li>Basic authentication with username and password or</li>
<li>Authentication through a bearer token</li>
</ol>

<p>Decoupling users from clusters provides the ability to define cross cluster users only once. A <code>user</code> entry and a <code>cluster</code> entry combine to make up a <code>context</code>. Several such (<code>cluster</code>, <code>user</code>) pairs are then defined in a third associative array of <code>name-&gt;context</code> entries. Context entries also provide a <code>namespace</code> field to specify the Kubernetes <code>namespace</code> to be used for that context.  The <code>current-context</code> may be set to define the context in use.</p>

<!-- contexts are more than a pair (2-tuple), they are a 3-tuple as they consist of user, context and namespace -->

<p>To declare the above components, kubeconfig files are written in YAML and similar to Pod manifests start with a versioned schema definition:</p>

<pre><code>apiVersion: v1
kind: Config
...
</code></pre>

<p>In the next step we will manually write out a kubeconfig file to fully understand these concepts. We will also be using the <code>kubectl</code> tool to more easily manipulate kubeconfig files, with a series of <code>kubectl config</code> subcommands. Refer to <a href="http://kubernetes.io/v1.1/docs/user-guide/kubectl/kubectl_config.html">the official Kubernetes <code>kubectl config</code> documentation</a> for full details.</p>

<p>As mentioned, kubeconfig files also define a way multiple configurations may be merged together along with override options specified from the command line. See the <a href="http://kubernetes.io/v1.1/docs/user-guide/kubeconfig-file.html#loading-and-merging-rules">loading and merging rules</a> section of the Kubernetes documentation for a technical overview of these rules. We will only define a single kubeconfig file in this tutorial.</p>

<h3 id="configure-kubectl:a91459261407d5d36808cf519d4f7594">Configure Kubectl</h3>

<p>As we have a theoretical understanding of what a kubeconfig if made up from, we will first manually write our default kubeconfig file (<code>~/.kube/config</code>).</p>

<p>Define the Digital Ocean cluster we just created as <code>do-cluster</code>:</p>

<pre><code>[label ~/.kube/config - snippet]

apiVersion: v1
kind: Config
clusters:
- name: do-cluster
  cluster:
    certificate-authority: ca.pem
    server: https://$CONTROLLER_PUBLIC_IP
</code></pre>

<blockquote>
<p><strong>Note</strong>: Relative paths are supported, in this tutorial we store our certificates in the same directory as our kubeconfig file (<code>~/.kube/</code>), modify these values to mirror your own configuration.</p>
</blockquote>

<p>Next, we define the <code>admin</code> user and specify the associated TLS assets we generated for certification based authentication:</p>

<pre><code>[label ~/.kube/config - snippet]
...
users:
- name: admin
  user:
    client-certificate: admin.pem
    client-key: admin-key.pem

</code></pre>

<p>Followed by the definition of the context combining these two, which we will name the <code>do-cluster-admin</code> context:</p>

<pre><code>[label ~/.kube/config - snippet]
...
contexts:
- name: do-cluster-admin
  context:
    cluster: do-cluster
    user: admin
</code></pre>

<p>As we did not specify a <code>namespace</code> for our context, the <code>default</code> namespace will be used.</p>

<p>We may set this as our current context in our kubeconfig file by adding the <code>current-context: do-cluster-admin</code> setting at the end.</p>

<p>Using the <code>cat</code> command to combine all the above snippets with a here-string for variable substitution, we write out the file as follows:</p>

<pre><code class="language-commmand">cat &gt; ~/.kube/config&lt;&lt;EOF
apiVersion: v1
kind: Config
clusters:
- name: do-cluster
  cluster:
    certificate-authority: ca.pem
    server: https://$CONTROLLER_PUBLIC_IP
users:
- name: admin
  user:
    client-certificate: admin.pem
    client-key: admin-key.pem
contexts:
- name: do-cluster-admin
  context:
    cluster: do-cluster
    user: admin
current-context: do-cluster-admin
EOF
</code></pre>

<p>If the kubeconfig file is not passed in to <code>kubectl</code> through the <code>--kubeconfig</code> flag, it will first look for a <code>kubeconfig</code> file in the current directory as well as the <code>$KUBECONFIG</code> environment variable. If none of these are set, <code>kubectl</code> will use the default <code>~/.kube/config</code> file we just created.</p>

<p>We may also generate the above file using kubectl with the following 4 commands:</p>

<p>Set the &laquo;do-cluster&raquo; entry:</p>

<pre><code class="language-command">kubectl config set-cluster do-cluster --server=https://$CONTROLLER_PUBLIC_IP --certificate-authority=$HOME/.kube/ca.pem
</code></pre>

<p>Set the &laquo;admin&raquo; user entry:</p>

<pre><code class="language-command">kubectl config set-credentials admin --client-key=$HOME/.kube/admin-key.pem --client-certificate=$HOME/.kube/admin.pem
</code></pre>

<p>Set the &laquo;do-cluster-admin&raquo; context:</p>

<pre><code>kubectl config set-context do-cluster-admin --cluster=do-cluster --user=admin
</code></pre>

<p>Set the <code>current-context</code>:</p>

<pre><code class="language-command">kubectl config use-context do-cluster-admin
</code></pre>

<p>Confirm your configuration was successful with the following command:</p>

<pre><code class="language-command">kubectl version
</code></pre>

<p>If everything worked so far, this should return output similar to:</p>

<pre><code>[secondary_label Output]
Client Version: version.Info{Major:&quot;1&quot;, Minor:&quot;1&quot;, GitVersion:&quot;v1.1.2&quot;, GitCommit:&quot;3085895b8a70a3d985e9320a098e74f545546171&quot;, GitTreeState:&quot;clean&quot;}
Server Version: version.Info{Major:&quot;1&quot;, Minor:&quot;1&quot;, GitVersion:&quot;v1.1.2&quot;, GitCommit:&quot;3085895b8a70a3d985e9320a098e74f545546171&quot;, GitTreeState:&quot;clean&quot;}
</code></pre>

<p>We may confirm the pods running in the kube-system namespace with the following command</p>

<pre><code class="language-command">kubectl get pods --namespace=kube-system
</code></pre>

<p>Expected output looks like:</p>

<pre><code>[secondary_label Output]
NAME                                      READY     STATUS    RESTARTS   AGE
kube-controller-188.166.252.4             3/3       Running   0          3h
kube-controller-manager-188.166.252.4     1/1       Running   0          3h
kube-scheduler-188.166.252.4              1/1       Running   0          3h
</code></pre>

<p>Indicating all 3 containers (<code>kube-apiserver</code>, <code>scheduler-elector</code> &amp; <code>controller-manager-elector</code>) of the <code>kube-controller</code> pod as well as the <code>kube-controller-manager</code> and <code>kube-scheduler</code> pods are running.</p>

<p>We now have our Etcd data store and first controller droplet ready, but we do not have any worker nodes yet to schedule workloads on.</p>

<pre><code class="language-command">kubectl get nodes
</code></pre>

<p>Returns an empty collection of worker nodes. We will spin up our first worker node next.</p>

<h2 id="step-9-provisioning-the-kubernetes-worker-droplets:a91459261407d5d36808cf519d4f7594">Step 9 — Provisioning The Kubernetes Worker Droplets</h2>

<p>Configuring the workers is significantly less complicated and we can reuse many of the controllers configuration.</p>

<p>Ensure your <code>DIGITALOCEAN_API_KEY</code>, <code>ETCD_PEER</code>, <code>CONTROLLER_PUBLIC_IP</code>, <code>CONTROLLER_PRIVATE_IP</code> and <code>region</code> environment variables are set for the next steps.</p>

<h3 id="the-etcd-flannel-docker-services:a91459261407d5d36808cf519d4f7594">The Etcd, Flannel &amp; Docker services</h3>

<p>As mentioned in the Etcd configuration section, we are running an Etcd daemon in proxy mode on each droplet for Flannel to access. As we are sharing our Etcd cluster between Flannel and Kubernetes we should note that exposing your Kubernetes data back end to each node is a bad practice. For production environments we should use a separate Etcd cluster to store the Flannel meta data configuration. If Flannel was not used to configure the overlay network, Etcd access would not be needed on the worker nodes at all.</p>

<p>Our cloud-config section for Etcd proxy daemon configuration as we saw before looks like this:</p>

<pre><code>[label cloud-config-worker.yaml - etcd proxy snippet]

coreos:
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: &quot;etcd-01=ETCD_PEER&quot;
  units:
    - name: &quot;etcd2.service&quot;
      command: &quot;start&quot;
</code></pre>

<p>We need to ensure Flannel starts and add a Drop-in for Docker to depend on flannel. (Flannel will use localhost to get it&rsquo;s network configuration)</p>

<pre><code>[label cloud-config-worker.yaml - flannel snippet

coreos:
  units:
    - name: &quot;flanneld.service&quot;
      command: &quot;start&quot;
    - name: &quot;docker.service&quot;
      command: &quot;start&quot;
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
</code></pre>

<h3 id="the-kubelet-service:a91459261407d5d36808cf519d4f7594">The kubelet service</h3>

<p>In order to facilitate secure communication between Kubernetes components, <code>kubeconfig</code> can also be used to define authentication settings for the kubelet. In this case, the kubelet and proxy are reading this configuration to communicate with the API. Refer back to the <a href="#introduction-to-kubeconfig-files">Introduction to kubeconfig files</a> section of this tutorial for a detailed explanation of the kubeconfig specification.</p>

<p>Very similar to our previous kubeconfig file, we define a single cluster, in this case called &laquo;local&raquo; with a <code>certificate-authority</code> path. and a single user called &laquo;kubelet&raquo; with certificate based authentication through a worker private key and signed certificate. The combination of this <code>user</code> and <code>cluster</code> is defined as the &laquo;kubelet-context&raquo; and set as the <code>current-context</code>.</p>

<pre><code class="language-line_numbers">[label kubelet-kubeconfig]

apiVersion: v1
kind: Config
clusters:
- name: local
  cluster:
    certificate-authority: /etc/kubernetes/ssl/ca.pem
users:
- name: kubelet
  user:
    client-certificate: /etc/kubernetes/ssl/worker.pem
    client-key: /etc/kubernetes/ssl/worker-key.pem
contexts:
- name: kubelet-context
  context:
    cluster: local
    user: kubelet
current-context: kubelet-context
</code></pre>

<p>Our kubelet parameters are</p>

<pre><code>kubelet \
  --api-servers=https://CONTROLLER_PUBLIC_IP \
  --register-node=true \
  --allow-privileged=true \
  --config=/etc/kubernetes/manifests \
  --hostname-override=$public_ipv4 \
  --cluster-dns=10.3.0.10 \
  --cluster-domain=cluster.local \
  --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
</code></pre>

<p>The <code>--api-servers</code> flag points to the https protocol to use port 443 and we use a placeholder for our <code>CONTROLLER_PUBLIC_IP</code> to make a generic cloud-config file which we can re-use for creating multiple clusters. We pass in the kubeconfig we described above with the <code>--kubeconfig</code> flag. Our worker nodes are registered to receive work by specifying the <code>--register-node=true</code> flag. We still configure our Kubelet to monitor a local directory for Pod manifests, although we will not be using this at this point. The remaining parameters are identical to the controller Droplets configuration.</p>

<p>Similar to our controller setup, we define a Systemd unit to wait for the Worker TLS assets to be in place and require the <code>kube-kubelet</code> service to depend on this unit.</p>

<pre><code>[label cloud-config-worker.yaml - tls ready snippet]

coreos:
  units:
    - name: &quot;tls-ready.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=-/usr/bin/mkdir -p /etc/kubernetes/ssl
        ExecStart=/bin/bash -c &quot;until [ `ls -1 /etc/kubernetes/ssl/{worker,worker-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \&quot;waiting for TLS assets...\&quot;;sleep 5; done&quot;
</code></pre>

<p>Add the <code>tls-ready.service</code> dependency to the <code>kube-kubelet</code> and <code>kube-proxy</code> services</p>

<pre><code>[label cloud-config-worker.yaml - kubelet snippet]

coreos:
  units:
    - name: &quot;kube-kubelet.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service tls-ready.service
        After=docker.service tls-ready.service
        [Service]
        ExecStartPre=-/bin/bash -c &quot;mkdir -p /etc/kubernetes/{manifests,ssl}&quot;
        ExecStart=/usr/bin/kubelet \
        --api-servers=https://CONTROLLER_PUBLIC_IP \
        --register-node=true \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml 
        Restart=always
        RestartSec=10

</code></pre>

<h3 id="the-kube-proxy-service:a91459261407d5d36808cf519d4f7594">The kube-proxy Service</h3>

<pre><code>[label cloud-config-worker.yaml - kube-proxy snippet]

coreos:
  units:
    - name: &quot;kube-proxy.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target tls-ready.service
        After=network-online.target tls-ready.service
        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        ExecStart=/opt/bin/kube-proxy \
        --master=https://CONTROLLER_PUBLIC_IP \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        Restart=always
        RestartSec=10
</code></pre>

<h3 id="the-final-worker-cloud-config-with-all-coreos-units:a91459261407d5d36808cf519d4f7594">The Final Worker cloud-config with all CoreOS Units</h3>

<ol>
<li><code>etcd2.service</code> snippet to start a local Etcd proxy, notice the <code>ETCD_PEER</code> placeholder.</li>
<li><code>flanneld.service</code> snippet to start the overlay network daemon with a drop-in to configure the network subnet</li>
<li><code>docker.service</code> drop-in snippet to add flannel dependency</li>
<li><code>tls-ready.service</code> to block other units until the TLS assets for the worker have been put in place</li>
<li><code>kubelet.service</code> snippet running the kubelet to register with our controller nodes</li>
<li><code>kube-proxy.service</code> snippet running the <code>kube-proxy</code> service</li>
</ol>

<pre><code class="language-line_numbers">[label cloud-config-worker.yaml]

#cloud-config

write-files:
  - path: /etc/kubernetes/worker-kubeconfig.yaml
    permissions: '0644'
    content: |
      apiVersion: v1
      kind: Config
      clusters:
      - name: local
        cluster:
          certificate-authority: /etc/kubernetes/ssl/ca.pem
      users:
      - name: kubelet
        user:
          client-certificate: /etc/kubernetes/ssl/worker.pem
          client-key: /etc/kubernetes/ssl/worker-key.pem
      contexts:
      - name: kubelet-context
        context:
          cluster: local
          user: kubelet
      current-context: kubelet-context
coreos:
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: &quot;etcd-01=ETCD_PEER&quot;
  units:
    - name: &quot;etcd2.service&quot;
      command: &quot;start&quot;
    - name: &quot;flanneld.service&quot;
      command: &quot;start&quot;
    - name: &quot;docker.service&quot;
      command: &quot;start&quot;
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
    - name: &quot;tls-ready.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=docker.service
        After=docker.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=-/usr/bin/mkdir -p /etc/kubernetes/ssl
        ExecStart=/bin/bash -c &quot;until [ `ls -1 /etc/kubernetes/ssl/{worker,worker-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \&quot;waiting for TLS assets...\&quot;;sleep 5; done&quot;
    - name: &quot;kube-proxy.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target tls-ready.service
        After=network-online.target tls-ready.service
        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        ExecStart=/opt/bin/kube-proxy \
        --master=https://CONTROLLER_PUBLIC_IP \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        Restart=always
        RestartSec=10
    - name: &quot;kube-kubelet.service&quot;
      command: &quot;start&quot;
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service tls-ready.service
        After=docker.service tls-ready.service
        [Service]
        ExecStart=/usr/bin/kubelet \
        --api-servers=https://CONTROLLER_PUBLIC_IP \
        --register-node=true \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
        Restart=always
        RestartSec=10
</code></pre>

<p>Use the above template to generate the worker node cloud config for this Digital Ocean cluster:</p>

<pre><code class="language-command">sed -e &quot;s|ETCD_PEER|${ETCD_PEER}|g;s|CONTROLLER_PUBLIC_IP|${CONTROLLER_PUBLIC_IP}|g;&quot; cloud-config-worker.yaml &gt; kube-worker.yaml
</code></pre>

<p>And send the command to create the Droplet</p>

<pre><code class="language-command">doctl d c --wait-for-active \
    -i &quot;CoreOS-alpha&quot; \
    -s 512mb \
    -r &quot;$region&quot; \
    -p \
    -k k8s-key \
    -uf kube-worker.yaml kube-worker-01
</code></pre>

<p><strong>Note</strong>: running <code>free -m</code> after freshly starting all Kubernetes services on a <code>512mb</code> Worker Droplet shows 129mb free, consider using 1024mb Droplets</p>

<p>We refresh the Droplet configuration and cache the json string returned in the $WORKER_JSON environment variable for subsequent commands.</p>

<pre><code class="language-command">WORKER_JSON=`doctl -f 'json' d f kube-worker-01.$region`

</code></pre>

<p>We parse the private and public IPs out as explained in the <a href="#working-with-doctl-responses">Working with doctl responses</a> section of this tutorial.</p>

<pre><code class="language-command">WORKER_PUBLIC_IP=`echo $WORKER_JSON | jq -r '.networks.v4[] | select(.type == &quot;public&quot;) | .ip_address'`
WORKER_PRIVATE_IP=`echo $WORKER_JSON | jq -r '.networks.v4[] | select(.type == &quot;private&quot;) | .ip_address'`
</code></pre>

<p>Confirm</p>

<pre><code class="language-command">echo $WORKER_PUBLIC_IP &amp;&amp; echo $WORKER_PRIVATE_IP
</code></pre>

<p><strong>Troubleshooting</strong>:</p>

<pre><code class="language-command">ssh core@$WORKER_PUBLIC_IP
</code></pre>

<pre><code class="language-command">journalctl -u oem-cloudinit -f
</code></pre>

<pre><code class="language-command">watch -n 1 'docker ps --format=&quot;table {{.Image}}\t{{.ID}}\t{{.Status}}\t{{.Ports}}&quot; -a' 
</code></pre>

<h3 id="generating-and-transferring-the-worker-tls-assets:a91459261407d5d36808cf519d4f7594">Generating and Transferring the Worker TLS assets</h3>

<p>As it is recommended to generate a unique certificate per worker, we will do so and transfer it to our worker droplet now.</p>

<p>The IP addresses and fully qualified hostnames of all worker nodes will be needed. The certificates generated for the worker nodes will need to reflect how requests will be routed to those nodes. In most cases this will be a routable IP and/or a routable hostname. These will be unique per worker; when you see them used below, consider it a loop and do that step for each worker.</p>

<p>This procedure generates a unique TLS certificate for every Kubernetes worker node in your cluster. While unique certificates are less convenient to generate and deploy, they do provide stronger security assurances and the most portable installation experience across multiple cloud-based and on-premises Kubernetes deployments.</p>

<p>We will use a common openssl configuration file for all workers. The certificate output will be customized per worker based on environment variables used in conjunction with the configuration file. Create the file worker-openssl.cnf on your local machine with the following contents.</p>

<pre><code class="language-line_numbers">[label ~/.kube/worker-openssl.cnf]

[req]
req_extensions = v3_req
distinguished_name = req_distinguished_name
[req_distinguished_name]
[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names
[alt_names]
IP.1 = $ENV::WORKER_IP
</code></pre>

<p>Generate the private key for our first Worker Droplet</p>

<pre><code class="language-command">openssl genrsa -out ~/.kube/worker-01-key.pem 2048
</code></pre>

<p>Generate the Certificate Signing Request, substituting the WORKER_IP environment variable:</p>

<pre><code class="language-command">WORKER_IP=${WORKER_PRIVATE_IP} openssl req -new -key ~/.kube/worker-01-key.pem -out worker-01.csr -subj &quot;/CN=kube-worker-01&quot; -config worker-openssl.cnf
</code></pre>

<p>Generate the worker certificate</p>

<pre><code class="language-command">WORKER_IP=${WORKER_PRIVATE_IP} openssl x509 -req -in worker-01.csr \
 -CA &quot;$HOME/.kube/ca.pem&quot; \
 -CAkey &quot;$HOME/.kube/ca-key.pem&quot; \
 -CAcreateserial \
 -out &quot;$HOME/.kube/worker-01.pem&quot; \
 -days 365 \
 -extensions v3_req \
 -extfile worker-openssl.cnf
</code></pre>

<!-- 

We no longer need the csr

```command
rm worker-01.csr
```
-->

<blockquote>
<p><strong>Note</strong>: the above command does not work on <code>git-for-windows</code> due to windows path conversions, it is recommended to copy the <code>worker-01.csr</code> and <code>worker-openssl.cnf</code> to <code>~/.kube/</code> and just run the command from within the <code>~/.kube/</code> directory (without the <code>&quot;$HOME/.kube/&quot;</code> parts)</p>
</blockquote>

<p>Copy the necessary certificates to the controller node. We store the files in the home directory first.</p>

<pre><code class="language-command">scp ~/.kube/worker-01-key.pem ~/.kube/worker-01.pem ~/.kube/ca.pem core@$WORKER_PUBLIC_IP:~
</code></pre>

<p>Move the certificates from the Home directory to the /etc/kubernetes/ssl path, fix the permissions and create links to match our generic kubeconfig (which expects <code>/etc/kubernetes/worker-key.pem</code> instead of <code>/etc/kubernetes/worker-01-key.pem</code>) by executing the following commands over ssh:</p>

<pre><code class="language-command">ssh core@$WORKER_PUBLIC_IP &lt;&lt;EOF
sudo mkdir -p /etc/kubernetes/ssl/
sudo mv ~core/*.pem /etc/kubernetes/ssl/
sudo chown root:root /etc/kubernetes/ssl/*.pem
sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
sudo ln -s /etc/kubernetes/ssl/worker-01.pem /etc/kubernetes/ssl/worker.pem
sudo ln -s /etc/kubernetes/ssl/worker-01-key.pem /etc/kubernetes/ssl/worker-key.pem
EOF
</code></pre>

<p>As soon as the certificates are available it will take just a few minutes for the kubelet and kube-proxy to start running on the worker and register with the Controller.</p>

<p>We can verify by watching the <code>kubectl get nodes</code>:</p>

<pre><code class="language-command">kubectl get nodes
</code></pre>

<p>Which should show output as follows:</p>

<pre><code>[secondary_label Output]
NAME              LABELS                                   STATUS    AGE
128.199.203.205   kubernetes.io/hostname=128.199.203.205   Ready     9m
</code></pre>

<p>We may repeat the above steps to create additional Worker droplets with their own TLS assets. We now have a working Kubernetes cluster, ready to start running our containerized applications. To facilitate the application deployment however, we are recommended to run a few cluster services and will proceed to do so in the next step.</p>

<h2 id="step-10-running-kubernetes-cluster-services:a91459261407d5d36808cf519d4f7594">Step 10 — Running Kubernetes Cluster Services</h2>

<p>Several cluster services are provided as cluster add-ons (UI/Dashboard, Image Registry, DNS, &hellip;). Deploying these add-ons is optional, but availability of some of these services is often expected by Kubernetes users. A full listing of all supported add-ons can be found within the Kubernetes GitHub repository at <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons">kubernetes/cluster/addons/</a>.</p>

<p>Add-ons are built on the same Kubernetes components as user-submitted jobs — Pods, Replication Controllers and Services, however, cluster add-ons are expected to specify the label: <code>kubernetes.io/cluster-service: &quot;true&quot;</code>.</p>

<p>One such cluster add-on facilitates the discovery of services running within Kubernetes, we will first define the problem and the options Kubernetes provides to solve this problem.</p>

<p>When Pods depend on each other (for example: front end services may depend on back end services), mechanisms need to be in place to enable service discovery. Within Kubernetes, Pods are short lived objects and their IPs change over time due to crashes or scheduling changes. Because of this, addressing Pods directly has now become difficult, thus Kubernetes introduced the concept of Service objects to address this problem. Service objects are long lived objects which get a static Virtual IP within the cluster, usually referred to as their <code>clusterIP</code>, to address sets of Pods internally or externally to the cluster. This <code>clusterIP</code> is stable as long as the Service object exists. Kubernetes sets up a load balancer forwarding traffic through this <code>clusterIP</code> to the Service EndPoints, unless you explicitly disable the load balancer (by setting <code>clusterIP</code> to <code>none</code>) and expect to work with a list of the Service EndPoints directly. Such Services without a <code>clusterIP</code> are called Headless. Service objects may also be created for services running outside of the Kubernetes cluster (by omitting the Pod selector) as long as you manually create the EndPoint definitions for these external services. Full details on how to do this are available within the <a href="http://kubernetes.io/v1.1/docs/user-guide/services.html">official Kubernetes documentation</a>.</p>

<p>Once Service objects have been defined, Kubernetes provides 2 ways of finding them:</p>

<ol>
<li>Through Environment variables, or</li>
<li>Using DNS</li>
</ol>

<p>Upon Pod creation, the kubelet adds a set of environment variables for each active Service within the same namespace, similar to how Docker links worked. These environment variables enforce an ordering requirement as any Service that a Pod wants to access must be created before the Pod itself and may require applications to be modified before they can run within Kubernetes. If we use DNS to discover services, we do not have these restrictions, but we are required to deploy the DNS cluster add-on.</p>

<p>As part of this tutorial we ensure our Kubernetes cluster is integrated with DNS for Service discovery by deploying the DNS add-on with <code>kubectl</code>.</p>

<h3 id="dns-integration-with-kubernetes:a91459261407d5d36808cf519d4f7594">DNS Integration with Kubernetes</h3>

<p>When enabled, the DNS add-on for Kubernetes will assign a DNS name for every Service object defined in the Kubernetes cluster.</p>

<p>At the time of writing, the DNS protocol implementation for the DNS add-on is provided by <strong>SkyDNS</strong>. SkyDNS is configured as a slave to the Kubernetes API Server with custom logic implemented in a bridge component called <strong>Kube2sky</strong>. SkyDNS itself is only a thin layer over <strong>Etcd</strong> to translate Etcd keys and values to the DNS protocol. In this way, SkyDNS can be as highly available and stable as the underlying Etcd cluster. We will have a closer look at how each of these 3 components work together and how we will deploy them as a single Pod.</p>

<!--
Services announce their availability by sending a POST with a small JSON payload, Each service has a Time To Live that allows SkyDNS to expire the records for the services that haven't update their availability within the TTL window. Services can send a periodic POST to SkyDNS updating their TTL to keep them in the pool.

-->

<p>We will create a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_replicationcontroller">Replication Controller</a> to run the DNS Pod and a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_service">Service</a> to expose its ports.</p>

<p>Our Replication Controller manifest starts, as we saw earlier, with the schema definition and a <code>metadata</code> section:</p>

<pre><code>[label DNS Add-on Manifest - snippet]
apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v9
  namespace: kube-system
  ...
</code></pre>

<!-- use labels effectively: http://kubernetes.io/v1.1/docs/user-guide/managing-deployments.html#using-labels-effectively -->

<p>A Replication Controller can be thought of as a process supervisor, but which supervises multiple Pods across multiple nodes instead of individual processes on a single node. The Replication Controller creates Pods from a template and uses labels and selectors to monitor the actual running Pods. The selector finds Pods within the cluster by label, the labels we&rsquo;ll use for this Replication Controller are the <code>k8s-app</code> and <code>version</code> labels. We specify these labels together with the <code>kubernetes.io/cluster-service: &quot;true&quot;</code> label required for cluster add-ons in the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podtemplatespec">PodTemplateSpec</a> as well as attach them to our Replication Controller itself.</p>

<p>At the time of writing we are using version 9 and refer to the DNS add-on as the <code>kube-dns</code> app. By default Replication Controllers will run 1 replica, but we explicitly set the <code>replicas</code> field to 1 for clarity in our <code>spec</code>. This looks as follows in our manifest file:</p>

<!--
**note:** [update to v10?](https://github.com/kubernetes/kubernetes/commit/5abfce45e1fb0cb9bf3b643f0ee53b812e6f83b0)
-->

<pre><code>[label DNS Add-on Manifest - snippet]
apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v9
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    version: v9
    kubernetes.io/cluster-service: &quot;true&quot;
spec:
  replicas: 1
  selector:
    k8s-app: kube-dns
    version: v9
  template:
    metadata:
      labels:
        k8s-app: kube-dns
        version: v9
        kubernetes.io/cluster-service: &quot;true&quot;
    spec:
      volumes:
  ...
</code></pre>

<p>We added the labels to the Replication Controller object in its <code>metadata</code> field. In the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_replicationcontrollerspec">ReplicationControllerSpec</a> we set the labels for the Pod <code>template.metadata</code> and also set the <code>replicas</code> and <code>selector</code> values. Let&rsquo;s look at the volumes and containers defined in the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podtemplatespec">PodTemplateSpec</a> next.</p>

<p>We will only define a volume for Etcd.  Giving Etcd a volume, outside of the union filesystem used by container runtimes such as Docker, will ensure optimal performance by reducing filesystem overhead. As the data is just a scratch space and it&rsquo;s fine to lose the data when the Pod is rescheduled on a different Node, it is sufficient to use an <a href="http://kubernetes.io/v1.1/docs/user-guide/volumes.html#emptydir">EmptyDir</a>-type volume:</p>

<pre><code>[label DNS Add-on Manifest - snippet]
...
      volumes:
      - name: etcd-storage
        emptyDir: {}
...
</code></pre>

<p>Let&rsquo;s look at the actual definitions of the containers in the Pod template. We see a container for each of the 3 components described earlier as well as an ExecHealthz sidecar container:</p>

<ol>
<li>Etcd - the storage for SkyDNS</li>
<li>Kube2sky - the glue between SkyDNS and Kubernetes</li>
<li>SkyDNS - the DNS server</li>
<li>ExecHealthz - sidecar container for health monitoring, see details below.</li>
</ol>

<p>The Etcd instance used by the DNS add-on is best ran separately from the Etcd cluster used by the Kubernetes API Services. For simplicity we run Etcd within the same Pod as our SkyDNS and Kube2sky components. This is sufficient considering the DNS add-on only requires a small subset of everything Etcd has to offer.</p>

<p>For the Etcd container we will use the busybox-based image available on the Google container registry, refer to the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/images/etcd">kubernetes/cluster/images/etcd</a> repository on GitHub to see the full details of how that image is made.</p>

<pre><code>[label DNS Add-on Manifest - snippet]
...
      - name: etcd
        image: gcr.io/google_containers/etcd:2.0.9
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        command:
        - /usr/local/bin/etcd
        - -data-dir
        - /var/etcd/data
        - -listen-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -advertise-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -initial-cluster-token
        - skydns-etcd
        volumeMounts:
        - name: etcd-storage
          mountPath: /var/etcd/data
...
</code></pre>

<p>We run this container with the <code>etcd-storage</code> volume mounted and used as the Etcd <code>data-dir</code>. We configure Etcd to listen on localhost for connections on both the IANA-assigned <code>2379</code> port as well as the legacy <code>4001</code> port, this is required for Kube2sky and SkyDNS which still connect to port <code>4001</code> by default.</p>

<p>This spec also applies resource limits which define an upper bound on the maximum amount of resources that will be made available to this container. Resource limits are crucial to enable the scheduling components within Kubernetes to be effective. Without a definition of the required resources, schedulers can do little more than round robin assignments. The CPU resource is defined in Compute Units per second (KCU) and in this case the unit is milli-KCUs, where 1 KCU will roughly be equivalent to a single CPU hyperthreaded core for some recent x86 processor. The memory resource is defined in bytes. For a full overview of Resource management within Kubernetes, refer to <a href="http://kubernetes.io/v1.1/docs/design/resources.html#resource-specifications">the official Kubernetes resource guidelines</a>.</p>

<p>The Kube2sky container uses another Busybox based-image made available on the Google Container Registry, refer to the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns/kube2sky">kubernetes/cluster/addons/dns/kube2sky</a> repository on GitHub to see the source for that image.</p>

<pre><code>[label DNS Add-on Manifest - snippet]
...
      - name: kube2sky
        image: gcr.io/google_containers/kube2sky:1.11
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -domain=cluster.local
...
</code></pre>

<p>The Kube2sky Docker image has the <code>Entrypoint</code> set to <code>/kube2sky</code>, thus we only need to pass on the <code>-domain</code> under which we want all DNS names to be hosted through the <code>args</code> array. This should match our kubelet configuration which we set to <code>cluster.local</code> in this tutorial, modify this value to mirror your own configuration.</p>

<p>Kube2sky discovers and authenticates with the Kubernetes API Service through environment variables provisioned and secrets mounted by the kubelet into the container, we will have a closer look at these in <a href="#step-11-deploying-kubernetes-ready-applications">Step 11 — Deploying Kubernetes-ready applications</a>. Once authenticated and connected, Kube2sky watches the Kubernetes API Service for changes in Service objects and publishes those changes to Etcd for SkyDNS. SkyDNS supports A and AAAA records to handle &laquo;legacy&raquo; services. With A/AAAA records the port number must be known by the client connection because that information is not in the returned records. Given we defined our cluster domain as <code>cluster.local</code>, the keys created by Kube2sky and served by SkyDNS will have the following DNS naming scheme:</p>

<pre><code>[label A Records naming scheme]
&lt;service_name&gt;.&lt;namespace_name&gt;.svc.cluster.local
</code></pre>

<p>For example: for a Service called &laquo;my-service&raquo; in the &laquo;default&raquo; namespace, an A record for <code>my-service.default.svc.cluster.local</code> is created. Other Pods within the same <code>default</code> namespace should be able to find the service simply by doing a name lookup for <code>my-service</code>, Pods which exist in other namespaces must use the fully qualified name.</p>

<p>For Service objects which define named ports, Kube2sky ensures SRV records are created with the following naming scheme:</p>

<pre><code>[label named ports SRV records naming scheme]
_&lt;port_name&gt;._&lt;port_protocol&gt;.&lt;service_name&gt;.&lt;namespace_name&gt;.svc.cluster.local
</code></pre>

<p>For example, If the Service called &laquo;my-service&raquo; in the &laquo;default&raquo; namespace has a port named &laquo;http&raquo; with a protocol of TCP, you can do a DNS SRV query for &laquo;_http._tcp.my-service.default.svc.cluster.local&raquo; to discover the port number for &laquo;http&raquo;.</p>

<p>We will confirm the above DNS records are served correctly after we have deployed the DNS add-on to our cluster.</p>

<p>The <a href="https://hub.docker.com/r/skynetservices/skydns/">skynetservices/skydns</a> image based on Alpine Linux is available on the Docker Hub at about <a href="https://imagelayers.io/?images=skynetservices%2Fskydns:2.5.3a">19MB</a> and comes with <code>dig</code>. The official <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns/skydns">kubernetes/cluster/addons/dns/skydns</a> add-on uses a busybox based image at about 41MB without <code>dig</code>. The discussion as to which image should be used in the long run can be followed on <a href="https://github.com/kubernetes/kubernetes/issues/10386">GitHub</a>. In this tutorial we opt to use the <code>skynetservices/skydns</code> image as the version tags are slightly more intuitive:</p>

<pre><code>[label DNS Add-on Manifest - snippet]
...
      - name: skydns
        image: skynetservices/skydns:2.5.3a
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -machines=http://localhost:4001
        - -addr=0.0.0.0:53
        - -domain=cluster.local.
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
...
</code></pre>

<p>The EntryPoint for both images is <code>/skydns/</code>, thus we only need to pass in 3 arguments. We point SkyDNS to the Etcd instance running within the Pod through the <code>-machines</code> flag. We define the address we want SkyDNS to bind to through the <code>-addr</code> flag and we specify the domain we want SkyDNS to serve records within through the <code>-domain</code> flag. We also expose the port SkyDNS is bound to on the Pod through named ports for both TCP and UDP protocols.</p>

<p>To monitor the health of the container with liveness probes, we run a health server as a sidecar container using the <a href="https://github.com/kubernetes/contrib/tree/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/exec-healthz">ExecHealthz</a> utility. By running a sidecar container, we do not make these liveness probes dependent on the container runtime to execute commands directly in the SkyDNS container (which also requires those binaries to be available within the container image). Instead our sidecar container will provide the <code>/healthz</code> http endpoint, this usage of a sidecar container illustrates very well the concept of creating single purpose and re-usable components and the power of Pods to bundle them. This is one of the fundamental features of Kubernetes Pods and you may reuse these Kubernetes components for your own application setup.</p>

<p>The ExecHealthz image available on the Google Container Registry uses Busybox as a base image. We use the <code>nslookup</code> utility bundled with Busybox for liveness probes as <code>dig</code> is not available in this image.</p>

<p>Add the ExecHealthz container with the following container spec:</p>

<pre><code>[label DNS Add-on Manifest - snippet]
...
      - name: healthz
        image: gcr.io/google_containers/exechealthz:1.0
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
        args:
        - -cmd=nslookup kubernetes.default.svc.cluster.local localhost &gt;/dev/null
        - -port=8080
        ports:
        - containerPort: 8080
          protocol: TCP
...
</code></pre>

<p>Our health check does a simple probe for the Kubernetes API service which, as discussed above, SkyDNS should serve under the <code>kubernetes.default.svc.cluster.local</code> DNS record.</p>

<p>We can now add the liveness and readiness probes via this sidecar health server to report on the health status of our SkyDNS container:</p>

<pre><code>[label DNS Add-on Manifest - snippet]
      - name: skydns
        image: skynetservices/skydns:2.5.3a
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -machines=http://localhost:4001
        - -addr=0.0.0.0:53
        - -domain=cluster.local.
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 1
          timeoutSeconds: 5
</code></pre>

<p>The full manifest of the Replication Controller for the <code>kube-dns-v9</code> add-on will be listed next for your reference, we will look at the manifest for the Service right after.</p>

<pre><code class="language-line_numbers">[label skydns-rc.yaml]

apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v9
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    version: v9
    kubernetes.io/cluster-service: &quot;true&quot;
spec:
  replicas: 1
  selector:
    k8s-app: kube-dns
    version: v9
  template:
    metadata:
      labels:
        k8s-app: kube-dns
        version: v9
        kubernetes.io/cluster-service: &quot;true&quot;
    spec:
      containers:
      - name: etcd
        image: gcr.io/google_containers/etcd:2.0.9
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        command:
        - /usr/local/bin/etcd
        - -data-dir
        - /var/etcd/data
        - -listen-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -advertise-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -initial-cluster-token
        - skydns-etcd
        volumeMounts:
        - name: etcd-storage
          mountPath: /var/etcd/data
      - name: kube2sky
        image: gcr.io/google_containers/kube2sky:1.11
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -domain=cluster.local
      - name: skydns
        image: skynetservices/skydns:2.5.3a
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -machines=http://localhost:4001
        - -addr=0.0.0.0:53
        - -domain=cluster.local.
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 1
          timeoutSeconds: 5
      - name: healthz
        image: gcr.io/google_containers/exechealthz:1.0
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
        args:
        - -cmd=nslookup kubernetes.default.svc.cluster.local localhost &gt;/dev/null
        - -port=8080
        ports:
        - containerPort: 8080
          protocol: TCP
      volumes:
      - name: etcd-storage
        emptyDir: {}
      dnsPolicy: Default
</code></pre>

<p>The kube-dns Service will expose the DNS Pod internally to the cluster on the fixed IP we assigned for our DNS server, this <code>clusterIP</code> has to match the value we passed to all our kubelets previously, which is <code>10.3.0.10</code> in this tutorial. Modify this value to mirror your own configuration. The full Service definition is listed below:</p>

<pre><code class="language-line_numbers">[label skydns-svc.yaml ]

apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: &quot;true&quot;
    kubernetes.io/name: &quot;KubeDNS&quot;
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.3.0.10
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP

</code></pre>

<p>In our Service object metadata we attach the same <code>k8s-app</code> label as our Pods and Replication Controller as well as the necessary labels for Kubernetes add-on services. In our <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_servicespec">ServiceSpec</a> our selector, used to route traffic to Pods with matching labels, only specifies the <code>k8s-app</code> label. This does not specify the version, allowing us to do rolling updates of our DNS add-on in the future, see the <a href="http://kubernetes.io/v1.1/docs/user-guide/update-demo/README.html">Rolling Update Example</a> for more details. Finally, we also define named ports for the DNS service on both TCP and UDP protocols. We will later confirm SRV records exist for these named ports of the <code>kube-dns</code> service in the <code>kube-system</code> namespace itself.</p>

<blockquote>
<p><strong>Note</strong>: Multiple yaml documents can be concatenated with the <code>---</code> separator. We may simplify management of multiple resources by grouping them together in the same file separated by <code>---</code>, we may just specify multiple resources through multiple <code>-f</code> arguments for the <code>kubectl create</code> command. See the official <a href="http://kubernetes.io/v1.1/docs/user-guide/managing-deployments.html#organizing-resource-configurations">Managing Deployments Guide</a></p>
</blockquote>

<p>The resources will be created in the order they appear in the file. Therefore, it&rsquo;s best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the replication controller(s).</p>

<p>use kubectl with multiple <code>-f</code> arguments:</p>

<pre><code class="language-command">kubectl create -f ./skydns-svc.yaml -f ./skydns-rc.yaml
</code></pre>

<p>And wait for the DNS add-on to start running</p>

<pre><code class="language-command">kubectl get pods --namespace=kube-system | grep kube-dns-v9
</code></pre>

<p>Create a Busybox pod to test the DNS from within the cluster using the following Pod manifest:</p>

<pre><code class="language-line_numbers">[label busybox.yaml]

apiVersion: v1
kind: Pod
metadata:
  name: busybox
  namespace: default
spec:
  containers:
  - image: busybox
    command:
      - sleep
      - &quot;3600&quot;
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Always
</code></pre>

<p>This busybox will sleep for 1 hour before exiting and being restarted by the kubelet, we will use it to test <code>nslookup</code> commands from within the cluster. Create the Pod:</p>

<pre><code class="language-command">kubectl create -f busybox.yaml
</code></pre>

<p>Although it seems we are only creating a Pod, Kubernetes will create a Replication Controller to manage this Pod for us. After a few seconds, confirm the Pod is running:</p>

<pre><code class="language-command">kubectl get pods busybox
</code></pre>

<p>When the Pod is running, output will look as follows:</p>

<pre><code>[secondary_label Output]
NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          14s
</code></pre>

<p>Do a DNS lookup from within the busybox Pod on your client&rsquo;s terminal with the following command:</p>

<pre><code>kubectl exec busybox -- nslookup kubernetes.default
</code></pre>

<p>The expected output should look as follows:</p>

<pre><code>[secondary_label Output]
Server:    10.3.0.10
Address 1: 10.3.0.10

Name:      kubernetes.default
Address 1: 10.3.0.1
</code></pre>

<p>If you are using the <code>skynetservices/skydns:5.2.3a</code> image, you may use the <code>dig</code> binary within to confirm the SRV records for the named ports are served as expected (the nslookup utility bundled in the busybox Pod does not support SRV queries).</p>

<p>To do this, get the name of the <code>kube-dns</code> Pod created by the <code>kube-dns</code> Replication Controller (Pod names are dynamic and change when they are restarted):</p>

<pre><code>dns_pod=`kubectl --namespace=kube-system get po | grep kube-dns | awk '{ print $1}'`
</code></pre>

<p>Open an interactive shell into the SkyDNS container of your kube-dns pod:</p>

<pre><code class="language-command">kubectl --namespace=kube-system exec $dns_pod -c skydns -it sh
</code></pre>

<p>We specify the container we want to execute commands in through the <code>-c</code> option of the <code>kubectl exec</code> command.</p>

<p>Use <code>dig</code> to query the SRV record for the port named dns using the UDP protocol and <code>sed</code> to only print the response from the <code>ANSWER SECTION</code> to the <code>Query Time</code> lines:</p>

<pre><code class="language-command">dig @localhost SRV _dns._udp.kube-dns.kube-system.svc.cluster.local | sed -n '/ANSWER SECTION:/,/Query time/ p'
</code></pre>

<p>We are using <code>sed</code> with the <code>-n</code> option to suppress all output, we specify a range of regular expression patterns (<code>/ANSWER SECTION/,/Query time/</code>) and instruct <code>sed</code> to print only lines within this range with the <code>p</code> command</p>

<p>The expected output should look as follows:</p>

<pre><code>[secondary_label Output]
;; ANSWER SECTION:
_dns._udp.kube-dns.kube-system.svc.cluster.local. 30 IN SRV 10 100 53 kube-dns.kube-system.svc.cluster.local.

;; ADDITIONAL SECTION:
kube-dns.kube-system.svc.cluster.local. 30 IN A 10.3.0.10

;; Query time: 3 msec
</code></pre>

<p>As you can see, using the SRV records created by the kube-dns add-on, we are able to get the port as well as the IP.</p>

<p>Refer also to the <a href="http://kubernetes.io/v1.1/docs/admin/dns.html">Official DNS Integration documentation</a> and the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns">DNS Add-on repository</a>.</p>

<h2 id="step-11-deploying-kubernetes-ready-applications:a91459261407d5d36808cf519d4f7594">Step 11 — Deploying Kubernetes-ready applications</h2>

<p>You should now have a Kubernetes cluster set up and be able to deploy Kubernetes-ready applications.</p>

<p>To better understand the inner workings of Kubernetes from a Pod perspective, we may use the <a href="https://github.com/kelseyhightower/inspector">Kubernetes Pod Inspector</a> application by Kelsey Hightower. With the below yaml file combining both the Replication Controller and Service, you can quickly deploy and expose this application on your cluster:</p>

<pre><code class="language-line_numbers">[label kube-inspector.yaml]

apiVersion: v1
kind: Service
metadata:
  name: inspector
  labels:
    app: inspector
spec:
  type: NodePort
  selector:
    app: inspector
  ports:
  - name: http
    nodePort: 31000
    port: 80
    protocol: TCP

---

apiVersion: v1
kind: ReplicationController
metadata:
  name: inspector-stable
  labels:
    app: inspector
    track: stable
spec:
  replicas: 1
  selector:
    app: inspector
    track: stable
  template:
    metadata:
      labels:
        app: inspector
        track: stable
    spec:
      containers:
      - name: inspector
        image: b.gcr.io/kuar/inspector:1.0.0

</code></pre>

<p>As seen previously, we provide our Replication Controller with the necessary labels and use the <code>b.gcr.io/kuar/inspector:1.0.0</code> image. Note that we are exposing the inspector application by telling Kubernetes to open port <code>31000</code> on every worker node (this will work if you ran the API service with <code>--service-node-port-range=30000-37000</code> as shown in Step 6 of this Tutorial).</p>

<p>Expected Output:</p>

<pre><code>[secondary_label Output]
You have exposed your service on an external port on all nodes in your
cluster.  If you want to expose this service to the external internet, you may
need to set up firewall rules for the service port(s) (tcp:31000) to serve traffic.

See http://releases.k8s.io/release-1.1/docs/user-guide/services-firewalls.md for more details.
service &quot;inspector&quot; created
replicationcontroller &quot;inspector-stable&quot; created
</code></pre>

<p>We can now point our web browser to <code>http://$WORKER_PUBLIC_IP:31000/env</code> on any worker node to access the Inspector Pod and view all environment variables published by the kubelet as well as access <code>http://$WORKER_PUBLIC_IP:31000/mnt?path=/var/run/secrets/kubernetes.io/serviceaccount</code> to see the secrets mounted into the Pods. To see how Kubernetes-ready applications can use these, refer to the <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/pkg/client/unversioned/helper.go#L324">InClusterConfig</a> function of the client Kubernetes client helper library and the <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/cluster/addons/dns/kube2sky/kube2sky.go#L491">KubeClient Setup</a>) section of Kube2Sky as an example implementation.</p>

<p>You may now proceed to set up a multi-tier web application (Guestbook) from the official Kubernetes documentation to visualize how the various Kubernetes components fit together.</p>

<p>See <a href="http://kubernetes.io/v1.1/examples/guestbook-go/README.html">the Guestbook Example app</a> from the Official Kubernetes documentation.</p>

<h2 id="conclusion:a91459261407d5d36808cf519d4f7594">Conclusion</h2>

<p>Following this Tutorial you have created a fully functional Kubernetes cluster. This gives you a great management and scheduling interface for working with services in logical groupings. As you have used many of the Kubernetes concepts to set up the cluster itself, you have a deep understanding of many of the core concepts and deployment workflow of Kubernetes. To review all the Kubernetes concepts, refer to the official <a href="http://kubernetes.io/v1.1/docs/user-guide/README.html#concept-guide">Kubernetes Concept Guide</a>.</p>

<p>You probably noticed that the steps above were still very manual, but the Cloud-config files created are flexible enough for you to automate the process.</p>

<h2 id="deleting-your-kubernetes-cluster:a91459261407d5d36808cf519d4f7594">Deleting your Kubernetes Cluster</h2>

<!-- probably won't include this section in the final tutorial...? -->

<p>If you decide you do no longer want to run this cluster (or want to start from scratch), below are the commands to do so:</p>

<p><strong>NOTE</strong> These commands destroy your cluster and all the data contained within without any backups, these commands are irreversible.</p>

<p>Repeat for every controller</p>

<pre><code class="language-command">doctl d d kube-controller-01.$region
</code></pre>

<p>Repeat for every worker</p>

<pre><code class="language-command">doctl d d kube-worker-01.$region
</code></pre>

<p>Delete all Kubernetes data</p>

<pre><code class="language-command">doctl d d etcd-01.$region
</code></pre>

<p>Delete the apiserver &amp; worker certificates as they are tied to the IPs of the Droplets, but keep the Admin and CA certificates.</p>

<pre><code class="language-command">rm ~/.kube/apiserver*.{pem,csr}
rm ~/.kube/worker*.{pem,csr}
rm *.srl
rm *openssl.cnf
</code></pre>

</div>


  <footer>
  <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
      
  
    <nav><ul class="pager">
    
        <li class="previous">
          <a href="http://so0k.github.io/2016/01/07/about-this-blog-and-my-setup/" title="About This Blog and My Setup">
            <span aria-hidden="true">&larr;</span>Previous
          </a>
        </li>
    

    
    </ul> </nav>
  


</div>

  <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
  

</div>

</footer>


</div>
</div>
      <footer class="footer">
    <div class="container hidden-print">
    <div class="row">
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
           <div class="pull-left">
  <a class="toplink" href="#">back to top</a>
</div>
<div class="pull-right">
  
</div>

        </div>
        <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12 text-center">
          <small>
              
    
<div class="container footline">
  

</div>


    
<div class="container copyright">
  
  Copyright &copy; 2015. All rights reserved.


</div>



          </small>
        </div>
     </div>
    </div>
</footer>

    <script src="http://so0k.github.io/js/jquery.min-2.1.4.js"></script>
<script src="http://so0k.github.io/js/bootstrap.min-3.3.5.js"></script>




<script src="http://so0k.github.io/js/highlight.pack.js"></script>
<script src="http://so0k.github.io/js/site.js"></script>
<script>hljs.initHighlightingOnLoad();</script>


<script>
  var _gaq=[['_setAccount','Your Google Analytics tracking code'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>


  </body>
</html>

