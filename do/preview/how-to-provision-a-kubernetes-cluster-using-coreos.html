<html class="wf-proximanova-n4-active wf-proximanova-n6-active wf-proximanova-i4-active wf-proximanova-n1-active wf-proximanova-n3-active wf-proximanova-n7-active wf-active">
<head>
  <link rel="stylesheet" type="text/css" href="/assets/css/do_min.css">
 <!--<script src="/assets/js/do-app_min.js"></script>-->
</head>
<body class="markdown-controller">
<header>
  <div class="main-menu-container">
    <div class="wrapper">
      <div class="logo">
          <span class="icon icon-cloud" style="color:#2481b7;"></span>
          <span class="logo-community-text" style="color:#2481b7;" >Preview</span>
      </div>
      
      <a class="mobile-nav-toggle icon-menu-thin" data-role="left">
        <span>Menu</span>
      </a>
      <nav>
        <ul>
          <li><a href="/2016/01/07/kubernetes-workshop-for-docker-saigon/">Back to blog post</a></li>
        </ul>
      </nav>
      <div class="clearfix"></div>
    </div>
  </div>
</header>
  <div class="wrapper layout-wrapper">
    <div class="pure-g">
      <div class="pure-u-1-1">
        <div class="question-single">
          <div id="markdown-preview-output" class="markdown-preview-output question-content content-body">
<!-- PREVIEW CONTENT HERE -->      

<h1 id="how-to-provision-a-kubernetes-cluster-using-coreos">How To Provision a Kubernetes Cluster Using CoreOS</h1>

<h3 id="introduction">Introduction</h3>

<p>Kubernetes is a system designed internally within Google to manage applications built within containers across a cluster of nodes. It handles the entire life cycle of a containerized application including deployment and scaling.</p>

<p>With AWS, Red Hat, Microsoft, IBM, Mirantis OpenStack, and VMware (and the list keeps growing) working to integrate Kubernetes into their platforms, going through this tutorial will provide you with a strong foundation and fundamental understanding of a framework that is here to stay.</p>

<p>In this tutorial, we will give step-by-step instructions on how to create a single-controller/multi-worker Kubernetes cluster on CoreOS hosted by Digital Ocean. This system will allow us to group related services together for deployment as a unit on a single host using what Kubernetes calls "<a href="https://coreos.com/kubernetes/docs/latest/pods.html" rel="nofollow">Pods</a>". Kubernetes also provides health checking functionality, high availability, and efficient usage of resources through schedulers.</p>

<p>This tutorial was tested with Kubernetes v1.1.2. Keep in mind that this software changes frequently. To see your version, once it's installed, run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl version
</li></ul></code></pre>
<h3 id="prerequisites-and-goals">Prerequisites and goals</h3>

<p>We will provision each component of our Kubernetes cluster as part of this tutorial, no existing architecture is required. Experience with Docker is expected, experience with Systemd and CoreOS is a plus, but each concept is introduced and explained as part of this tutorial. If you are not familiar with CoreOS, it may be helpful to review <a href="https://www.digitalocean.com/community/tutorials/an-introduction-to-coreos-system-components" rel="nofollow">some basic information about the CoreOS system</a>.</p>

<p>After a high-level overview of the Kubernetes Architecture, we will configure our client machine to work with our Digital Ocean resources from the terminal using Doctl &amp; Jq. Once this is done we will be able to quickly and repeatedly provision our droplets with <code>cloud-config</code> files. This allows us to declaratively customize network configuration, Systemd units, and other OS-level items.</p>

<p>We will first provision an Etcd cluster to reliably provide storage of meta data across a cluster of machines. Etcd provides a great way to store configuration data reliably for Kubernetes. Thanks to the watch support provided by Etcd, coordinating components can be notified very quickly of changes. This component is crucial to our Kubernetes cluster.</p>

<p>With the help of our Etcd cluster, we will also configure <a href="https://coreos.com/flannel/docs/latest/flannel-config.html" rel="nofollow">Flannel</a>, a network fabric layer that provides each machine with an individual subnet for container communication. This satisfies a fundamental requirement for running a Kubernetes cluster. Docker will be configured to use this networking layer for its containers. </p>

<p>We will provision our Kubernetes controller Droplet and to ensure the security of our Kubernetes cluster, we will generate the required certificates for communication between Kubernetes components using <code>openssl</code> &amp; securely transfer these using <code>scp</code> to each Droplet. We will configure the command line client utility, <code>kubectl</code>, to work with our cluster from our client next.</p>

<p>Finally, we will provision worker nodes pointing to the controller nodes and deploy the internal cluster DNS through the DNS add-on. We will have a fully functional Kubernetes cluster allowing us to deploy our workloads and easily add worker nodes as required with the <code>cloud-config</code> files defined in this tutorial.</p>

<p>Working through this tutorial may take you a few hours, but it will give you a good understanding of the moving pieces of your cluster and set you up for success in the long run.</p>

<p>The structure and idea for this tutorial was taken from the <a href="https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/getting-started.md" rel="nofollow">Getting started with CoreOS and Kubernetes Guide</a> and updated with detailed step by step instructions for Digital Ocean. Let's get started. </p>

<h2 id="kubernetes-architectural-overview">Kubernetes Architectural Overview</h2>

<p>In this section we will give an overview of the Kubernetes Architecture. For a more detailed look, refer to <a href="http://kubernetes.io/v1.1/docs/design/architecture.html" rel="nofollow">the official Kubernetes documentation</a>.</p>

<p>At a high level, we need to differentiate the services that run on every node, referred to as node agents (<code>kubelet</code>, ... ), the controller services (APIs, scheduler, ...) that compose the cluster-level control plane and the distributed storage solution (Etcd).</p>

<p>A crucial component on every node is the <strong>kubelet</strong>. The kubelet is responsible for what's running on each individual Droplet and making sure it keeps running. The kubelet controls the container runtime, in this tutorial <strong>Docker</strong> provides the container runtime and must also run on each node. Docker takes care of the details of downloading images and running containers. The kubelet registers nodes with the cluster, sends events and status updates and reports the resource utilization of the node.</p>

<p>To facilitate routing between containers as well as simplify service discovery, each node also runs the <strong>kube-proxy</strong>. The proxy is a simple network proxy and load balancer which can do simple TCP and UDP stream forwarding (round robin) across a set of back ends. The proxy is a crucial part for the Kubernetes services model. The proxy communicates with the controller services to keep up to date. See the <a href="https://github.com/kubernetes/kubernetes/wiki/Services-FAQ" rel="nofollow">Kubernetes' services FAQ</a> for more details.</p>

<p>Worker node services are configured to be managed from the controller services. &lt;!-- this is meant to highlight the difference between the same service deployed on worker nodes vs controller nodes. On worker nodes, the services register with the controller nodes, on controller nodes they are often bootstrapped... --&gt;</p>

<p>The first controller service we will highlight is the <strong>API server</strong>. The API server serves up the <a href="http://kubernetes.io/v1.1/docs/api.html" rel="nofollow">Kubernetes API</a> through a REST interface. It is intended to be a CRUD-y service, with most/all business logic implemented in separate components or in plug-ins. It's responsible for validating the requests and updating the corresponding objects in Etcd. The API server is void of state and will be the main component replicated and load balanced across controller nodes in a High Availability configuration.</p>

<p>The second controller service to highlight is the <strong>scheduler</strong>. The scheduler is responsible for assigning workloads to nodes in the cluster. This component uses the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_binding" rel="nofollow">binding</a> API to achieve this. The scheduler is pluggable and support for multiple cluster schedulers and even user-provided schedulers is expected, but not available yet when this tutorial was written.</p>

<p>All other cluster-level functions are performed by the <strong>controller manager</strong> component at the time of writing. This component embeds the core control loops shipped with Kubernetes. Each controller is a control loop that watches the shared state of the cluster through the API Server and makes changes attempting to move the current state towards the desired state. These controllers may eventually be split into separate components in future Kubernetes versions to make them independently pluggable.</p>

<p>As the scheduler and controller manager components modify cluster state, only one instance of each can run within the cluster. In High Availability configurations a process of master election is required for these components. We will explain and apply master election for these components as part of this tutorial, we will however only provision 1 controller node and no control plane load balancer. Setting up the control plane load balancers and appropriate TLS artifacts are left as an exercise for the reader.</p>

<p>Below is a high level diagram of these Kubernetes components in a High Availability set-up.</p>

<p>&lt;!-- created with http://asciiflow.com/ --&gt;</p>
<pre class="code-pre "><code langs=""> Etcd                             Controller Nodes                                           Worker Nodes
                                    +--------------------+                                     +--------------------+
                                    |                    |                                     |                    |
+--------------------+          +---+ API Server         &lt;---------+  +------------------------+ Kubelet            |
|                    |          |   |                    |         |  |                        |                    |
| Etcd  cluster      &lt;----------+   | Controller Manager*|         |  |                        | Docker             |
|                    |              |                    |         |  |                        |                    |
|                    |              | Scheduler*         |         |  |                        | Proxy              |
|                    |              |                    |         |  |                        |                    |
|                    |              | Kubelet            |         |  |                        |                    |
|                    |              |                    |         |  |                        |                    |
|                    |              | Docker             |         |  |                        |                    |
|                    |              |                    |       +-+--v---------------+        |                    |
|                    |              | Proxy              |       |                    |        |                    |
|                    |              |                    |       | Control Plane      |        |                    |
|                    |              +--------------------+       |                    |        +--------------------+
|                    |                                           | Load Balancer      |
+-^--^---------------+              +--------------------+       |                    |        +--------------------+
  |  |                              |                    |       |                    |        |                    |
  |  +------------------------------+ API Server         &lt;-------+                    &lt;--------+ Kubelet            |
  |                                 |                    |       |                    |        |                    |
  |                                 | Kubelet            |       |                    |        | Docker             |
  |                                 |                    |       |                    |        |                    |
  |                                 | Docker             |       |                    |        | Proxy              |
  |                                 |                    |       |                    |        |                    |
  |                                 | Proxy              |       |                    |        |                    |
  |                                 |                    |       +-+--^---------------+        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 |                    |         |  |                        |                    |
  |                                 +--------------------+         |  |                        +--------------------+
  |                                                                |  |
  |                                 +--------------------+         |  |                        +--------------------+
  |                                 |                    |         |  |                        |                    |
  +---------------------------------+ API Server         &lt;---------+  +------------------------+ Kubelet            |
                                    |                    |                                     |                    |
                                    | Kubelet            |                                     | Docker             |
                                    |                    |                                     |                    |
                                    | Docker             |                                     | Proxy              |
                                    |                    |                                     |                    |
                                    | Proxy              |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    |                    |                                     |                    |
                                    +--------------------+                                     +--------------------+

</code></pre>
<p>Refer to the official diagram for a more detailed break down: http://kubernetes.io/v1.1/docs/design/architecture.png?raw=true</p>

<h2 id="step-1-—-configuring-our-client-machine">Step 1 — Configuring Our Client Machine.</h2>

<p>As the first step in this tutorial we will ensure our client machine is correctly configured to complete all subsequent steps.</p>

<p>The default folder for storing Kubernetes cluster certificates and config-related files is <code>$HOME/.kube/</code>. For this tutorial, we will store our cluster configuration and certificates in this folder, ensure the folder exists:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">mkdir ~/.kube
</li></ul></code></pre>
<p>We will be using the Digital Ocean Control TooL (<a href="https://github.com/digitalocean/doctl" rel="nofollow">Doctl</a>) as well as the Command-line JSON processor <a href="https://github.com/stedolan/jq" rel="nofollow">Jq</a> to manage our Digital Ocean resources from our terminal. This will allow us to quickly repeat commands and automate our Kubernetes cluster setup further down the line.</p>

<p>We will set up Doctl and Jq as well as introduce the basics on how to use these tools within this step. </p>

<p>At the end of this step a correctly configured client environment is expected, if you skip this step ensure first that you have:</p>

<ol>
<li>Configured your environment to create and destroy Droplets in a single Digital Ocean region from the terminal. Ensure you set the <code>$region</code> and <code>$DIGITALOCEAN_API_KEY</code> variables for the rest of this tutorial.</li>
<li>Created the SSH key for all Droplets in our cluster. Ensure the private key is loaded with your SSH agent and the public key is stored as a Digital Ocean resource named <code>k8s-key</code>.</li>
</ol>

<p>Follow the sub-steps to achieve this.</p>

<h3 id="setting-up-doctl">Setting up Doctl</h3>

<p>To use Doctl from your terminal and follow the Kubernetes cluster config from this tutorial, you will need to generate a Personal Access Token with write permissions through the Digital Ocean Control Panel. Refer to <a href="https://www.digitalocean.com/community/tutorials/how-to-use-the-digitalocean-api-v2#how-to-generate-a-personal-access-token" rel="nofollow">this tutorial</a> for information on how to do this, continue with these steps once you have your Personal Access Token ready.</p>

<p>For all of the steps in this tutorial, we will assign our token to a variable called <code>DIGITALOCEAN_API_KEY</code>. For example, by running the following command in bash (replace the highlighted text with your own token):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">export DIGITALOCEAN_API_KEY=<span class="highlight">77e027c7447f468068a7d4fea41e7149a75a94088082c66fcf555de3977f69d3</span>
</li></ul></code></pre>
<p>Review <a href="https://github.com/digitalocean/doctl/releases/latest" rel="nofollow">the latest Doctl release</a> and choose the right binary archive for your environment:</p>

<table class="pure-table"><thead>
<tr>
<th>Operating System</th>
<th>Binary</th>
</tr>
</thead><tbody>
<tr>
<td>OSX</td>
<td>darwin-amd64-doctl.tar.bz2</td>
</tr>
<tr>
<td>Linux</td>
<td>linux-amd64-doctl.tar.bz2</td>
</tr>
<tr>
<td>Windows</td>
<td>windows-amd-64-doctl.zip</td>
</tr>
</tbody></table>

<p>For example, to download the archive for the <code>0.0.16</code> release (used in this tutorial) to your home directory on a Linux 64-bit host, run the following commands in your terminal:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -Lo ~/doctl.tar.bz2 https://github.com/digitalocean/doctl/releases/download/0.0.16/linux-amd64-doctl.tar.bz2
</li></ul></code></pre>
<p>Next, we need to extract the downloaded archive. We will also need to add <code>doctl</code> to a location included in our <code>PATH</code> environment variable, <code>/usr/bin</code> or <code>/opt/bin</code> for example. The following command will extract <code>doctl</code> directly to <code>/usr/bin</code> making it available for all users on a Linux host. This command requires <code>sudo</code> rights:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="#">tar xjf doctl.tar.bz2 -C /usr/bin
</li></ul></code></pre>
<p>Finally, validate that <code>doctl</code> has been downloaded successfully by confirming the installed version:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl --version
</li></ul></code></pre>
<p>If you followed the steps above, this should return:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>doctl version 0.0.16
</code></pre>
<h3 id="finding-help-about-doctl">Finding help about Doctl</h3>

<p>An overview of Doctl and several usage examples are available on <a href="https://github.com/digitalocean/doctl" rel="nofollow">the Doctl GitHub repository</a>. Additionally, invoking Doctl without any arguments will print out usage instructions as well. Note that every Digital Ocean resource type has a corresponding Doctl command. Every command has subcommands to manage the resource as well as instructions available through the <code>help</code> subcommand or the <code>--help</code> flag. </p>

<p>For example, to review the available commands for <code>droplet</code> resources, run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl droplet help
</li></ul></code></pre>
<p>This should return:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>NAME:
   doctl droplet - Droplet commands. Lists by default.

USAGE:
   doctl droplet [global options] command [command options] [arguments...]

VERSION:
   0.0.16

COMMANDS:
   create, c            Create droplet.
   list, l              List droplets.
   find, f              &lt;Droplet name&gt; Find the first Droplet whose name matches the first argument.
   destroy, d           [--id | &lt;name&gt;] Destroy droplet.
   reboot               [--id | &lt;name&gt;] Reboot droplet.
   power_cycle          [--id | &lt;name&gt;] Powercycle droplet.
   shutdown             [--id | &lt;name&gt;] Shutdown droplet.
   poweroff, off        [--id | &lt;name&gt;] Power off droplet.
   poweron, on          [--id | &lt;name&gt;] Power on droplet.
   password_reset       [--id | &lt;name&gt;] Reset password for droplet.
   resize               [--id | &lt;name&gt;] Resize droplet.
   help, h              Shows a list of commands or help for one command

GLOBAL OPTIONS:
   --help, -h   show help
</code></pre>
<p>Notice that the <code>droplet</code> command will <code>list</code> droplets by default. </p>

<p>To get more information about the <code>droplet</code> <code>create</code> command, run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl droplet create --help
</li></ul></code></pre>
<p>This should return:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>NAME:
   create - Create droplet.

USAGE:
   command create [command options] [arguments...]

OPTIONS:
   --domain, -d                         Domain name to append to the hostname. (e.g. server01.example.com)
   --add-region                         Append region to hostname. (e.g. server01.sfo1)
   --user-data, -u                      User data for creating server.
   --user-data-file, --uf               A path to a file for user data.
   --ssh-keys, -k                       Comma seperated list of SSH Key names. (e.g. --ssh-keys Work,Home)
   --size, -s "512mb"                   Size of Droplet.
   --region, -r "nyc3"                  Region of Droplet.
   --image, -i "ubuntu-14-04-x64"       Image slug of Droplet.
   --backups, -b                        Turn on backups.
   --ipv6, -6                           Turn on IPv6 networking.
   --private-networking, -p             Turn on private networking.
   --wait-for-active                    Don't return until the create has succeeded or failed.
</code></pre>
<h3 id="setting-up-your-ssh-keys-with-doctl">Setting up your SSH Keys with Doctl</h3>

<p>Every CoreOS droplet that you will provision for your Kubernetes cluster, will need to have at least one SSH public key installed during its creation process. The key(s) will be installed to the <code>core</code> user's authorized keys file, and you will need the corresponding private key(s) to log in to your CoreOS server.</p>

<p>If you do not already have any SSH keys associated with your Digital Ocean account, do so now by following steps one and two of this tutorial: <a href="https://www.digitalocean.com/community/tutorials/how-to-use-ssh-keys-with-digitalocean-droplets" rel="nofollow">How To Use SSH Keys with Digital Ocean Droplets</a>. You may opt to use Doctl to add the new SSH keys to your account rather then copying the SSH Keys into the Digital Ocean control panel manually. This can be achieved by passing in your <code>DIGITALOCEAN_API_KEY</code> environment variable as the <code>--api-key</code> to Doctl and adding the public key of your newly created SSH key with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl --api-key $DIGITALOCEAN_API_KEY keys create &lt;key-name&gt; &lt;path-to-public-key&gt;
</li></ul></code></pre>
<p><strong>Note</strong>: Doctl will automatically try to use the <code>$DITIGALOCEAN_API_KEY</code> env variable as the <code>--api-key</code> if it exists and we do not need to explicitly pass it in every time. We will omit this in future Doctl commands.<br></p>

<p>Add your private key to your SSH agent on your client machine, using <code>ssh-agent</code> as follows:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh-add &lt;path-to-private-key&gt;
</li></ul></code></pre>
<p>or use the <code>-i &lt;path-to-private-key&gt;</code> flag each time connecting to your droplets over <code>ssh</code> if you do not have a running <code>ssh-agent</code>.</p>

<p>For example, in this tutorial we will store our key pair in our home directory as <code>~/.ssh/id_rsa</code> and upload the public key as <code>k8s-key</code> to our Digital Ocean account, combining all the above steps together, would look like this:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div># Generate the key pair
ssh-keygen -t rsa
Generating public/private rsa key pair.
Enter file in which to save the key (/home/demo/.ssh/id_rsa):
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /home/demo/.ssh/id_rsa.
Your public key has been saved in /home/demo/.ssh/id_rsa.pub.
The key fingerprint is:
4a:dd:0a:c6:35:4e:3f:ed:27:38:8c:74:44:4d:93:67 demo@a
The key's randomart image is:
+--[ RSA 2048]----+
|          .oo.   |
|         .  o.E  |
|        + .  o   |
|     . = = .     |
|      = S = .    |
|     o + = +     |
|      . o + o .  |
|           . o   |
|                 |
+-----------------+
# Upload public key to Digital Ocean account as k8s-key
doctl keys create k8s-key /home/demo/.ssh/id_rsa.pub
# Add private key to SSH Agent
ssh-add ~/.ssh/id_rsa
</code></pre>
<h3 id="managing-droplets-with-doctl">Managing Droplets with Doctl</h3>

<p>To verify that your account &amp; keys are setup correctly, we will create a new CoreOS Alpha droplet named <code>"do-test"</code> from the terminal.</p>

<p>&lt;!-- note that this will cause a charge? --&gt;</p>

<p>For the remainder of this tutorial, we will be creating all droplets within the same Digital Ocean region. Choose your region and store it into a variable called <code>$region</code>. Review the list of all available regions by running the <code>doctl region</code> command first.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl region
</li></ul></code></pre>
<p>For example, we will use the Amsterdam 2 region for the rest of this tutorial. Choose the region most appropriate for your case:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">export region="ams2"
</li></ul></code></pre>
<p>Now create the Droplet with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl droplet create \
</li><li class="line" prefix="$">    --image "coreos-alpha" \
</li><li class="line" prefix="$">    --size "512mb" \
</li><li class="line" prefix="$">    --region "$region" \
</li><li class="line" prefix="$">    --private-networking \
</li><li class="line" prefix="$">    --ssh-keys k8s-key \
</li><li class="line" prefix="$">    "do-test"
</li></ul></code></pre>
<p>With the above command, we created a <code>"512mb"</code> droplet, in the region of our choice, requesting a <code>private_ip</code> and adding our ssh-key (<code>k8s-key</code>) to the droplet for remote access. Once the command completed, Doctl returns information about the new Digital Ocean resource that was just created.</p>

<p>First, confirm you can list all your droplets and their status with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl droplet list
</li></ul></code></pre>
<p>This should output a list similar to below:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>ID          Name            IP Address              Status  Memory  Disk    Region
8684261     do-test.ams2    198.211.118.106         new     512MB   20GB    ams2
</code></pre>
<p>Note that, to speed up its usage, Doctl has several shortcuts. For example, the shortcut for the <code>droplet</code> command is <code>d</code>. Moreover, the default action for the <code>droplet</code> command is <code>list</code>, allowing us to re-write the above command as follows:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d
</li></ul></code></pre>
<p>Returning the same results as before. On Linux you can <code>watch</code> this list to capture when the droplet status changes from <code>new</code> to <code>active</code> (which will take the same amount of time it would take when provisioning through the web control panel).</p>

<p>Once the CoreOS droplet has fully been provisioned and its status changed from <code>new</code> to <code>active</code>, ensure your SSH Key was added correctly by connecting as the <code>core</code> user, run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@<span class="highlight">198.211.118.106</span>
</li></ul></code></pre>
<p>Replace the IP highlighted above by the public IP of your droplet, as listed by the previous <code>doctl d</code> command. As this is the first time you connect to this server, you may be prompted to confirm the fingerprint of the server:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>The authenticity of host '198.211.118.106 (198.211.118.106)' can't be established.
ED25519 key fingerprint is SHA256:wp/zkg0UQifNYrxEsMVg2AEawqSVpRS+3mBAQ6TBNlU.
Are you sure you want to continue connecting (yes/no)?
</code></pre>
<p>For more information about accessing your Droplet remotely, see <a href="https://www.digitalocean.com/community/tutorials/how-to-connect-to-your-droplet-with-ssh#ssh-login-as-root" rel="nofollow">How To Connect To Your Droplet with SSH</a>.</p>

<p>You should now be connected to a fully functional CoreOS droplet running in Digital Ocean. </p>

<p>Press <code>CTRL+D</code> or enter <code>exit</code> to log out. Keep the <code>do-test</code> droplet running to complete the exercises in the next section.</p>

<h3 id="working-with-doctl-responses">Working with Doctl responses</h3>

<p>By default, Doctl will return yaml responses, but it is possible to change the format of the response with the <code>-f</code> flag. Using the <code>json</code> format will allow us to easily act on the data returned by Doctl through the Command-line JSON processor <a href="https://github.com/stedolan/jq" rel="nofollow">Jq</a>. </p>

<p>Jq comes installed on several Linux distributions (i.e. CoreOS). However, to download and setup Jq manually, review <a href="https://github.com/digitalocean/doctl/releases/latest" rel="nofollow">the latest releases</a> and choose the right release  for your environment:</p>

<table class="pure-table"><thead>
<tr>
<th>Operating System</th>
<th>Binary</th>
</tr>
</thead><tbody>
<tr>
<td>OSX</td>
<td>jq-osx-amd64</td>
</tr>
<tr>
<td>Linux</td>
<td>jq-linux64</td>
</tr>
<tr>
<td>Windows</td>
<td>jq-win64.exe</td>
</tr>
</tbody></table>

<p>For example, to download the <code>1.5</code> release from a shell directly to your <code>/usr/bin/</code> directory on a Linux 64-bit host (which requires <code>sudo</code> rights), run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="#">curl -Lo /usr/bin/jq https://github.com/stedolan/jq/releases/download/jq-1.5/jq-linux64
</li></ul></code></pre>
<p>This will make the <code>jq</code> command available for all users. </p>

<p>Validate Jq has been downloaded successfully by confirming the installed version:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">jq --version
</li></ul></code></pre>
<p>If you followed the steps above, this should return:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>jq-1.5
</code></pre>
<p>Using the <code>-f json</code> argument for Doctl together with Jq allows us to, for example, extract the number of CPUs a droplet has:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl -f json d find do-test.$region | jq '.vcpus'
</li></ul></code></pre>
<p>In the above command, the <code>find</code> command for droplets (shortcut <code>f</code>) returns all properties of a droplet matching the name provided, including the droplet's <code>vcpus</code> property. This <code>json</code> data is passed on to Jq with an argument to only return the <code>vcpus</code> property to us.</p>

<p>Another example of using Jq to manipulate the data returned by Doctl is given next, extracting the raw <code>public_key</code> for an existing Digital Ocean SSH Key, the key named <code>k8s-key</code> in our example:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl -f json keys f k8s-key | jq --raw-output '.public_key'
</li></ul></code></pre>
<p>With Output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>ssh-rsa AAAAB3Nza... user@host
</code></pre>
<p>By default Jq will format strings as json strings, but using the <code>--raw-output</code> flag (shortcut <code>-r</code>), as can be seen above, will make Jq write strings directly to standard output. This is very useful for our scripts.</p>

<p>Finally, the real power of Jq becomes evident when we need to retrieve an array of network interfaces (<code>ipv4</code>) assigned to a droplet, filter the array based on a property <code>.type</code> with possible values <code>"public"</code> or <code>"private"</code> and extract the raw value of the <code>ip_address</code> property.</p>

<p>We'll break this down as follows. Notice first that the following command will return an array of all the IPv4 network interfaces assigned to a droplet:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl -f json d f do-test.$region | jq -r '.networks.v4[]'
</li></ul></code></pre>
<p>Which will return a result similar to the following text block:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>{
  "ip_address": "10.129.73.216",
  "netmask": "255.255.0.0",
  "gateway": "10.129.0.1",
  "type": "private"
}
{
  "ip_address": "198.211.118.106",
  "netmask": "255.255.192.0",
  "gateway": "198.211.128.1",
  "type": "public"
}
</code></pre>
<p>Next, we direct Jq to apply a filter to the array of network interfaces based on the <code>type</code> property using the <code>select</code> statement and only return the <code>.ip_address</code> property of the filtered network interface:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl -f json d f do-test.$region | jq -r '.networks.v4[] | select(.type == "private")  | .ip_address'
</li></ul></code></pre>
<p>The above command effectively returns the private <code>ip_address</code> of our droplet directly to standard out. We will use this command often to store droplet IP addresses into environment variables. The output of the command may look like:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>10.129.73.216
</code></pre>
<p>Finally, destroy your <code>do-test</code> droplet with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d d do-test.$region
</li></ul></code></pre>
<p>Which will output:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Droplet do-test.ams2 destroyed.
</code></pre>
<p>For a full explanation of all the features of Jq, kindly refer to <a href="https://stedolan.github.io/jq/manual/" rel="nofollow">the Jq manual</a>.</p>

<h3 id="using-doctl-to-configure-coreos-droplets-with-cloud-config">Using Doctl to configure CoreOS Droplets with cloud-config</h3>

<p>For a short introduction to <code>cloud-config</code> files, kindly refer to the section on <a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-a-coreos-cluster-on-digitalocean#write-a-cloud-config-file" rel="nofollow">writing cloud-config files</a> of the <a href="https://www.digitalocean.com/community/tutorial_series/getting-started-with-coreos-2" rel="nofollow">Getting Started with CoreOS</a> series. We will explain every aspect of <code>cloud-config</code> files we rely on as we write our own in this tutorial.</p>

<p>One of the most useful aspects of <code>cloud-config</code> files is that they allow you to define a list of arbitrary Systemd units to start after booting. To understand these <code>coreos.units</code> better, refer to the <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files" rel="nofollow">understanding Systemd units and unit files tutorial</a>. We will walk you through many Systemd unit examples within this tutorial.</p>

<p>We will heavily rely on these config files to manage the configuration of our droplets, giving us a way to consistently provision our Kubernetes clusters on Digital Ocean in the future. It is important however to note that <code>cloud-config</code> files are not intended as a replacement for configuration management tools such as Chef/Puppet/Ansible/Salt/TerraForm and we may benefit more adopting one of these tools in the long run.</p>

<p>Please ensure you use the <a href="https://coreos.com/validate" rel="nofollow">CoreOS validator</a> to validate any <code>cloud-config</code> file you write as part of this tutorial. Ensuring the config files are valid prior to creating the droplets will help avoid frustration and time loss. Also refer to the <a href="https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-issues-with-your-coreos-servers" rel="nofollow">general troubleshooting tutorial for CoreOS on Digital Ocean</a> when faced with CoreOS issues.</p>

<p>For this tutorial, we will be passing <code>cloud-config</code> files through the <code>--user-data-file</code> option (shortcut <code>--uf</code>) when creating droplets from the terminal with Doctl.</p>

<p>To see how this works, follow the below steps to create a Droplet with a custom <code>motd</code> and automatic reboots switched off, as an exercise.</p>

<p>First, create a <code>test.yaml</code> file in your working directory, with the content as follows. </p>
<div class="code-label " title="test.yaml">test.yaml</div><pre class="code-pre "><code langs="">#cloud-config

write_files:
  - path: "/etc/motd"
    permissions: "0644"
    owner: "root"
    content: |
      Good news, everyone!
coreos:
  update:
    group: alpha
    reboot-strategy: off
</code></pre>
<p>The <code>write_files</code> <a href="https://coreos.com/os/docs/latest/cloud-config.html#write_files" rel="nofollow">directive</a> defines a set of files to create on the local filesystem. For each file, we specify the absolute location on disk through the <code>path</code> key and the data to be write through the <code>content</code> key. The <code>coreos.update.*</code> <a href="https://coreos.com/os/docs/latest/cloud-config.html#update" rel="nofollow">parameters</a> manipulate settings related to how CoreOS instances are updated, setting the <code>reboot-strategy</code> to <code>off</code> will instruct the CoreOS reboot manager (Locksmith) to disable rebooting after updates are applied. </p>

<p>Create a new droplet named <code>ccfg-test</code>, using this <code>test.yaml</code> file with the following command (this command will take about a minute to complete, please be patient):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d c --wait-for-active \
</li><li class="line" prefix="$">    -i "CoreOS-alpha" \
</li><li class="line" prefix="$">    -s 512mb \
</li><li class="line" prefix="$">    -r "$region" \
</li><li class="line" prefix="$">    -p \
</li><li class="line" prefix="$">    -k k8s-key \
</li><li class="line" prefix="$">    -uf test.yaml ccfg-test
</li></ul></code></pre>
<p>We are using the <code>d</code> shortcut to manage our <code>droplet</code> resources and <code>c</code> as a shortcut for <code>create</code>. The <code>--wait-for-active</code> flag will ensure Doctl waits for the droplet to become active before returning control to our terminal, which is why we had to wait.</p>

<p>Once Doctl returned control, you may need to give your Droplet some more time to boot and load the SSH Daemon before attempting to connect.</p>

<p>Try to connect via the public ip of this droplet with the following one-liner.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@`doctl d | grep ccfg-test | awk '{print $3}'`
</li></ul></code></pre>
<p>In this case we are using the droplet listing command piped into <code>grep</code> to filter down to the droplet we just created and we capture the third column, which is the public IP, using <code>awk</code>. The result should be similar to below, confirming our <code>motd</code> was written correctly once we confirm the authenticity of our new droplet:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>The authenticity of host '37.139.21.41 (37.139.21.41)' can't be established.
ED25519 key fingerprint is SHA256:VtdI6P5sRqvQC0dGWE1ffLYTq1yBIWoRFdWc6qcm+04.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '37.139.21.41' (ED25519) to the list of known hosts.
<span class="highlight">Good news, everyone!</span>
</code></pre>
<p>If you are prompted for a password, ensure your SSH Agent has the private key associated with your <code>k8s-key</code> loaded and try again.</p>

<p>If you happened to destroy a droplet directly prior to creating the one that you are connecting to, you may see a warning like this:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
...
</code></pre>
<p>If this is the case, your new droplet probably has the same IP address as the old, destroyed droplet, but it has a different host SSH key. This is fine, and you can remove the warning, by deleting the old droplet's host key from your system, with this command (replacing <code>$SERVER_IP_ADDRESS</code> with your droplet public ip):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh-keygen -R $SERVER_IP_ADDRESS
</li></ul></code></pre>
<p>Now try connecting to your server again.</p>

<p>Finally, destroy this test droplet with a command similar to below:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d d ccfg-test.$region
</li></ul></code></pre>
<p><strong>Note</strong>: At the time of writing, user provided <code>cloud-config</code> files can not be modified once a droplet has been created. To change the <code>cloud-config</code> the droplets need to be re-created. Take this into consideration when writing cloud-config files which limit ssh access to certain user accounts as these may be reset after every reboot.</p>

<p>With the above commands in our toolbox, we are ready to start a highly automated Kubernetes configuration on Digital Ocean.</p>

<h2 id="step-2-—-provisioning-the-data-storage-back-end">Step 2 — Provisioning The Data Storage Back End.</h2>

<p>No matter if you are using Swarm with Docker overlay networks or Kubernetes, a data storage back end is required for the infrastructure meta data.</p>

<p>Kubernetes uses Etcd for data storage and for cluster consensus between different software components. Etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. It is open-source and available on <a href="https://github.com/coreos/etcd" rel="nofollow">GitHub</a>. We will introduce the minimum concepts necessary to set up Etcd for our Kubernetes cluster, full Etcd documentation is available <a href="https://coreos.com/etcd/docs/latest/" rel="nofollow">here</a>. </p>

<p>Your Etcd cluster will be heavily utilized since all objects are stored within and every scheduling decision is recorded. It is recommended that you run a multi-droplet cluster to gain maximum performance and reliability of this important part of your Kubernetes cluster. Of your Kubernetes components, you should only give the <code>kube-apiserver</code> component read/write access to Etcd. You do not want the Etcd cluster used by Kubernetes exposed to every node in your cluster (or worse, to the Internet at large), because access to Etcd is equivalent to root in your cluster.</p>

<p>&lt;!-- TODO: We should share Etcd with flannel..., it means Etcd is exposed on every node... --&gt;</p>

<p>For development &amp; testing environments, a single droplet running Etcd will suffice. </p>

<p>For production environments it is highly recommended that Etcd is ran as a dedicated cluster separately from Kubernetes components. Use the <a href="https://coreos.com/os/docs/latest/cluster-architectures.html" rel="nofollow">CoreOS cluster architecture overview</a> as well as the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md" rel="nofollow">official Etcd clustering guide</a> to bootstrap a new Etcd cluster on Digital Ocean. If you do not have an existing Etcd cluster, you can bootstrap a fresh Etcd cluster on Digital Ocean either by:</p>

<ul>
<li>Using the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md#public-etcd-discovery-service" rel="nofollow">public Etcd discovery service</a> or</li>
<li>Using <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/clustering.md#dns-discovery" rel="nofollow">DNS discovery</a></li>
</ul>

<p>Additionally, refer to the official Etcd guides on <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/security.md" rel="nofollow">securing your Etcd cluster</a> and to get a <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md" rel="nofollow">full overview of Etcd configuration flags</a></p>

<p>In this tutorial, instead of being slowed down and distracted by generating new discovery URLs and bootstrapping Etcd, it's easier to start a single Etcd node. Since the full Etcd daemon isn't running on all of the machines, we'll gain a little bit of extra CPU and RAM to play with. However, for ease of configuration of all the cluster services, we will run a local Etcd in <strong>proxy mode</strong> on every Worker node (this daemon will listen on localhost and proxy all requests to the Etcd node). This allows us to configure every cluster component with the local Etcd proxy.</p>

<p>If you already have an Etcd cluster and wish to skip this step, ensure that you have set the <code>$ETCD_PEER</code> environment variable to your Etcd cluster before proceeding with the rest of this tutorial.</p>

<h3 id="deploying-with-a-single-etcd-node">Deploying with a single Etcd node</h3>

<p>Since we're only using a single Etcd node, there is no need to include a discovery token. There isn't any high availability for Etcd in this configuration, but that's assumed to be OK for development and testing. Boot this machine first so you can configure the rest with its IP address.</p>

<p>Etcd is configurable through command-line flags and environment variables. To start Etcd automatically using custom settings with Systemd, we may store manually created partial Systemd unit drop-ins under: <code>/etc/systemd/system/etcd2.service.d/</code>. </p>

<p>Alternatively, we may use the <a href="https://github.com/coreos/coreos-cloudinit/blob/master/Documentation/cloud-config.md#etcd2" rel="nofollow"><code>coreos.etcd2.*</code></a> parameters in our <code>cloud-config</code> file to let CoreOS automatically generate the Etcd drop-ins on startup for us.</p>

<p><strong>Note</strong>: <code>cloud-config</code> generated drop-ins are stored under <code>/run/systemd/system/etcd2.service.d/</code>.<br></p>
<pre class="code-pre "><code langs="">#cloud-config

coreos:
  etcd2:
    name: "etcd-01"
    advertise-client-urls: "http://$private_ipv4:2379"
    listen-client-urls: "http://$private_ipv4:2379, http://127.0.0.1:2379"
    listen-peer-urls: "http://$private_ipv4:2380, http://127.0.0.1:2380"
    #bootstrapping
    initial-cluster: "etcd-01=http://$private_ipv4:2380"
    initial-advertise-peer-urls: "http://$private_ipv4:2380"
</code></pre>
<p>As we will use a single region for our Kubernetes cluster, we configure our Etcd instance to listen for incoming requests on the <code>private_ip</code> and <code>localhost</code> only, this may give us a little protection from the public Internet - but not from other droplets within the same region. For a production setup, it is recommended to follow the official Etcd guides on <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/security.md" rel="nofollow">securing your Etcd cluster</a>. </p>

<p>We set the <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-listen-client-urls" rel="nofollow"><code>-listen-client-urls</code></a> flag to listen for client traffic and <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-listen-peer-urls" rel="nofollow"><code>-listen-peer-urls</code></a> flag to listen for peer traffic coming from Etcd proxies running on other cluster nodes. We use the <code>$private_ipv4</code> substitution variable made available by the Digital Ocean <a href="https://www.digitalocean.com/community/tutorials/how-to-troubleshoot-common-issues-with-your-coreos-servers#checking-for-access-to-the-metadata-service" rel="nofollow">metadata service</a> in our <code>cloud-config</code> files. We use the <a href="http://www.iana.org/assignments/service-names-port-numbers/service-names-port-numbers.xhtml?search=etcd" rel="nofollow">IANA-assigned</a> Etcd ports <code>2379</code> for client traffic and <code>2380</code> for peer traffic.</p>

<p><strong>Note</strong>: Several Etcd applications, such as <a href="https://github.com/skynetservices/skydns" rel="nofollow">SkyDNS</a>, still rely on Etcd's legacy port <code>4001</code>. We did not configure Etcd to listen on this port, but you may need to do this to support older Etcd applications in your infrastructure.<br></p>

<p>Our Etcd node will advertise itself with it's <code>private_ip</code> to clients as we define <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-advertise-client-urls" rel="nofollow"><code>-advertise-client-urls</code></a> to overwrite the default of <code>localhost</code>. this is important to avoid loops for our <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/proxy.md" rel="nofollow">Etcd proxy</a> running on our worker nodes. We are also required to configure a <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-name" rel="nofollow">name</a> for our Etcd instance to overwrite the default name for static bootstrapping. To bootstrap our single node Etcd cluster we directly provide the <code>initial</code> clustering flags <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-initial-cluster" rel="nofollow"><code>-initial-cluster</code></a> and <a href="https://github.com/coreos/etcd/blob/release-2.2/Documentation/configuration.md#-initial-advertise-peer-urls" rel="nofollow"><code>-initial-advertise-peer-urls</code></a> as we do not rely on cluster discovery. </p>

<p>Next, we tell Systemd to start our Etcd service on boot by providing a unit definition for the <code>etcd2</code> service in the same <code>cloud-config</code> file as well and as this component is crucial and we only have a single node, we turn off the CoreOS <code>reboot-strategy</code> which is <code>on</code> by default.</p>

<p>Combining all of the above, our <code>cloud-config</code> file for our Etcd Droplet should look as follows:</p>
<div class="code-label " title="etcd-01.yaml">etcd-01.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">#cloud-config
</li><li class="line" prefix="3">
</li><li class="line" prefix="4">coreos:
</li><li class="line" prefix="5">  etcd2:
</li><li class="line" prefix="6">    name: "etcd-01"
</li><li class="line" prefix="7">    advertise-client-urls: "http://$private_ipv4:2379"
</li><li class="line" prefix="8">    listen-client-urls: "http://$private_ipv4:2379, http://127.0.0.1:2379"
</li><li class="line" prefix="9">    listen-peer-urls: "http://$private_ipv4:2380, http://127.0.0.1:2380"
</li><li class="line" prefix="10">    #bootstrapping
</li><li class="line" prefix="11">    initial-cluster: "etcd-01=http://$private_ipv4:2380"
</li><li class="line" prefix="12">    initial-advertise-peer-urls: "http://$private_ipv4:2380"
</li><li class="line" prefix="13">  units:
</li><li class="line" prefix="14">    - name: "etcd2.service"
</li><li class="line" prefix="15">      command: "start"
</li><li class="line" prefix="16">  update:
</li><li class="line" prefix="17">    group: alpha
</li><li class="line" prefix="18">    reboot-strategy: off
</li></ul></code></pre>
<p><a href="https://coreos.com/validate" rel="nofollow">Validate</a> your cloud-config file, then create your <code>etcd-01</code> droplet with the following command. :</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d c --wait-for-active \
</li><li class="line" prefix="$">    -i "CoreOS-alpha" \
</li><li class="line" prefix="$">    -s 512mb \
</li><li class="line" prefix="$">    -r "$region" \
</li><li class="line" prefix="$">    -p \
</li><li class="line" prefix="$">    -k k8s-key \
</li><li class="line" prefix="$">    -uf etcd-01.yaml etcd-01
</li></ul></code></pre>
<p>Again we are waiting for the droplet creation to be completed before proceeding. When ready, give the Droplet some time to start the SSH daemon, then connect::</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@`doctl d | grep etcd-01 | awk '{print $3}'`
</li></ul></code></pre>
<p>Confirm Etcd is running:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">systemctl status etcd2
</li></ul></code></pre>
<p>This should return output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>● etcd2.service - etcd2
   Loaded: loaded (/usr/lib64/systemd/system/etcd2.service; disabled; vendor preset: disabled)
  Drop-In: /run/systemd/system/etcd2.service.d
           └─20-cloudinit.conf
   Active: active (running) since Sat 2015-11-11 23:19:13 UTC; 6min ago
 Main PID: 841 (etcd2)
   CGroup: /system.slice/etcd2.service
           └─841 /usr/bin/etcd2

Nov 11 23:19:13 etcd-01.ams2 systemd[1]: Started etcd2.
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: added local member ce2a822cea30bfca [http://10.129.69.201:2379] to cluster 7e27652122e8b2ae
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca is starting a new election at term 1
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca became candidate at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca received vote from ce2a822cea30bfca at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: ce2a822cea30bfca became leader at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: raft.node: ce2a822cea30bfca elected leader ce2a822cea30bfca at term 2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: setting up the initial cluster version to 2.2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: set the initial cluster version to 2.2
Nov 11 23:19:13 etcd-01.ams2 etcd2[841]: published {Name:etcd-01 ClientURLs:[http://10.129.69.201:2379]} to cluster 7e27652122e8b2ae
</code></pre>
<p>Confirm the cluster is healthy: </p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">etcdctl cluster-health
</li></ul></code></pre>
<p>This should return output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>member ce2a822cea30bfca is healthy: got healthy result from http://10.129.69.201:2379
cluster is healthy
</code></pre>
<p>Close the connection to the droplet and note down the Etcd endpoint your kubernetes will use, <code>http://10.129.69.201:2379</code> for clients and <code>http://10.129.69.201:2380</code> for peers, in the example above.</p>

<p>if we were to script this assignment, using Jq we can extract the <code>private_ip</code> property of the droplet and format the result as required:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">export ETCD_PEER=`doctl -f json d f etcd-01.$region | jq -r '.networks.v4[] | select(.type == "private")  | "http://\(.ip_address):2380"'`
</li></ul></code></pre>
<p>Refer to [Working with Doctl responses](#) of Step 1 in this tutorial for a full explanation of the above command and confirm:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo $ETCD_PEER
</li></ul></code></pre>
<p>This should return output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>http://10.129.69.201:2380
</code></pre>
<p>For our cluster nodes, Etcd will run in proxy mode pointing to our Etcd Droplet <code>private_ip</code>. The relevant part of the cluster nodes <code>cloud-config</code> for Etcd proxies will look like this:</p>
<div class="code-label " title="cloud-config-*.yaml - cluster etcd proxy config snippet">cloud-config-*.yaml - cluster etcd proxy config snippet</div><pre class="code-pre "><code langs="">...
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: "etcd-01=ETCD_PEER"
  units:
    - name: "etcd2.service"
      command: "start"
...
</code></pre>
<p>Where we will script the substitution of the <code>ETCD_PEER</code> placeholder with the following <code>sed</code> command, prior to creating the Droplets:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">sed -i -e "s|ETCD_PEER|${ETCD_PEER}|g;" cloud-config-*.yaml
</li></ul></code></pre>
<p><strong>Note</strong>: Because our variable value includes forward slashes, we are using <code>sed</code> with the pipeline "|" character as separator for the "s" command instead of the more common forward slash. Whichever character follows the "s" command is used as the separator by <code>sed</code>.<br></p>

<p>We will proceed with reviewing the networking requirements for Kubernetes and how we achieve them on Digital Ocean in the next section.</p>

<h2 id="step-3-—-configuring-the-network-fabric-layer">Step 3 — Configuring The Network Fabric Layer.</h2>

<p>As explained in the introduction of this tutorial, Kubernetes has the fundamental networking requirement of ensuring that all containers are routable without network translation or port brokering on the hosts. In other words, this means every Droplet is required to have its own IP range within the cluster. To achieve this on Digital Ocean, Flannel will be used to provide an overlay network across multiple Droplets and configure Docker to use this networking layer for its containers.  </p>

<p>Flannel runs an agent, <code>flanneld</code>, on each host which is responsible for allocating a subnet lease out of a pre-configured address space. When enabling Flannel on CoreOS, <a href="https://coreos.com/flannel/docs/latest/flannel-config.html#under-the-hood" rel="nofollow">CoreOS will ensure</a> Docker is automatically configured to use the Flannel overlay network for its containers. </p>

<p>Flannel uses Etcd to store the network configuration, allocated subnets, and auxiliary data (such as host's IP). The Etcd storage back end for Flannel may be ran separately in the cluster. To reduce the complexity in this tutorial however, we will configure Flannel to share the external Etcd cluster with Kubernetes which is acceptable for Testing and Development only. By default, Flannel looks up its configuration under the <code>/coreos.com/network/config</code> key within Etcd. To run Flannel on each node in a consistent way, we are required to publish the Flannel configuration to Etcd under this key.</p>

<p>At the bare minimum, the configuration must provide Flannel an IP range (subnet) that it should use for the overlay network. The IP subnet used by Flannel should not overlap with the public and private IP ranges used by the Digital Ocean Droplets, <code>10.2.0.0/16</code> is the IP range used in this tutorial. This /16 range will be assigned for the entire overlay network and used by containers and Pods across the cluster Droplets. By default, Flannel will allocate a /24 to each Droplet. This default, along with the minimum and maximum subnet IP addresses is <a href="https://coreos.com/flannel/docs/latest/flannel-config.html#publishing-config-to-etcd" rel="nofollow">overridable in config</a>.</p>

<p>The forwarding of packets by Flannel is achieved using one of several strategies that are known as back ends. In this tutorial we will configure Flannel to use the <code>vxlan</code> back end which is built on the performant in-kernel VXLAN tunneling protocol to encapsulate the packets for the overlay network.</p>

<p>If we were to use the <a href="https://github.com/coreos/etcd/tree/master/etcdctl" rel="nofollow"><code>etcdctl</code></a> utility, which is shipped with CoreOS, directly from the terminal of any of our Droplets to publish this configuration, it would look like this:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">etcdctl set /coreos.com/network/config '{"Network":"10.2.0.0/16", "Backend": {"Type": "vxlan"}}
</li></ul></code></pre>
<p>With the above command, <code>etcdctl</code> uses the localhost, which in our hosts will be an Etcd daemon running in proxy mode forwarding the configuration to our Etcd cluster.</p>

<p>To facilitate the cluster bootstrap, we will put this command into a drop-in for the <code>flanneld.service</code> as part of our Kubernetes controller's <code>cloud-config</code>. You can create this <code>cloud-config-controller.yaml</code> file and add the snippets as we go through them in this tutorial, however we will re-order the snippets in the final file and provide a full listing of the file as part of this tutorial.</p>
<div class="code-label " title="cloud-config-controller.yaml - flannel drop-in snippet">cloud-config-controller.yaml - flannel drop-in snippet</div><pre class="code-pre "><code langs="">...
  units:
    - name: flanneld.service
      command: start
      drop-ins:
        - name: 50-network-config.conf
          content: |
            [Unit]
            Requires=etcd2.service
            [Service]
            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network":"10.2.0.0/16", "Backend": {"Type": "vxlan"}}'
...
</code></pre>
<p><strong>Note</strong>: any services that run Docker containers must come after the <code>flanneld.service</code> and should include <code>Requires=flanneld.service</code>, <code>After=flanneld.service</code>, and <code>Restart=always|on-failure</code> directives. These directives are necessary because <code>flanneld.service</code> may fail due to Etcd not being available yet. It will keep restarting and it is important for Docker based services to also keep trying until Flannel is up.<br></p>

<p>In order for Flannel to manage the pod network in the cluster, Docker needs to be configured to use it. All we need to do is require that <code>flanneld</code> is running prior to Docker starting.</p>

<p>We're going to do this with a Systemd drop-in, which is a method for appending or overriding parameters of a Systemd unit. In this case we're appending two dependency rules:</p>
<div class="code-label " title="cloud-config-controller - docker config snippet">cloud-config-controller - docker config snippet</div><pre class="code-pre "><code langs="">...
  units:
    - name: docker.service
      command: start
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
...
</code></pre>
<h2 id="step-4-—-understanding-where-to-get-the-kubernetes-artifacts">Step 4 — Understanding Where To Get The Kubernetes Artifacts</h2>

<p>At the time of writing, the <a href="http://kubernetes.io/v1.1/docs/getting-started-guides/scratch.html#downloading-and-extracting-kubernetes-binaries" rel="nofollow">Official</a> guidelines require the Kubernetes binaries and Docker images wrapping those binaries to be downloaded as part of the full Kubernetes release archive located under the <a href="https://github.com/kubernetes/kubernetes/releases" rel="nofollow">Kubernetes</a> repository on GitHub.</p>

<p>At the same time, all Kubernetes artifacts are also stored on the <code>kubernetes-release</code> bucket on Google cloud storage for every release.</p>

<p>To confirm the current stable release of Kubernetes run:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt
</li></ul></code></pre>
<p>This returned v1.1.2 at the time of writing.</p>

<p>To list all Kubernetes release binaries stored in the Google cloud storage bucket for a particular release, you can either use the python 2.7 based <a href="https://cloud.google.com/storage/docs/gsutil" rel="nofollow">Gsutil</a> from the Google SDK as follows:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">gsutil ls -R gs://kubernetes-release/release/v1.1.2/bin/linux/amd64
</li></ul></code></pre>
<p>Or without Python, directly talking to the <a href="https://cloud.google.com/storage/docs/json_api/v1/buckets/list" rel="nofollow">Google cloud platform API</a> using <code>curl</code> &amp; <code>jq</code> (<a href="https://stedolan.github.io/jq/manual/" rel="nofollow">Jq ref</a>):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$"> curl -sL  https://www.googleapis.com/storage/v1/b/kubernetes-release/o?prefix='release/v1.1.2/bin/linux/amd64' | jq '"https://storage.googleapis.com/kubernetes-release/\(.items[].name)"'
</li></ul></code></pre>
<p>A combined binary is provided as the <a href="https://releases.k8s.io/release-1.1/cmd/hyperkube" rel="nofollow">Hyperkube</a>. This Hyperkube is an all-in-one binary, allowing you to run any Kubernetes component as a subcommand of the <code>hyperkube</code> command. The Hyperkube is also made available within a Docker image. The Dockerfile of this container can be reviewed <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/cluster/images/hyperkube/Dockerfile" rel="nofollow">here</a>.</p>

<p>The plan for the Kubernetes release process is to publish the Kubernetes images on the Google Container Registry, under the <code>google_containers</code> repository: <code>gcr.io/google_containers/hyperkube:$TAG</code>, where TAG is the latest stable release tag (i.e.: <code>v1.1.2</code>). </p>

<p>For example, we would obtain the Hyperkube image with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">docker pull gcr.io/google_containers/hyperkube:v1.1.2
</li></ul></code></pre>
<p><span class="note">
<strong>Note</strong>: At the time of writing, Kubernetes images were not yet being pushed to the Google Container Registry as part of the release process. Any available images were pushed as a one-off. Refer to <a href="https://github.com/kubernetes/kubernetes/issues/11751" rel="nofollow">the following support ticket</a> for an updated status of the release process.<br></span></p>

<p>Moreover, as the Hyperkube combines all binaries, is based on <code>debian:jessie</code> and includes additional packages such <code>iptables</code> (required by the <code>kube-proxy</code>), its size is considerable:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>gcr.io/google_containers/hyperkube    v1.1.2    aa1283b0c02d    2 weeks ago    254.3 MB
</code></pre>
<p>As a result, for this tutorial, we will run the <code>kube-proxy</code> binary outside of a container, the same way we run the <code>kubelet</code> or any system daemon. For the <code>kube-apiserver</code>, <code>kube-controller-manager</code> and <code>kube-scheduler</code> it is recommended to run these as containers and we will take a closer look at the available Docker images now.</p>

<p>As can be seen in the listings earlier, tarred repositories for Docker images wrapping the Kubernetes binaries are also available:</p>

<table class="pure-table"><thead>
<tr>
<th>binary_name</th>
<th>base image</th>
<th>size</th>
</tr>
</thead><tbody>
<tr>
<td>kube-apiserver</td>
<td>busybox</td>
<td>47.97 MB</td>
</tr>
<tr>
<td>kube-controller-manager</td>
<td>busybox</td>
<td>40.12 MB</td>
</tr>
<tr>
<td>kube-scheduler</td>
<td>busybox</td>
<td>21.44 MB</td>
</tr>
</tbody></table>

<p>Assuming you have access to a Docker daemon, we can <code>curl</code> and <code>load</code> these Docker images with the following commands.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -sLo ./kube-apiserver.tar https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver.tar
</li><li class="line" prefix="$">docker load -i ./kube-apiserver.tar
</li></ul></code></pre>
<p>We have loaded the <code>kube-apiserver</code> image in this example.</p>

<p>The Kubernetes build script tags these images as <code>gcr.io/google_containers/$binary_name:$md5_sum</code>. To easily run a container from this image, or push the image to a private registry for bootstrapping, we may re-tag the images with the following commands:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">#get md5 via docker_tag
</li><li class="line" prefix="$">docker_tag="$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-apiserver.docker_tag)"
</li><li class="line" prefix="$">#re-tag
</li><li class="line" prefix="$">docker tag -f "gcr.io/google_containers/kube-apiserver:${docker_tag}" "kube-apiserver:1.1.2"
</li></ul></code></pre>
<p>In the above command we first get the md5_sum from the cloud storage bucket and use it to re-tag the image.</p>

<p>&lt;!-- do we really need to mention the Kuar images? They are mentioned in every Workshop from Kelsey and used a lot... may be just to clarify they are not official and their presence is not guaranteed.. --&gt;</p>

<p><strong>A note about the Kubernetes Up and Running images</strong>:</p>

<p>Non-official images are also made available by Kelsey Hightower for his Kubernetes Up And Running book through his <code>kuar</code> repository (backed by the Kubernetes Up And Running <a href="https://cloud.google.com/container-registry/docs/#using_an_existing_google_cloud_storage_bucket" rel="nofollow">Google cloud storage bucket</a>), we may list the contents of that bucket using the <code>repositories/library</code> key, as follows:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">gsutil ls gs://kuar/repositories/library/
</li></ul></code></pre>
<p>Or without Python, using <code>curl</code>, <code>jq</code> and <code>grep</code> to grab all v1.1.2 Kubernetes tagged images:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -sL  https://www.googleapis.com/storage/v1/b/kuar/o?prefix='repositories/library/kube' | jq .items[].name | grep tag_1.1.2
</li></ul></code></pre>
<p>The <code>kuar</code> images are more easily ran with a single Docker command using the registry endpoint: <code>b.gcr.io/kuar/$binary_name:$version</code>, for example: to run the <code>kube-apiserver</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">docker run b.gcr.io/kuar/kube-apiserver:1.1.2
</li></ul></code></pre>
<p>This is much easier than manually <code>curl</code>-ing, <code>load</code>-ing, re-<code>tag</code>-ing and <code>run</code>-ing the images, but keep in mind that these are not the official Kubernetes images and their availability is not guaranteed.</p>

<h2 id="step-5-—-initializing-the-kubernetes-cluster-pki">Step 5 — Initializing The Kubernetes Cluster PKI</h2>

<p>In this step we will initialize the root Certificate Authority used by our Cluster.</p>

<p>In this tutorial we will configure the Kubernetes API Server to use client certificate authentication to enable encryption and prevent traffic interception and man-in-the-middle attacks. This means it is necessary to have a Certificate Authority (CA) which will be trusted as the root authority for the cluster and use it to generate the proper credentials. The necessary assets can also be generated from an existing Public Key Infrastructure (PKI), if already available.</p>

<p>For this tutorial we will use Self-Signed certificates. Every certificate is created by submitting Certificate Signing Requests (CSRs) to a CA. A CSR contains information identifying whom the certificate request is for, including the public key associated with the private key of the requesting party. The CA will sign the CSR, effectively returning what is from then on referred to as "the certificate".</p>

<p>For a detailed overview of OpenSSL, refer to the <a href="https://www.digitalocean.com/community/tutorials/openssl-essentials-working-with-ssl-certificates-private-keys-and-csrs" rel="nofollow">OpenSSL Essentials guide</a> on Digital Ocean.</p>

<h3 id="initialize-cluster-root-ca">Initialize Cluster Root CA</h3>

<p>Generate the private key for your root certificate into the default <code>$HOME/.kube</code> folder, which we should have created as part of our client machine setup, with the following OpenSSL command to generate a 2048 bit RSA private key:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl genrsa -out ~/.kube/ca-key.pem 2048
</li></ul></code></pre>
<p>This <code>ca-key.pem</code> private key will be used to generate the self-signed <code>ca.pem</code> certificate which will be trusted by all your Kubernetes components, as well as every Worker node and Administrator key pair. This key needs to be closely guarded and kept in a secure location for future use. </p>

<p>Next, use the private key to generate the self-signed root <code>ca.pem</code> certificate with the following openssl command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl req -x509 -new -nodes -key ~/.kube/ca-key.pem -days 10000 -out ~/.kube/ca.pem -subj "/CN=kube-ca"
</li></ul></code></pre>
<p>The <code>ca.pem</code> certificate will be used as the root certificate to verify the authenticity of certificates by every component within your Kubernetes cluster, you will copy this file to the controller and worker Droplets as well as the Administrator clients.</p>

<p>Confirm your Root CA assets exist in the expected location:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ls -1 ~/.kube/ca*.pem
</li></ul></code></pre>
<p>Output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>/home/demo/.kube/ca.pem  
/home/demo/.kube/ca-key.pem
</code></pre>
<p>We now have all the necessary ingredients to generate certificates for all of our cluster components. We will come back to <code>openssl</code> to generate each as required.</p>

<h2 id="step-6-—-provisioning-the-kubernetes-controller-droplets">Step 6 — Provisioning The Kubernetes Controller Droplets</h2>

<p>In this step we will create a single Kubernetes controller Droplet.</p>

<p>Most of the Kubernetes controller configuration will be done through <code>cloud-config</code>, aside from placing the TLS assets on disk. The <code>cloud-config</code> we create will take into account the possibility to have load-balanced controller nodes for High Availability in the future. How this affects our configuration will be discussed in detail in the [Controller Services set up: Master Election](#) section.</p>

<p><span class="note">
<strong>Note</strong>: The TLS assets shouldn't be stored in the <code>cloud-config</code> for enhanced security. If you do prefer to transfer the TLS assets as part of the <code>cloud-config</code> refer to <a href="https://coreos.com/os/docs/latest/customizing-docker.html#cloud-config5" rel="nofollow">this CoreOS tutorial</a> as an example of storing TLS assets within the <code>cloud-config</code> file.<br></span></p>
</p>

<p>We will now introduce every Kubernetes component and its configuration to incrementally add to our controller Droplet's <code>cloud-config</code> file and when completed, we will create the controller Droplet with Doctl from our client machine. Once the Droplet is ready we will generate and transfer the TLS assets.</p>

<h3 id="a-deep-dive-into-the-kubelet">A Deep Dive Into The Kubelet</h3>

<p>As seen in the Architectural overview section, Kubernetes is made up of several components. One such fundamental component is the <a href="http://kubernetes.io/v1.1/docs/admin/kubelet.html" rel="nofollow"><code>kubelet</code></a>. The <code>kubelet</code> is responsible for what's running on each individual Droplet within your cluster. You can think of it as a process watcher like <code>systemd</code>, but focused on running containers. It has one job: given a set of containers to run, make sure they are all running.</p>

<p>The unit of execution Kubernetes works with is the <a href="http://kubernetes.io/v1.1/docs/user-guide/pods.html" rel="nofollow">Pod</a>, not the individual container. A Pod is a collection of containers and volumes sharing the same execution environment. The containers within a Pod share a single IP, in our case this IP is provided by Docker within the Flannel overlay network. Pods are defined by a JSON or YAML file called a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod" rel="nofollow">Pod manifest</a>.</p>

<p>&lt;!-- this is duplicated from the Kubernetes overview part --&gt;<br>
Within a Kubernetes cluster, the <code>kubelet</code> functions as a local agent that watches for Pod specs via the Kubernetes API server. The <code>kubelet</code> is also responsible for registering a node with a Kubernetes cluster, sending events and pod status, and reporting resource utilization.</p>

<p>While the <code>kubelet</code> plays an important role in a Kubernetes cluster, it also works well in standalone mode - outside of a Kubernetes cluster. With the <code>kubelet</code> running in standalone mode we will be able to use containers to distribute our binaries, monitor container resource utilization through the built-in support for <a href="https://github.com/google/cadvisor/blob/master/README.md" rel="nofollow">cAdvisor</a> and establish resource limits for the daemon services. The <code>kubelet</code> provides a convenient interface for managing containers on a local system, allowing us to update our controller services by updating the containers without rebuilding our unit files. To achieve this, the <code>kubelet</code> supports the configuration of a manifest directory, which is monitored for pod manifests every 20 seconds by default.</p>

<p>We will use our controller's <code>cloud-config</code> to configure &amp; start the <code>kube-kubelet.service</code> in standalone mode on our controller Droplet. We will run the <code>kube-proxy</code> service in the same way. Next, we will deploy all the Kubernetes cluster control services using a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_pod" rel="nofollow">Pod manifest</a> placed in the <code>manifest</code> directory of the controller as soon as the TLS assets become available. The kubelet will start and make sure all containers within the Pod keep running, just as if the Pod was submitted via the API. The cool trick here is that we don't have an API running yet, but the Pod will function in the exact same way, which simplifies troubleshooting later on.</p>

<p>&lt;!-- TODO: Should we always curl the kubelet for consistency? --&gt;</p>

<p><strong>Note</strong>: Please note that only CoreOS Alpha or Beta images come with the Kubernetes kubelet. The Stable channel has never contained a version which included the <code>kubelet</code>. If a Droplet was booted from the Beta or Alpha channels and then moved to the Stable channel, it will lose the <code>kubelet</code> when it updates to the stable release.<br></p>

<p>At the time of writing, the CoreOS Alpha image on Digital Ocean has the following version:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>$ cat /etc/lsb-release
DISTRIB_ID=CoreOS
DISTRIB_RELEASE=891.0.0
</code></pre>
<p>And the Kubelet bundled with this is:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>$ kubelet --version=true
Kubernetes v1.1.2+3085895
</code></pre>
<p>If you are required to use the CoreOS stable channel or need a different Kubelet version, you may <code>curl</code> the <code>kubelet</code> binary as part of the <code>cloud-config</code> using the paths identified in [Step 4 — Understanding Where To Get The Kubernetes Artifacts](#) of this tutorial.</p>
<div class="code-label " title="cloud-config-* - kubelet snippet">cloud-config-* - kubelet snippet</div><pre class="code-pre "><code langs="">...
ExecStartPre=/usr/bin/curl -sLo /opt/bin/kubelet -z /opt/bin/kubelet https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubelet
ExecStartPre=/usr/bin/chmod +x /opt/bin/kubelet
...
</code></pre>
<p><strong>Note</strong>: The <code>-z</code> option of <code>curl</code> will only download newer files based on a date expression or, as used here - given an existing local file - only if the date of the remote file is newer than the date of the local file. This will generate a warning if the local file does not exist, as shown below.<br></p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Warning: Illegal date format for -z/--timecond (and not a file name).
Warning: Disabling time condition. See curl_getdate(3) for valid date syntax.
</code></pre>
<p>Wherever we <code>curl</code> binaries with the <code>-z</code> option as part of a Systemd unit, these warnings will show in the journal and can safely be ignored.</p>

<h3 id="running-the-kubelet-in-standalone-mode">Running the Kubelet in standalone mode</h3>

<p>The parameters we will pass on to the kubelet are as follows, we will break these down one by one next:</p>
<pre class="code-pre "><code langs="">kubelet \
  --api-servers=http://127.0.0.1:8080 \
  --register-node=false \
  --allow-privileged=true \
  --config=/etc/kubernetes/manifests \
  --hostname-override=$public_ipv4 \
  --cluster-dns=10.3.0.10 \
  --cluster-domain=cluster.local
</code></pre>
<p>The <code>kubelet</code> will communicate with the API server through <code>localhost</code> as we specify this with the <code>--api-servers</code> flag, but it will not register our controller node for cluster work as we set the <code>--register-node=false</code> flag, this ensures our controller Pods will not be affected by Pods scheduled by users within the cluster. As mentioned in the Kubelet deep dive section, to run the <code>kubelet</code> in standalone mode, we need to point it to a manifest directory. We set the <code>kubelet</code> manifest directory via the <code>--config</code> flag, which will be <code>/etc/kubernetes/manifests</code> in our setup. To facilitate the routing between Droplets, we also override the hostname with the Droplet public IP through the <code>--hostname-override</code> flag.</p>

<p>CoreOS Linux ships with reasonable defaults for the kubelet, which have been optimized for security and ease of use. However, we are going to loosen the security restrictions in order to enable support for privileged containers through the <code>--allow-privileged=true</code> flag.</p>

<p><strong>Service Discovery and Kubernetes Services</strong></p>

<p>To enable service discovery within the Kubernetes cluster, we need to provide our <code>kubelet</code> with the service IP for the cluster DNS component as well as the DNS domain. The <code>kubelet</code> will pass this on as the DNS server and DNS search suffix to each container running within the cluster. In this tutorial we will deploy DNS as a service within our Kubernetes cluster through <a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns" rel="nofollow">the cluster DNS add-on</a>, for which Kubernetes uses cluster Virtual IPs (VIPs). Routing to these VIPs is handled by the Kubernetes proxy components and VIPs are not required to be routable between nodes.</p>

<p>We configure Kubernetes to use the <code>10.3.0.0/24</code> IP range for all services. Each service will be assigned a cluster IP in this range. This range must not overlap with any IP ranges assigned to Pods as configured in our Flannel overlay network, or the Digital Ocean public and private IP ranges. The API server will take the first IP in that range (<code>10.3.0.1</code>) by itself and we will configure the DNS service to take the static IP of <code>10.3.0.10</code>. Modify these values to mirror your own configuration.</p>

<p>We must pass on this DNS service IP to the <code>kubelet</code> via the <code>--cluster-dns</code> flag and the DNS domain via the <code>--cluster-domain</code> flag.</p>

<p>If the <code>kubelet</code> is bundled with CoreOS (Alpha/Beta), it is stored on <code>/usr/bin/kubelet</code>, if you manually download (Stable) to another path (<code>/opt/bin/kubelet</code> for example), make sure to update the paths in the snippet below. Prior to starting the Kubelet service, we will also ensure the <code>manifests</code> and <code>ssl</code> directories exist on the host using the <code>ExecStartPre</code> directive, preceded by "-" which indicates to Systemd that failure of the command will be tolerated.</p>

<p>We combine all the information above in the <a href="https://www.digitalocean.com/community/tutorials/understanding-systemd-units-and-unit-files" rel="nofollow">systemd service unit</a> file for running the kubelet. We add a dependency on the <code>docker.service</code> and make sure the unit restarts on failure. This is only a snippet of our controller droplet's full <code>cloud-config</code>:</p>
<div class="code-label " title="cloud-config-controller.yaml kubelet - snippet">cloud-config-controller.yaml kubelet - snippet</div><pre class="code-pre "><code langs="">...
  units:
    - name: "kube-kubelet.service"
      command: "start"
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service
        After=docker.service
        [Service]
        ExecStartPre=-/bin/bash -c "mkdir -p /etc/kubernetes/{manifests,ssl}"
        ExecStart=/usr/bin/kubelet \
        --api-servers=http://127.0.0.1:8080 \
        --register-node=false \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local
        Restart=always
        RestartSec=10
...
</code></pre>
<p>With this configuration, all state-less controller services will be managed through the Pod manifests dropped into the <code>kubelet</code>'s manifest folder (<code>/etc/kubernetes/manifests</code>). After configuring the <code>kube-proxy</code> service next, we will go through the structure of a Pod manifest and the Pod manifest section for each controller service. We will finalize the controller configuration section with an overview of the full Kubernetes controller Pod manifests and the full controller <code>cloud-config</code> file.</p>

<h3 id="running-the-kubernetes-proxy-service">Running the Kubernetes Proxy Service</h3>

<p>All nodes should run <code>kube-proxy</code> (Running <code>kube-proxy</code> on a "controller" node is not strictly required, but being consistent is easier.) The proxy is responsible for directing traffic destined for specific services and pods to the correct location. The proxy communicates with the API server periodically to keep up to date.</p>

<p>&lt;!-- TODO: change "option" to "command" --&gt;<br>
Unlike the <code>kubelet</code>, the <code>kube-proxy</code> binary is currently not shipped with any CoreOS release. The URL to download the binary is described in [Step 4 — Understanding Where To Get The Kubernetes Artifacts](#) of this tutorial. We will <code>curl</code> the binary from this URL prior to starting the service by providing the following <code>ExecStartPre</code> directives within the <code>[Service]</code> section of our Systemd unit:</p>
<pre class="code-pre "><code langs="">ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
</code></pre>
<p>We will also delay the <code>kube-proxy</code> daemon from starting until the <code>kube-apiserver</code> service has started with the following <code>ExecStartPre</code> option:</p>
<pre class="code-pre "><code langs="">ExecStartPre=/bin/bash -c "until /usr/bin/curl http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
</code></pre>
<p>Both the controller and worker nodes in your cluster will run the proxy. The following <code>kube-proxy</code> parameters will be defined in our systemd service unit:</p>

<ul>
<li><code>--master=http://127.0.0.1:8080</code>: The address of the Kubernetes API server for our Kubernetes Controller node. In the section below, we will configure our <code>kube-apiserver</code> to bind to the network of the host and be reachable on the loopback interface.</li>
<li><code>--proxy-mode=iptables</code>: The proxy mode for our <code>kube-proxy</code>. At the time of writing the following two options are valid: <code>userspace</code> (older, stable) or <code>iptables</code> (experimental). If the <code>iptables</code> mode is selected, but the system's kernel or iptables versions are insufficient, this always falls back to the <code>userspace</code> proxy.</li>
<li><code>--hostname-override=$public_ipv4</code>: to facilitate routing without DNS resolution.</li>
</ul>
<div class="code-label " title="cloud-config-controller.yaml kube-proxy - snippet">cloud-config-controller.yaml kube-proxy - snippet</div><pre class="code-pre "><code langs="">
  units:
    - name: kube-proxy.service
      command: start
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target
        After=network-online.target
        [Service]
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        # wait for kube-apiserver to be up and ready
        ExecStartPre=/bin/bash -c "until /usr/bin/curl http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
        ExecStart=/opt/bin/kube-proxy \
        --master=http://127.0.0.1:8080 \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        TimeoutStartSec=10
        Restart=always
        RestartSec=10
</code></pre>
<p><strong>Note</strong>: At the time of writing, the <code>kube-proxy</code> binary is 18.3MB while the docker wrapped image based on Debian with the Iptables package installed is over 180MB, downloading the <code>kube-proxy</code> binary takes less than 10 seconds and is therefore the method used in this tutorial as opposed to running the proxy in a privileged Hyperkube container.</p>

<p><span class="note">
<strong>Note</strong>: By setting the <code>TimeoutStartSec</code> to 10, Systemd will fail the kube-proxy if it hasn't started after 10 seconds, but it will be restarted after the specified <code>RestartSec</code> timeout. We may notice these failures in the journal until the <code>kube-apiserver</code> has started. Until we are certain the <code>kube-apiserver</code> has started, these warning may be ignored.<br></span></p>

<p>For the full overview of all <code>kube-proxy</code> arguments, refer to <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/docs/admin/kube-proxy.md" rel="nofollow">the official documentation</a>.</p>

<h3 id="pre-loading-kubernetes-containers">Pre-Loading Kubernetes containers</h3>

<p>As seen in [Step 4 — Understanding Where To Get The Kubernetes Artifacts](#) of this tutorial, we may pull the combined <code>hyperkube</code> image (<code>gcr.io/google_containers/hyperkube:v1.1.2</code>), use the unofficial Kubernetes Up and Running Images (<code>b.gcr.io/kuar/$binary:$version</code>) or <code>curl</code> and <code>load</code> each image individually.</p>

<p>As explained earlier, we will <code>curl</code> and <code>load</code> each image individually, see <a href="http://kubernetes.io/v1.1/docs/user-guide/images.html#pre-pulling-images" rel="nofollow">Pre-pulling images</a> for details on how pre-pulled images may affect the Kubelet setup. The following script combines the commands described in Step 4:</p>
<div class="code-label " title="pull-kube-images.sh">pull-kube-images.sh</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">#!/bin/bash
</li><li class="line" prefix="3">tag=1.1.2
</li><li class="line" prefix="4">docker_wrapped_binaries=(
</li><li class="line" prefix="5">   "kube-apiserver"
</li><li class="line" prefix="6">   "kube-controller-manager"
</li><li class="line" prefix="7">   "kube-scheduler"
</li><li class="line" prefix="8">   #"kube-proxy"
</li><li class="line" prefix="9">)
</li><li class="line" prefix="10">temp_dir="$(mktemp -d -t 'kube-server-XXXX')"
</li><li class="line" prefix="11">
</li><li class="line" prefix="12">for binary in "${docker_wrapped_binaries[@]}"; do
</li><li class="line" prefix="13">  docker_tag="$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)"
</li><li class="line" prefix="14">  echo "downloading ${binary} ${docker_tag}"
</li><li class="line" prefix="15">  curl -Lo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
</li><li class="line" prefix="16">  echo "loading docker image"
</li><li class="line" prefix="17">  docker load -i ${temp_dir}/${binary}.tar
</li><li class="line" prefix="18">  echo "tagging docker image as ${binary} ${tag}"
</li><li class="line" prefix="19">  docker tag -f "gcr.io/google_containers/${binary}:${docker_tag}" "${binary}:${tag}"
</li><li class="line" prefix="20">done
</li><li class="line" prefix="21">
</li><li class="line" prefix="22">echo "cleaning up temp dir"
</li><li class="line" prefix="23">rm -rf "${temp_dir}"
</li></ul></code></pre>
<p>You may modify this script to push these images to a local registry to simplify the provisioning of your cluster nodes, assuming a local Docker registry is available. In this case, our cluster nodes will easily pull the containers from the local registry and we do not need to include this script as part of our controller Droplet configuration. For this tutorial, we assume such registry is not available and embed the above script into the <code>cloud-config</code> file of every controller node.</p>

<p>After embedding this script in a Systemd unit, stripping the detailed output and specifying that the network needs to be up as well as the docker service needs to be successfully loaded, our <code>cloud-config</code> file will have the following snippet to pre-pull the Kubernetes images:</p>
<div class="code-label " title="cloud-config-controller.yaml pull-kube-images - snippet">cloud-config-controller.yaml pull-kube-images - snippet</div><pre class="code-pre "><code langs="">...
write-files:
  - path: /opt/bin/pull-kube-images.sh
    permissions: '0755'
    content: |
      #!/bin/bash
      tag=1.1.2
      docker_wrapped_binaries=(
        "kube-apiserver"
        "kube-controller-manager"
        "kube-scheduler"
      )
      temp_dir="$(mktemp -d -t 'kube-server-XXXX')"
      for binary in "${docker_wrapped_binaries[@]}"; do
        docker_tag="$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)"
        curl -sLo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
        docker load -i ${temp_dir}/${binary}.tar
        docker tag -f "gcr.io/google_containers/${binary}:${docker_tag}" "${binary}:${tag}"
      done;
      rm -rf "${temp_dir}";
      exit $?
...
coreos:
  units:
    - name: pull-kube-images.service
      command: start
      content: |
        [Unit]
        Description=Pull and load all Docker wrapped Kubernetes binaries
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target docker.service
        After=network-online.target docker.service
        [Service]
        ExecStart=/opt/bin/pull-kube-images.sh
        RemainAfterExit=yes
        Type=oneshot
...
</code></pre>
<p>In the above <code>cloud-config</code> snippet, we use the <code>write_files</code> directive to store the script on disk and make it executable as well as define a <code>oneshot</code> service to run that script. All our controller services will depend on the successful completion of this service. Oneshot services are flagged as successful by Systemd even after exiting.</p>

<h3 id="introduction-to-kubernetes-manifest-files">Introduction to Kubernetes Manifest files</h3>

<p>Our kubelet will be used to manage our controller services within containers based on manifest files. In this section we will have a closer look at the structure of these files, you may refer to this section to understand manifest files better.</p>

<p>Kubernetes manifests can be written using YAML or JSON, but only YAML provides the ability to add comments. All of the manifests accepted and returned by the server have a schema, identified by the <code>kind</code> and <code>apiVersion</code> fields. These fields are required for proper decoding of the object.</p>

<p>The <code>kind</code> field takes a string that identifies the schema of an object, in our case we are writing a manifest to create Pod objects, as such we write <code>kind: Pod</code> in our Pod manifest.</p>

<p>The <code>apiVersion</code> field takes a string that identifies the API group &amp; version of the schema of an object. API groups will enable the Kubernetes API to be broken down into modular groups which can then be enabled/disabled individually, versioned separately as well as provide 3rd parties the ability to develop Kubernetes plug-ins without naming conflicts. At the time of writing there are only 2 API groups:</p>

<ul>
<li>The "core" group, which currently consists of the original monolithic Kubernetes v1 API. This API group is simply omitted and specified only by it's version, for example: <code>apiVersion: v1</code></li>
<li>The "extensions" group, which is the first API group introduced with v1.1. The <code>extensions</code> API group is still in <code>v1beta1</code> at the time of writing, as such this API group is specified as <code>apiVersion: extensions/v1beta1</code>. Resources within the <code>extensions</code> API group can be enabled or disabled through the <code>--runtime-config</code> flag passed on to the apiserver. For example, to disable <code>HorizontalPodAutoscalers</code> and <code>Jobs</code> we may set <code>--runtime-config=extensions/v1beta1/horizontalpodautoscalers=false,extensions/v1beta1/jobs=false</code>.</li>
</ul>

<p>For a more detailed explanation of the Kubernetes API, refer to <a href="http://kubernetes.io/v1.1/docs/api.html" rel="nofollow">the API documentation</a>.</p>

<p>Once the schema has been specified, Pod manifests mainly consist of the following key structures:</p>

<ul>
<li>A <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_objectmeta" rel="nofollow">metadata</a> structure for describing the pod and its labels</li>
<li>A <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podspec" rel="nofollow">spec</a> structure for describing volumes, and a list of containers that will run in the Pod.</li>
</ul>

<p>The <code>name</code> and <code>namespace</code> of the <code>metadata</code> structure are generally user provided. The <a href="http://kubernetes.io/v1.1/docs/user-guide/identifiers.html#names" rel="nofollow">name</a> has to be unique within the namespace specified. An empty <a href="http://kubernetes.io/v1.1/docs/user-guide/namespaces.html" rel="nofollow">namespace</a> is equivalent to the <code>default</code> namespace. In our case, we will scope our Pods related to the Kubernetes system environment to the <code>kube-system</code> namespace. We will combine all stateless controller service containers within one Pod and call it the <code>kube-controller</code> Pod.</p>

<p>Every Pod <code>spec</code> structure must at least have a list of <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_container" rel="nofollow">containers</a> with a minimum of 1 container. Each container in a pod must have a unique <code>name</code>. For example, the API service container may be named identical to it's binary name: <code>kube-apiserver</code>. Next, we may specify the <code>image</code> for the container, the <code>command</code> ran within the container (equivalent to the Docker image <code>entrypoint</code> array), the <code>args</code> passed on to the container process (equivalent to Docker image <code>cmd</code> array), the <code>volumeMounts</code>, ...</p>

<p>A special requirement for our controller service containers is that they need to use the host's network namespace. This can be achieved by setting the <code>hostNetwork: true</code> for the <code>spec</code> structure of our controller Pod manifest.</p>

<p>Thus, this is how our Pod manifest for the controller services starts:</p>
<div class="code-label " title="kube-controller.yaml - header">kube-controller.yaml - header</div><pre class="code-pre "><code langs="">
apiVersion: v1
kind: Pod
metadata:
  name: kube-controller
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
  - name: "kube-apiserver"
  ...
</code></pre>
<h3 id="controller-services-set-up-master-election">Controller Services set up: Master Election</h3>

<p>By using a single Kubernetes controller Droplet, we have a single point of failure in our infrastructure. To ensure high availability we will need to run multiple controller nodes at some point. Every stateless Kubernetes component, such as the <code>kube-apiserver</code>, can be scaled across multiple controller nodes without concern. However, there are components which modify the state of the cluster, such as the <code>kube-controller-manager</code> and the <code>kube-scheduler</code>. Of these components, only one instance may modify the state at a time. To achieve this, we need to have a way to ensure only 1 instance for each of these components is running which is done by setting up master election per component.</p>

<p>At the time of writing, master election is not integrated within the <code>kube-controller-manager</code> and <code>kube-scheduler</code>, but is planned to be added in the future. Until then, a powerful and generic master election utility called the <a href="https://github.com/kubernetes/contrib/tree/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master" rel="nofollow">Podmaster</a> is recommended. </p>

<p>The Podmaster is a small (8MB) utility written in Go that uses Etcd's atomic <code>CompareAndSwap</code> functionality to implement master election. The first controller node to reach the Etcd cluster wins the race and becomes the master node for that service, marking itself as such with an expiring key identifying the service. The Podmaster will then periodically extend its service key. If a Podmaster finds the service key it monitors has <a href="https://github.com/kubernetes/contrib/blob/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master/podmaster.go#L89" rel="nofollow">expired</a>, it attempts to take over by setting itself as the new master for that service. If it is the current master, the Podmaster copies the manifest of its service into the manifests directory of its host, ensuring a single instance of the service is always running within the cluster. If the Podmaster finds it is no longer the master, it removes the manifest file from the manifests directory of its host, ensuring the kubelet will no longer run the service on that controller node.</p>

<p>A Podmaster instance may run for each service requiring master election, each instance takes the key identifying the service as well as a source manifest file and a destination manifest file. The Podmaster itself will run inside a container and a <a href="https://github.com/kubernetes/contrib/blob/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/pod-master/Dockerfile" rel="nofollow">Docker image</a> wrapping the Podmaster can be pulled from the Google Container Registry under the <code>gcr.io/google_containers/podmaster</code> repository. At the time of writing there is only 1 tag: <code>1.1</code>.</p>

<p>Even though we are only creating one controller node in this tutorial, we will set up the master election for the controller manager and scheduler service by storing their manifest files under the <code>/srv/kubernetes/manifests</code> path and letting Podmaster instances copy the manifest files to the <code>/etc/kubernetes/manifests</code> path on the elected master node.</p>

<p>In a single-controller deployment, the Podmaster will simply ensure that the <code>kube-scheduler</code> and <code>kube-controller-manager</code> run on the current node. In a multi-controller deployment, the Podmaster will be responsible for ensuring no additional instances are started, unless a machine dies, in which case the Podmaster will ensure new instances are started on one of the other controller nodes.</p>

<p>As our Podmasters depend on Kubernetes volumes, we will see the full Podmaster configurations after defining the Kubernetes volumes and <code>kube-apiserver</code> Pod manifests.</p>

<h3 id="controller-services-set-up-kubernetes-volumes">Controller Services set up: Kubernetes Volumes</h3>

<p>At its core, a Kubernetes volume is just a directory, possibly with some data in it, which is accessible to the containers in a Pod. How that directory comes to be, the medium that backs it, and the contents of it are determined by the particular volume type used. Each volume type is backed by a Kubernetes volume plug-in.</p>

<p>For our controller services, we ensure the Pod is tied to our controller node, and we will use <code>HostPath</code> type volumes. <code>HostPath</code> type volumes represent a pre-existing file or directory on the host machine that is directly exposed to the container. They are generally used for system agents or other privileged things that are allowed to see the host machine. </p>

<p>We will place our API server certificates, once generated, in the following pre-defined path on the host:</p>

<ul>
<li>File: <code>/etc/kubernetes/ssl/ca.pem</code></li>
<li>File: <code>/etc/kubernetes/ssl/apiserver.pem</code></li>
<li>File: <code>/etc/kubernetes/ssl/apiserver-key.pem</code></li>
</ul>

<p>The address of the controller node is required to generate these API Server certificates. On Digital Ocean, this address is not known in advance. Therefore, we will generate the certificates and securely copy them after we provision our controller Droplet in a separate step, but we will prepare our <code>cloud-config</code> and the volumes defined in our Pod manifests to expect these certificates into these pre-defined host paths.</p>

<p>Every <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_volume" rel="nofollow">volume</a> requires a name which is unique within the Pod. The name is how we reference the volumes when we mount them into the Pod containers. </p>

<p>We define the following volume collection as part of our controller Pod manifest: </p>

<ol>
<li>a <code>HostPath</code> volume to provision the Kubernetes TLS Credentials from the parent directory <code>/etc/kubernetes/ssl</code>. </li>
<li>a <code>HostPath</code> volume for the list of "well-known" ca certificates - which, under CoreOS, is located under the read-only <code>/usr/share/ca-certificates</code> path.</li>
<li>a <code>HostPath</code> volume to provision as a source for manifest files for master election for the Podmaster <code>/srv/kubernetes/manifests</code></li>
<li>a <code>HostPath</code> volume for the Podmaster to access the host manifest folder <code>/etc/kubernetes/manifests</code>, where it will store the destination manifest files.</li>
</ol>
<div class="code-label " title="kube-controller.yaml - volumes">kube-controller.yaml - volumes</div><pre class="code-pre "><code langs="">spec:
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
    - hostPath:
        path: /srv/kubernetes/manifests
      name: manifest-src
    - hostPath:
        path: /etc/kubernetes/manifests
      name: manifest-dst
</code></pre>
<h3 id="controller-services-set-up-the-kube-apiserver">Controller Services set up: The kube-apiserver</h3>

<p>The first controller service we will configure in our controller Pod manifest is the API server. The API server is where most of the magic happens. It is stateless by design and takes in API requests, processes them and stores the result in Etcd if needed, and then returns the result of the request. the API server will run on every controller Droplet and will be stored directly under the kubelet manifest folder.</p>

<p>In this tutorial we are using individual Docker images wrapping each Kubernetes binary, in our Pod manifest we specify this binary as the entrypoint for the containers through the <code>command</code> array together with all of its arguments.</p>

<p>Below is the <code>kube-apiserver</code> container spec for our controller Pod, we will go through each argument in detail right after:</p>
<div class="code-label " title="kube-controller.yaml - api-server container ">kube-controller.yaml - api-server container </div><pre class="code-pre "><code langs="">
  containers:
    - name: "kube-apiserver"
      image: "kube-apiserver:1.1.2"
      command: 
        - "kube-apiserver"
        - "--etcd-servers=http://127.0.0.1:2379"
        - "--bind-address=0.0.0.0"
        - "--secure_port=443"
        - "--advertise-address=$public_ipv4"
        - "--service-cluster-ip-range=<span class="highlight">10.3.0.0/24</span>"
        - "--service-node-port-range=<span class="highlight">30000-37000</span>"
        - "--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
        - "--allow-privileged=true"
        - "--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem"
        - "--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
        - "--client-ca-file=/etc/kubernetes/ssl/ca.pem"
        - "--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
      ports:
        - containerPort: 443
          hostPort: 443
          name: https
        - containerPort: 8080
          hostPort: 8080
          name: local
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
</code></pre>
<p>As highlighted in the [Kubernetes manifest files](#) section of this tutorial, our <code>kube-controller</code> Pod uses the host's network namespace and each container running within the Pod can reach the host services, such as the Etcd proxy, over localhost.</p>

<ul>
<li><p><code>--etcd-servers=[]</code>: By design, the <code>kube-apiserver</code> component is the only Kubernetes component communicating with Etcd. We specify the location of the Etcd cluster through the <code>--etcd-servers=[]</code> flag. This flag takes a comma separated list of etcd servers to watch. In this tutorial we bind an Etcd proxy for the cluster to the loopback interface of each Droplet, thus the Etcd cluster can be reached through <code>http://127.0.0.1:2379</code>. Also note that by default, Kubernetes objects are stored under the <code>/registry</code> key in Etcd. We could prefix this path by also setting the <code>--etcd-prefix="/foo"</code> flag, but wont do this for this tutorial. </p></li>
<li><p><code>--bind-address=0.0.0.0</code>: The IP address on which the API server listens for requests. We explicitely configure our API server to listen on all interfaces of the host.</p></li>
<li><p><code>--secure-port=443</code>: To enable HTTPS with authentication and authorization we need to set this flag.</p></li>
</ul>

<p>&lt;!-- TODO: use $private_ipv4? --&gt;</p>

<ul>
<li><p><code>--advertise-address=$public_ipv4</code>: The IP address on which to advertise the apiserver to members of the cluster. This address must be reachable by the rest of the cluster. If blank, the <code>--bind-address</code> will be used, which would not work in our set up. </p></li>
<li><p><code>--service-cluster-ip-range=10.3.0.0/24</code>: A required CIDR notation IP range from which to assign service cluster IPs. See the [Running the kubelet in standalone mode](#) section for more details on how this is used within Kubernetes, we use <code>10.3.0.0/24</code> for Kubernetes Services within this Tutorial. Modify these values to mirror your own configuration.</p></li>
<li><p><code>--service-node-port-range=30000-37000</code>: A port range to reserve for services with NodePort visibility. If we do not specify this range we will not be able to run some of the Kubernetes <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_servicespec" rel="nofollow">service</a> examples using <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_serviceport" rel="nofollow">nodePort</a>.</p></li>
<li><p><code>--admission-control=[]</code>: In Kubernetes, API requests need to pass through a chain of <a href="http://kubernetes.io/v1.1/docs/admin/admission-controllers.html" rel="nofollow">admission controllers</a> after authentication and authorization but prior to being accepted and executed. Admission controllers are chained plug-ins, many advanced features in Kubernetes require an admission control plug-in to be enabled in order to properly support the feature. As a result, a Kubernetes API server that is not properly configured with the right set of admission control plug-ins is an incomplete server and will not support all the features you expect. The recommended set of admission controllers for Kubernetes 1.0 is <code>NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota</code>. We would like to highlight the <code>NamespaceLifecycle</code> plug-in which ensures that API requests in a non-existant Namespace are rejected. Due to this, we will be required to manually create the <code>kube-system</code> namespace used by our controller services once our <code>kube-apiserver</code> is available or our other nodes won't be able to discover them.</p></li>
<li><p><code>--allow-privileged=true</code>: We have to explicitely allow privileged containers to run in our cluster.</p></li>
<li><p><code>--tls-cert-file="etc/kubernetes/ssl/apiserver.pem"</code>: The certificate used for SSL/TLS connections to the API Server. We will generate The apiserver certificate containing host identities (DNS name, IP, ..) and securely copy it to our controller Droplet in a separate step. If HTTPS serving is enabled, and <code>--tls-cert-file</code> and <code>--tls-private-key-file</code> are not provided, a self-signed certificate and key are generated for the public address and saved to <code>/var/run/kubernetes</code>. If you intend to use this approach, ensure to provide a volume for <code>/var/run/kubernetes/</code> as well.</p></li>
<li><p><code>--tls-private-key-file="/etc/kubernetes/ssl/apiserver-key.pem"</code>: The API Server private key matching the <code>--tls-cert-file</code> we generated.</p></li>
<li><p><code>--client-ca-file="/etc/kubernetes/ssl/ca.pem"</code>: The trusted certificate authority, Kubernetes will check all incoming HTTPs request for a client certificate signed by this trusted CA. Any request presenting a client certificate signed by one of the authorities in the <code>client-ca-file</code> is authenticated with an identity corresponding to the CommonName of the client certificate. </p></li>
<li><p><code>--service-account-key-file="/etc/kubernetes/ssl/apiserver-key.pem"</code>: used to verify <a href="http://kubernetes.io/v1.1/docs/user-guide/service-accounts.html" rel="nofollow">ServiceAccount</a> tokens. We explicitely set this to the same private key as our <code>--tls-private-key-file</code> flag. If - unspecified, <code>--tls-private-key-file</code> is also used.</p></li>
</ul>

<p>Refer to the full <a href="http://kubernetes.io/v1.1/docs/admin/kube-apiserver.html" rel="nofollow">kube-apiserver reference</a> for a full overview of all API server flags.</p>

<h3 id="controller-services-set-up-the-kube-system-namespace">Controller Services set up: The kube-system namespace</h3>

<p>As soon as the <code>kube-apiserver</code> is available we need to create the <code>kube-system</code> namespace used by our controller services or our cluster nodes won't be able to discover them. In this section we define the Systemd unit responsible for this. </p>

<p>We wait until the <code>kube-apiserver</code> service has started, the same way as our <code>kube-proxy</code> service was configured to wait:</p>
<pre class="code-pre "><code langs="">ExecStartPre=/bin/bash -c "until /usr/bin/curl -s http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
</code></pre>
<p>The command to create the namespace using the Kubernetes API is:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
</li></ul></code></pre>
<p>We are passing in a the Manifest file as a JSON string in this case.</p>

<p>Putting the command into a oneshot Systemd unit which depends on a successful start of the kubelet service, gives us the following unit definition:</p>
<div class="code-label " title="cloud-config-controller.yaml create kube-system namespace - snippet">cloud-config-controller.yaml create kube-system namespace - snippet</div><pre class="code-pre "><code langs="">
coreos:
  units:
    - name: "create-kube-system-ns.service"
      command: "start"
      content: |
        [Unit]
        Description=Create the kube-system namespace
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        ExecStartPre=/bin/bash -c "until /usr/bin/curl -s http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
        ExecStart=/usr/bin/curl -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
        RemainAfterExit=yes
        Type=oneshot
</code></pre>
<h3 id="controller-services-set-up-the-kube-podmaster">Controller Services set up: The kube-podmaster</h3>

<p>In this section we add our Podmaster containers to our <code>kube-controller</code> Pod manifest. As mentioned in the [Controller Services: Master Election](#) section of this tutorial, the <code>kube-scheduler</code> and <code>kube-controller-manager</code> services require master election. We will create 1 Podmaster container for each component requiring master election and define the Pod manifests in the following sections.</p>

<p>We will go into the contents of the <code>kube-scheduler.yaml</code> Pod manifest and <code>kube-controller-manager.yaml</code> Pod manifest after finalizing this <code>kube-controller.yaml</code> Pod manifest.</p>

<p>As the <code>kube-controller</code> Pod shares the host network, our Podmaster containers can reach the Etcd cluster via the localhost Etcd proxy. To ease the setup, we overwrite the hostname the Podmaster stores in the master reservation with the Droplet public IP by setting the <code>--whoami</code> flag. The Droplet IP is always routable without the need for DNS services. We mount the <code>manifest-src</code> volume as a read only volume within the Podmaster containers. The <code>manifest-dst</code> volume is the path monitored by the Kubelet and needs to be writable by the Podmaster.</p>

<p>Here is the Podmaster container managing the master election for the <code>kube-scheduler</code> service</p>

<p>&lt;!-- TODO: should this be $private_ipv4?--&gt;</p>
<div class="code-label " title="kube-controller.yaml - scheduler-elector - snippet ">kube-controller.yaml - scheduler-elector - snippet </div><pre class="code-pre "><code langs="">
  containers:
    - name: "scheduler-elector"
      image: "gcr.io/google_containers/podmaster:1.1"
      args:
        - "--whoami=$public_ipv4"
        - "--etcd-servers=http://127.0.0.1:2379"
        - "--key=<span class="highlight">scheduler</span>"
        - "--source-file=<span class="highlight">/src/manifests/kube-scheduler.yaml</span>"
        - "--dest-file=<span class="highlight">/dst/manifests/kube-scheduler.yaml</span>"
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
</code></pre>
<p>For the <code>kube-scheduler</code> our Podmaster sets the value of the <code>scheduler</code> key in Etcd to record which controller Droplet is the master. We point this Podmaster to the <code>kube-scheduler</code> Pod manifest source and destination files.</p>

<p>For the <code>kube-controller-manager</code> the master elector looks almost identical, apart from the key, source and destination manifest files. The key used for the <code>kube-controller-manager</code> is <code>controller</code> and the <code>kube-controller-manager.yaml</code> Pod manifest file is used instead.</p>

<p>&lt;!-- TODO: should this be $private_ipv4?--&gt;</p>
<div class="code-label " title="kube-controller.yaml - controller-manager-elector - snippet ">kube-controller.yaml - controller-manager-elector - snippet </div><pre class="code-pre "><code langs="">
  containers
    - name: "controller-manager-elector"
      image: "gcr.io/google_containers/podmaster:1.1"
      args:
        - "--whoami=$public_ipv4"
        - "--etcd-servers=http://127.0.0.1:2379"
        - "--key=<span class="highlight">controller</span>"
        - "--source-file=<span class="highlight">/src/manifests/kube-controller-manager.yaml</span>"
        - "--dest-file=<span class="highlight">/dst/manifests/kube-controller-manager.yaml</span>"
      volumeMounts:
        - mountPath: /src/manifests
          name: manifest-src
          readOnly: true
        - mountPath: /dst/manifests
          name: manifest-dst
</code></pre>
<h3 id="combining-kube-controller-pod-manifest-snippets">Combining kube-controller Pod manifest snippets</h3>

<p>Combining all the <code>kube-controller.yaml</code> snippets above into a single kube-controller Pod manifest:</p>
<div class="code-label " title="kube-controller.yaml">kube-controller.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Pod
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: kube-controller
</li><li class="line" prefix="6">  namespace: kube-system
</li><li class="line" prefix="7">spec:
</li><li class="line" prefix="8">  hostNetwork: true
</li><li class="line" prefix="9">  volumes:
</li><li class="line" prefix="10">    - hostPath:
</li><li class="line" prefix="11">        path: /etc/kubernetes/ssl
</li><li class="line" prefix="12">      name: ssl-certs-kubernetes
</li><li class="line" prefix="13">    - hostPath:
</li><li class="line" prefix="14">        path: /usr/share/ca-certificates
</li><li class="line" prefix="15">      name: ssl-certs-host
</li><li class="line" prefix="16">    - hostPath:
</li><li class="line" prefix="17">        path: /srv/kubernetes/manifests
</li><li class="line" prefix="18">      name: manifest-src
</li><li class="line" prefix="19">    - hostPath:
</li><li class="line" prefix="20">        path: /etc/kubernetes/manifests
</li><li class="line" prefix="21">      name: manifest-dst
</li><li class="line" prefix="22">  containers:
</li><li class="line" prefix="23">    - name: "kube-apiserver"
</li><li class="line" prefix="24">      image: "kube-apiserver:1.1.2"
</li><li class="line" prefix="25">      command: 
</li><li class="line" prefix="26">        - "kube-apiserver"
</li><li class="line" prefix="27">        - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="28">        - "--bind-address=0.0.0.0"
</li><li class="line" prefix="29">        - "--secure_port=443"
</li><li class="line" prefix="30">        - "--advertise-address=$public_ipv4"
</li><li class="line" prefix="31">        - "--service-cluster-ip-range=10.3.0.0/24"
</li><li class="line" prefix="32">        - "--service-node-port-range=30000-37000"
</li><li class="line" prefix="33">        - "--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
</li><li class="line" prefix="34">        - "--allow-privileged=true"
</li><li class="line" prefix="35">        - "--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem"
</li><li class="line" prefix="36">        - "--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="37">        - "--client-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="38">        - "--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="39">      ports:
</li><li class="line" prefix="40">        - containerPort: 443
</li><li class="line" prefix="41">          hostPort: 443
</li><li class="line" prefix="42">          name: https
</li><li class="line" prefix="43">        - containerPort: 8080
</li><li class="line" prefix="44">          hostPort: 8080
</li><li class="line" prefix="45">          name: local
</li><li class="line" prefix="46">      volumeMounts:
</li><li class="line" prefix="47">        - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="48">          name: ssl-certs-kubernetes
</li><li class="line" prefix="49">          readOnly: true
</li><li class="line" prefix="50">        - mountPath: /etc/ssl/certs
</li><li class="line" prefix="51">          name: ssl-certs-host
</li><li class="line" prefix="52">          readOnly: true
</li><li class="line" prefix="53">    - name: "scheduler-elector"
</li><li class="line" prefix="54">      image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="55">      args:
</li><li class="line" prefix="56">        - "--whoami=$public_ipv4"
</li><li class="line" prefix="57">        - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="58">        - "--key=scheduler"
</li><li class="line" prefix="59">        - "--source-file=/src/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="60">        - "--dest-file=/dst/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="61">      volumeMounts:
</li><li class="line" prefix="62">        - mountPath: /src/manifests
</li><li class="line" prefix="63">          name: manifest-src
</li><li class="line" prefix="64">          readOnly: true
</li><li class="line" prefix="65">        - mountPath: /dst/manifests
</li><li class="line" prefix="66">          name: manifest-dst
</li><li class="line" prefix="67">    - name: "controller-manager-elector"
</li><li class="line" prefix="68">      image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="69">      args:
</li><li class="line" prefix="70">        - "--whoami=$public_ipv4"
</li><li class="line" prefix="71">        - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="72">        - "--key=controller"
</li><li class="line" prefix="73">        - "--source-file=/src/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="74">        - "--dest-file=/dst/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="75">      volumeMounts:
</li><li class="line" prefix="76">        - mountPath: /src/manifests
</li><li class="line" prefix="77">          name: manifest-src
</li><li class="line" prefix="78">          readOnly: true
</li><li class="line" prefix="79">        - mountPath: /dst/manifests
</li><li class="line" prefix="80">          name: manifest-dst
</li></ul></code></pre>
<h3 id="the-kube-controller-pod-pre-conditions">The kube-controller Pod Pre-conditions</h3>

<p>The <code>kube-apiserver</code> requires the TLS assets to be in place, if these are not in place the container will die after starting. The <code>kubelet</code> will create a new container every 5 minutes until the container stays up. To keep the error logs and dead containers minimal during first boot, we prefer to hold off on putting the <code>kube-controller</code> Pod manifest in the kubelet manifest directory until the <code>kube-apiserver</code> TLS assets are available. We will use the <code>write_files</code> directive to create the <code>kube-controller</code> Pod manifest under the <code>/srv/kubernetes/manifests/</code> Path until then.</p>

<p>We will use a Systemd unit to monitor the <code>/etc/kubernetes/ssl</code> path and copy the <code>kube-controller</code> manifest file to the kubelet manifest directory as soon as the TLS assets are detected.</p>

<p>The following loop sleeps until all 3 TLS assets required on the controller node, are available: </p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo "waiting for TLS assets...";sleep 5; done
</li></ul></code></pre>
<p>Putting this into a oneshot Systemd unit which starts as soon as the kubelet is ready, gives us the following unit definition:</p>
<div class="code-label " title="cloud-config-controller.yaml start controller pod - snippet">cloud-config-controller.yaml start controller pod - snippet</div><pre class="code-pre "><code langs="">...
coreos:
  units:
    - name: "tls-ready.service"
      command: "start"
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/bin/bash -c "until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \"waiting for TLS assets...\";sleep 5; done"
        ExecStart=/usr/bin/cp /srv/kubernetes/manifests/kube-controller.yaml /etc/kubernetes/manifests/
...
</code></pre>
<p>We will now proceed with defining the master elected <code>kube-scheduler</code> and <code>kube-controller-manager</code> Pod manifests which will also be stored under the <code>/srv/kubernetes/manifests</code> path.</p>

<h3 id="controller-services-set-up-the-kube-controller-manager-pod-manifest">Controller Services set up: The kube-controller-manager Pod manifest</h3>

<p>The controller manager embeds the core control loops within Kubernetes such as the replication controller, endpoints controller, namespace controller and serviceaccount controller. In short, a control loop watches the shared state of the cluster through the <code>kube-apiserver</code> and makes changes attempting to move the current state towards the desired state.</p>

<p>For example, if you increased the replica count for a replication controller, the controller manager would generate a scale up event, which would cause a new Pod to get scheduled in the cluster. The controller manager communicates with the API to submit these events.</p>

<p>We start writing this Pod manifest in exactly the same way as our <code>kube-controller</code> Pod manifest, but with it's own unique name in the <code>kube-system</code> namespace:</p>
<div class="code-label " title="/srv/kubernetes/manifests/kube-controller-manager.yaml - header - snippet">/srv/kubernetes/manifests/kube-controller-manager.yaml - header - snippet</div><pre class="code-pre "><code langs="">
apiVersion: v1
kind: Pod
metadata:
  name: kube-controller-manager
  namespace: kube-system
spec:
  hostNetwork: true
  containers:
    - name: "<span class="highlight">kube-controller-manager</span>"
...
</code></pre>
<p>This Pod also shares the network with the host (<code>hostNetwork: true</code>), allowing the containers running within to access the <code>kube-apiserver</code> through localhost as well as exposing themselves to the kubelet over localhost.</p>

<p>We define volumes for the ssl certificates and list off "well-known" ca certificates stored on the host so we can mount these into the Pod containers:</p>

<p>&lt;!-- TODO: Do we need the "well-known" certificates mount? It seems it is not used?? --&gt;</p>
<div class="code-label " title="/srv/kubernetes/manifests/kube-controller-manager.yaml - volumes - snippet">/srv/kubernetes/manifests/kube-controller-manager.yaml - volumes - snippet</div><pre class="code-pre "><code langs="">
spec:
  ...
  volumes:
    - hostPath:
        path: /etc/kubernetes/ssl
      name: ssl-certs-kubernetes
    - hostPath:
        path: /usr/share/ca-certificates
      name: ssl-certs-host
</code></pre>
<p>Our <code>kube-controller-manager</code> is called with the following arguments:</p>
<pre class="code-pre "><code langs="">kube-controller-manager \
  --master=http://127.0.0.1:8080 \
  --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
  --root-ca-file=/etc/kubernetes/ssl/ca.pem
</code></pre>
<p>We provide the address of the <code>kube-apiserver</code> via the <code>--master=http://127.0.0.1:8080</code> flag. We provide the private key (to sign service account tokens) and our Kubernetes cluster root CA certificate for inclusion in service account tokens via the <code>--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem</code> and <code>--root-ca-file=/etc/kubernetes/ssl/ca.pem</code> flags respectively.</p>

<p>We are also adding a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_probe" rel="nofollow">livenessProbe</a> to our Pod manifest. This is a diagnostic performed periodically by the kubelet on a container. The LivenessProbe hints to the kubelet when a container is <a href="http://kubernetes.io/v1.1/docs/user-guide/pod-states.html#container-probes" rel="nofollow">unhealthy</a>. If the LivenessProbe fails, the kubelet will kill the container and the container will be subjected to its <code>RestartPolicy</code>. If <code>RestartPolicy</code> is not set, the default value is <code>Always</code>. The default state of Liveness before the initial delay is <code>Success</code>. The state of Liveness for a container when no probe is provided is assumed to be <code>Success</code>. </p>

<p>The <code>httpGet</code> handler used in our livenessProbe performs an HTTP Get against the provided IP address on a specified port and path expecting on success that the response has a status code greater than or equal to 200 and less than 400. Note the default port used by <code>kube-controller-manager</code> is always <code>10252</code> and the Kubernetes <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/pkg/healthz" rel="nofollow">"healtz" package</a> registers a handler on the '/healthz' path , that serves 200s.</p>

<p>This gives us the following container spec for our <code>kube-controller-manager</code> container:</p>
<div class="code-label " title="/srv/kubernetes/manifests/kube-controller-manager.yaml - containers - snippet ">/srv/kubernetes/manifests/kube-controller-manager.yaml - containers - snippet </div><pre class="code-pre "><code langs="">
spec:
  ...
  containers:
    - name: "kube-controller-manager"
      image: "kube-controller-manager:1.1.2"
      command: 
        - "kube-controller-manager"
        - "--master=http://127.0.0.1:8080"
        - "--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
        - "--root-ca-file=/etc/kubernetes/ssl/ca.pem"
      livenessProbe:
        httpGet:
          host: 127.0.0.1
          path: /healthz
          port: <span class="highlight">10252</span>
        initialDelaySeconds: 15
        timeoutSeconds: 1
      volumeMounts:
        - mountPath: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
          readOnly: true
        - mountPath: /etc/ssl/certs
          name: ssl-certs-host
          readOnly: true
</code></pre>
<p>Refer to <a href="http://kubernetes.io/v1.1/docs/admin/kube-controller-manager.html" rel="nofollow">the official kube-controller-manager reference</a> for a full overview of all arguments.</p>

<p>Combining the above snippets together, the full <code>kube-controller-manager.yaml</code> Pod manifest file will look as follows:</p>
<div class="code-label " title="/srv/kubernetes/manifests/kube-controller-manager.yaml">/srv/kubernetes/manifests/kube-controller-manager.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Pod
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: kube-controller-manager
</li><li class="line" prefix="6">  namespace: kube-system
</li><li class="line" prefix="7">spec:
</li><li class="line" prefix="8">  hostNetwork: true
</li><li class="line" prefix="9">  volumes:
</li><li class="line" prefix="10">    - hostPath:
</li><li class="line" prefix="11">        path: /etc/kubernetes/ssl
</li><li class="line" prefix="12">      name: ssl-certs-kubernetes
</li><li class="line" prefix="13">    - hostPath:
</li><li class="line" prefix="14">        path: /usr/share/ca-certificates
</li><li class="line" prefix="15">      name: ssl-certs-host
</li><li class="line" prefix="16">  containers:
</li><li class="line" prefix="17">    - name: "kube-controller-manager"
</li><li class="line" prefix="18">      image: "kube-controller-manager:1.1.2"
</li><li class="line" prefix="19">      command: 
</li><li class="line" prefix="20">        - "kube-controller-manager"
</li><li class="line" prefix="21">        - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="22">        - "--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="23">        - "--root-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="24">      livenessProbe:
</li><li class="line" prefix="25">        httpGet:
</li><li class="line" prefix="26">          host: 127.0.0.1
</li><li class="line" prefix="27">          path: /healthz
</li><li class="line" prefix="28">          port: 10252
</li><li class="line" prefix="29">        initialDelaySeconds: 15
</li><li class="line" prefix="30">        timeoutSeconds: 1
</li><li class="line" prefix="31">      volumeMounts:
</li><li class="line" prefix="32">        - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="33">          name: ssl-certs-kubernetes
</li><li class="line" prefix="34">          readOnly: true
</li><li class="line" prefix="35">        - mountPath: /etc/ssl/certs
</li><li class="line" prefix="36">          name: ssl-certs-host
</li><li class="line" prefix="37">          readOnly: true
</li></ul></code></pre>
<h3 id="controller-services-set-up-the-kube-scheduler-pod-manifest">Controller Services set up: The kube-scheduler Pod manifest</h3>

<p>The scheduler is the last major piece of our control services. It monitors the API for unscheduled pods, finds them a machine to run on, and communicates the decision back to the API.</p>

<p>The full <code>kube-scheduler.yaml</code> Pod manifest file introduces no new concepts, does liveness probes on the scheduler default port of <code>10251</code>, doesn't require any volumes and looks as follows:</p>
<div class="code-label " title="/srv/kubernetes/manifests/kube-scheduler.yaml">/srv/kubernetes/manifests/kube-scheduler.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Pod
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: kube-scheduler
</li><li class="line" prefix="6">  namespace: kube-system
</li><li class="line" prefix="7">spec:
</li><li class="line" prefix="8">  hostNetwork: true
</li><li class="line" prefix="9">  containers:
</li><li class="line" prefix="10">    - name: "kube-scheduler"
</li><li class="line" prefix="11">      image: "kube-scheduler:1.1.2"
</li><li class="line" prefix="12">      command:
</li><li class="line" prefix="13">        - "kube-scheduler"
</li><li class="line" prefix="14">        - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="15">      livenessProbe:
</li><li class="line" prefix="16">        httpGet:
</li><li class="line" prefix="17">          host: 127.0.0.1
</li><li class="line" prefix="18">          path: /healthz
</li><li class="line" prefix="19">          port: <span class="highlight">10251</span>
</li><li class="line" prefix="20">        initialDelaySeconds: 15
</li><li class="line" prefix="21">        timeoutSeconds: 1
</li></ul></code></pre>
<p>Refer to <a href="http://kubernetes.io/v1.1/docs/admin/kube-scheduler.html" rel="nofollow">the official kube-scheduler reference</a> for a full overview of all arguments.</p>

<h3 id="embedding-all-pod-manifests-into-the-controller-cloud-config">Embedding all Pod manifests into the Controller cloud-config</h3>

<p>We will embed the Pod manifest files constructed above into our controller <code>cloud-config</code> file and store each manifest under the following paths:</p>

<ol>
<li>/srv/kubernetes/manifests/kube-scheduler.yaml</li>
<li>/srv/kubernetes/manifests/kube-controller-manager.yaml</li>
<li>/srv/kubernetes/manifests/kube-controller.yaml</li>
</ol>

<p>This is achieved through the <code>write-files</code> directive highlighted earlier.</p>
<div class="code-label " title="cloud-config-controller.yaml pod-manifests - snippet">cloud-config-controller.yaml pod-manifests - snippet</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">#cloud-config
</li><li class="line" prefix="2">
</li><li class="line" prefix="3">write-files:
</li><li class="line" prefix="4">  - path: "/srv/kubernetes/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="5">    permissions: "0644"
</li><li class="line" prefix="6">    owner: "root"
</li><li class="line" prefix="7">    content: |
</li><li class="line" prefix="8">      apiVersion: v1
</li><li class="line" prefix="9">      kind: Pod
</li><li class="line" prefix="10">      metadata:
</li><li class="line" prefix="11">        name: kube-scheduler
</li><li class="line" prefix="12">        namespace: kube-system
</li><li class="line" prefix="13">      spec:
</li><li class="line" prefix="14">        hostNetwork: true
</li><li class="line" prefix="15">        containers:
</li><li class="line" prefix="16">          - name: "kube-scheduler"
</li><li class="line" prefix="17">            image: "kube-scheduler:1.1.2"
</li><li class="line" prefix="18">            command:
</li><li class="line" prefix="19">              - "kube-scheduler"
</li><li class="line" prefix="20">              - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="21">            livenessProbe:
</li><li class="line" prefix="22">              httpGet:
</li><li class="line" prefix="23">                host: 127.0.0.1
</li><li class="line" prefix="24">                path: /healthz
</li><li class="line" prefix="25">                port: 10251
</li><li class="line" prefix="26">              initialDelaySeconds: 15
</li><li class="line" prefix="27">              timeoutSeconds: 1
</li><li class="line" prefix="28">  - path: "/srv/kubernetes/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="29">    permissions: "0644"
</li><li class="line" prefix="30">    owner: "root"
</li><li class="line" prefix="31">    content: |
</li><li class="line" prefix="32">      apiVersion: v1
</li><li class="line" prefix="33">      kind: Pod
</li><li class="line" prefix="34">      metadata:
</li><li class="line" prefix="35">        name: kube-controller-manager
</li><li class="line" prefix="36">        namespace: kube-system
</li><li class="line" prefix="37">      spec:
</li><li class="line" prefix="38">        hostNetwork: true
</li><li class="line" prefix="39">        volumes:
</li><li class="line" prefix="40">          - hostPath:
</li><li class="line" prefix="41">              path: /etc/kubernetes/ssl
</li><li class="line" prefix="42">            name: ssl-certs-kubernetes
</li><li class="line" prefix="43">          - hostPath:
</li><li class="line" prefix="44">              path: /usr/share/ca-certificates
</li><li class="line" prefix="45">            name: ssl-certs-host
</li><li class="line" prefix="46">        containers:
</li><li class="line" prefix="47">          - name: "kube-controller-manager"
</li><li class="line" prefix="48">            image: "kube-controller-manager:1.1.2"
</li><li class="line" prefix="49">            command:
</li><li class="line" prefix="50">              - "kube-controller-manager"
</li><li class="line" prefix="51">              - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="52">              - "--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="53">              - "--root-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="54">            livenessProbe:
</li><li class="line" prefix="55">              httpGet:
</li><li class="line" prefix="56">                host: 127.0.0.1
</li><li class="line" prefix="57">                path: /healthz
</li><li class="line" prefix="58">                port: 10252
</li><li class="line" prefix="59">              initialDelaySeconds: 15
</li><li class="line" prefix="60">              timeoutSeconds: 1
</li><li class="line" prefix="61">            volumeMounts:
</li><li class="line" prefix="62">              - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="63">                name: ssl-certs-kubernetes
</li><li class="line" prefix="64">                readOnly: true
</li><li class="line" prefix="65">              - mountPath: /etc/ssl/certs
</li><li class="line" prefix="66">                name: ssl-certs-host
</li><li class="line" prefix="67">                readOnly: true
</li><li class="line" prefix="68">  - path: "/srv/kubernetes/manifests/kube-controller.yaml"
</li><li class="line" prefix="69">    permissions: "0644"
</li><li class="line" prefix="70">    owner: "root"
</li><li class="line" prefix="71">    content: |
</li><li class="line" prefix="72">      apiVersion: v1
</li><li class="line" prefix="73">      kind: Pod
</li><li class="line" prefix="74">      metadata:
</li><li class="line" prefix="75">        name: kube-controller
</li><li class="line" prefix="76">        namespace: kube-system
</li><li class="line" prefix="77">      spec:
</li><li class="line" prefix="78">        hostNetwork: true
</li><li class="line" prefix="79">        volumes:
</li><li class="line" prefix="80">          - hostPath:
</li><li class="line" prefix="81">              path: /etc/kubernetes/ssl
</li><li class="line" prefix="82">            name: ssl-certs-kubernetes
</li><li class="line" prefix="83">          - hostPath:
</li><li class="line" prefix="84">              path: /usr/share/ca-certificates
</li><li class="line" prefix="85">            name: ssl-certs-host
</li><li class="line" prefix="86">          - hostPath:
</li><li class="line" prefix="87">              path: /srv/kubernetes/manifests
</li><li class="line" prefix="88">            name: manifest-src
</li><li class="line" prefix="89">          - hostPath:
</li><li class="line" prefix="90">              path: /etc/kubernetes/manifests
</li><li class="line" prefix="91">            name: manifest-dst
</li><li class="line" prefix="92">        containers:
</li><li class="line" prefix="93">          - name: "kube-apiserver"
</li><li class="line" prefix="94">            image: "kube-apiserver:1.1.2"
</li><li class="line" prefix="95">            command: 
</li><li class="line" prefix="96">              - "kube-apiserver"
</li><li class="line" prefix="97">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="98">              - "--bind-address=0.0.0.0"
</li><li class="line" prefix="99">              - "--secure_port=443"
</li><li class="line" prefix="100">              - "--advertise-address=$public_ipv4"
</li><li class="line" prefix="101">              - "--service-cluster-ip-range=10.3.0.0/24"
</li><li class="line" prefix="102">              - "--service-node-port-range=30000-37000"
</li><li class="line" prefix="103">              - "--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
</li><li class="line" prefix="104">              - "--allow-privileged=true"
</li><li class="line" prefix="105">              - "--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem"
</li><li class="line" prefix="106">              - "--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="107">              - "--client-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="108">              - "--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="109">            ports:
</li><li class="line" prefix="110">              - containerPort: 443
</li><li class="line" prefix="111">                hostPort: 443
</li><li class="line" prefix="112">                name: https
</li><li class="line" prefix="113">              - containerPort: 8080
</li><li class="line" prefix="114">                hostPort: 8080
</li><li class="line" prefix="115">                name: local
</li><li class="line" prefix="116">            volumeMounts:
</li><li class="line" prefix="117">              - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="118">                name: ssl-certs-kubernetes
</li><li class="line" prefix="119">                readOnly: true
</li><li class="line" prefix="120">              - mountPath: /etc/ssl/certs
</li><li class="line" prefix="121">                name: ssl-certs-host
</li><li class="line" prefix="122">                readOnly: true
</li><li class="line" prefix="123">          - name: "scheduler-elector"
</li><li class="line" prefix="124">            image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="125">            args:
</li><li class="line" prefix="126">              - "--whoami=$public_ipv4"
</li><li class="line" prefix="127">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="128">              - "--key=scheduler"
</li><li class="line" prefix="129">              - "--source-file=/src/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="130">              - "--dest-file=/dst/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="131">            volumeMounts:
</li><li class="line" prefix="132">              - mountPath: /src/manifests
</li><li class="line" prefix="133">                name: manifest-src
</li><li class="line" prefix="134">                readOnly: true
</li><li class="line" prefix="135">              - mountPath: /dst/manifests
</li><li class="line" prefix="136">                name: manifest-dst
</li><li class="line" prefix="137">          - name: "controller-manager-elector"
</li><li class="line" prefix="138">            image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="139">            args:
</li><li class="line" prefix="140">              - "--whoami=$public_ipv4"
</li><li class="line" prefix="141">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="142">              - "--key=controller"
</li><li class="line" prefix="143">              - "--source-file=/src/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="144">              - "--dest-file=/dst/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="145">            volumeMounts:
</li><li class="line" prefix="146">              - mountPath: /src/manifests
</li><li class="line" prefix="147">                name: manifest-src
</li><li class="line" prefix="148">                readOnly: true
</li><li class="line" prefix="149">              - mountPath: /dst/manifests
</li><li class="line" prefix="150">                name: manifest-dst
</li><li class="line" prefix="151">
</li></ul></code></pre>
<h3 id="the-final-controller-cloud-config-with-all-coreos-units">The Final Controller cloud-config with all CoreOS Units</h3>

<p>To finally create the controller Droplet, we will combine all above <code>cloud-config</code> snippets into a single <code>cloud-config</code> file:</p>

<ol>
<li><code>write-files</code> snippets:

<ol>
<li><code>/opt/bin/pull-kube-images.sh</code> script to pre-load the Kubernetes docker images</li>
<li><code>/srv/kubernetes/manifests/kube-scheduler.yaml</code> Pod manifest source for the <code>kube-scheduler</code></li>
<li><code>/srv/kubernetes/manifests/kube-controller-manager.yaml</code> Pod manifest source for the <code>kube-controller-manager</code></li>
<li><code>/etc/kubernetes/manifests/kube-controller.yaml</code> Pod manifest to start the <code>kube-apiserver</code>, <code>controller-manager-elector</code> and <code>scheduler-elector</code></li>
</ol></li>
<li><code>etcd2.service</code> snippet to start a local Etcd proxy, notice the <code>ETCD_PEER</code> placeholder.</li>
<li><code>flanneld.service</code> snippet to start the overlay network daemon with a drop-in to configure the network subnet</li>
<li><code>docker.service</code> drop-in snippet to add flannel dependency</li>
<li><code>kubelet.service</code> snippet running the kubelet in standalone mode</li>
<li><code>kube-proxy.service</code> snippet running the <code>kube-proxy</code> service</li>
<li><code>kube-pull-images.service</code> snippet running the script to pre-load the Kubernetes docker images</li>
<li><code>create-kube-system.service</code> snippet creating the <code>kube-system</code> namespace as soon as the API server is available</li>
</ol>

<p>Several of these services depend on the TLS assets, which we generate as soon as the IP addresses are known for our Droplet.</p>

<p>In a multi-controller set-up, every controller node may be created using this <code>cloud-config</code>, although the <code>flanneld</code> drop-in and <code>create-kube-system.service</code> unit only need to be ran once within the cluster and are not required on subsequent controller nodes.</p>

<p>&lt;!-- TODO: test if including the flannel config commands twice cause the other controller nodes to fail? --&gt;</p>

<p>As we are running a single controller node, we are also turning off CoreOS updates and reboots in our <code>cloud-config</code>.</p>
<div class="code-label " title="cloud-config-controller.yaml">cloud-config-controller.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">#cloud-config
</li><li class="line" prefix="2">
</li><li class="line" prefix="3">write-files:
</li><li class="line" prefix="4">  - path: /opt/bin/pull-kube-images.sh
</li><li class="line" prefix="5">    permissions: '0755'
</li><li class="line" prefix="6">    content: |
</li><li class="line" prefix="7">      #!/bin/bash
</li><li class="line" prefix="8">      tag=1.1.2
</li><li class="line" prefix="9">      docker_wrapped_binaries=(
</li><li class="line" prefix="10">        "kube-apiserver"
</li><li class="line" prefix="11">        "kube-controller-manager"
</li><li class="line" prefix="12">        "kube-scheduler"
</li><li class="line" prefix="13">      )
</li><li class="line" prefix="14">      temp_dir="$(mktemp -d -t 'kube-server-XXXX')"
</li><li class="line" prefix="15">      for binary in "${docker_wrapped_binaries[@]}"; do
</li><li class="line" prefix="16">        docker_tag="$(curl -sL https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.docker_tag)"
</li><li class="line" prefix="17">        curl -sLo ${temp_dir}/${binary}.tar https://storage.googleapis.com/kubernetes-release/release/v${tag}/bin/linux/amd64/${binary}.tar
</li><li class="line" prefix="18">        docker load -i ${temp_dir}/${binary}.tar
</li><li class="line" prefix="19">        docker tag -f "gcr.io/google_containers/${binary}:${docker_tag}" "${binary}:${tag}"
</li><li class="line" prefix="20">      done;
</li><li class="line" prefix="21">      rm -rf "${temp_dir}";
</li><li class="line" prefix="22">      exit $?
</li><li class="line" prefix="23">  - path: "/srv/kubernetes/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="24">    permissions: "0644"
</li><li class="line" prefix="25">    owner: "root"
</li><li class="line" prefix="26">    content: |
</li><li class="line" prefix="27">      apiVersion: v1
</li><li class="line" prefix="28">      kind: Pod
</li><li class="line" prefix="29">      metadata:
</li><li class="line" prefix="30">        name: kube-scheduler
</li><li class="line" prefix="31">        namespace: kube-system
</li><li class="line" prefix="32">      spec:
</li><li class="line" prefix="33">        hostNetwork: true
</li><li class="line" prefix="34">        containers:
</li><li class="line" prefix="35">          - name: "kube-scheduler"
</li><li class="line" prefix="36">            image: "kube-scheduler:1.1.2"
</li><li class="line" prefix="37">            command:
</li><li class="line" prefix="38">              - "kube-scheduler"
</li><li class="line" prefix="39">              - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="40">            livenessProbe:
</li><li class="line" prefix="41">              httpGet:
</li><li class="line" prefix="42">                host: 127.0.0.1
</li><li class="line" prefix="43">                path: /healthz
</li><li class="line" prefix="44">                port: 10251
</li><li class="line" prefix="45">              initialDelaySeconds: 15
</li><li class="line" prefix="46">              timeoutSeconds: 1
</li><li class="line" prefix="47">  - path: "/srv/kubernetes/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="48">    permissions: "0644"
</li><li class="line" prefix="49">    owner: "root"
</li><li class="line" prefix="50">    content: |
</li><li class="line" prefix="51">      apiVersion: v1
</li><li class="line" prefix="52">      kind: Pod
</li><li class="line" prefix="53">      metadata:
</li><li class="line" prefix="54">        name: kube-controller-manager
</li><li class="line" prefix="55">        namespace: kube-system
</li><li class="line" prefix="56">      spec:
</li><li class="line" prefix="57">        hostNetwork: true
</li><li class="line" prefix="58">        volumes:
</li><li class="line" prefix="59">          - hostPath:
</li><li class="line" prefix="60">              path: /etc/kubernetes/ssl
</li><li class="line" prefix="61">            name: ssl-certs-kubernetes
</li><li class="line" prefix="62">          - hostPath:
</li><li class="line" prefix="63">              path: /usr/share/ca-certificates
</li><li class="line" prefix="64">            name: ssl-certs-host
</li><li class="line" prefix="65">        containers:
</li><li class="line" prefix="66">          - name: "kube-controller-manager"
</li><li class="line" prefix="67">            image: "kube-controller-manager:1.1.2"
</li><li class="line" prefix="68">            command:
</li><li class="line" prefix="69">              - "kube-controller-manager"
</li><li class="line" prefix="70">              - "--master=http://127.0.0.1:8080"
</li><li class="line" prefix="71">              - "--service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="72">              - "--root-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="73">            livenessProbe:
</li><li class="line" prefix="74">              httpGet:
</li><li class="line" prefix="75">                host: 127.0.0.1
</li><li class="line" prefix="76">                path: /healthz
</li><li class="line" prefix="77">                port: 10252
</li><li class="line" prefix="78">              initialDelaySeconds: 15
</li><li class="line" prefix="79">              timeoutSeconds: 1
</li><li class="line" prefix="80">            volumeMounts:
</li><li class="line" prefix="81">              - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="82">                name: ssl-certs-kubernetes
</li><li class="line" prefix="83">                readOnly: true
</li><li class="line" prefix="84">              - mountPath: /etc/ssl/certs
</li><li class="line" prefix="85">                name: ssl-certs-host
</li><li class="line" prefix="86">                readOnly: true
</li><li class="line" prefix="87">  - path: "/srv/kubernetes/manifests/kube-controller.yaml"
</li><li class="line" prefix="88">    permissions: "0644"
</li><li class="line" prefix="89">    owner: "root"
</li><li class="line" prefix="90">    content: |
</li><li class="line" prefix="91">      apiVersion: v1
</li><li class="line" prefix="92">      kind: Pod
</li><li class="line" prefix="93">      metadata:
</li><li class="line" prefix="94">        name: kube-controller
</li><li class="line" prefix="95">        namespace: kube-system
</li><li class="line" prefix="96">      spec:
</li><li class="line" prefix="97">        hostNetwork: true
</li><li class="line" prefix="98">        volumes:
</li><li class="line" prefix="99">          - hostPath:
</li><li class="line" prefix="100">              path: /etc/kubernetes/ssl
</li><li class="line" prefix="101">            name: ssl-certs-kubernetes
</li><li class="line" prefix="102">          - hostPath:
</li><li class="line" prefix="103">              path: /usr/share/ca-certificates
</li><li class="line" prefix="104">            name: ssl-certs-host
</li><li class="line" prefix="105">          - hostPath:
</li><li class="line" prefix="106">              path: /srv/kubernetes/manifests
</li><li class="line" prefix="107">            name: manifest-src
</li><li class="line" prefix="108">          - hostPath:
</li><li class="line" prefix="109">              path: /etc/kubernetes/manifests
</li><li class="line" prefix="110">            name: manifest-dst
</li><li class="line" prefix="111">        containers:
</li><li class="line" prefix="112">          - name: "kube-apiserver"
</li><li class="line" prefix="113">            image: "kube-apiserver:1.1.2"
</li><li class="line" prefix="114">            command:
</li><li class="line" prefix="115">              - "kube-apiserver" 
</li><li class="line" prefix="116">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="117">              - "--bind-address=0.0.0.0"
</li><li class="line" prefix="118">              - "--secure_port=443"
</li><li class="line" prefix="119">              - "--advertise-address=$public_ipv4"
</li><li class="line" prefix="120">              - "--service-cluster-ip-range=10.3.0.0/24"
</li><li class="line" prefix="121">              - "--service-node-port-range=30000-37000"
</li><li class="line" prefix="122">              - "--admission-control=NamespaceLifecycle,LimitRanger,SecurityContextDeny,ServiceAccount,ResourceQuota"
</li><li class="line" prefix="123">              - "--allow-privileged=true"
</li><li class="line" prefix="124">              - "--tls-cert-file=/etc/kubernetes/ssl/apiserver.pem"
</li><li class="line" prefix="125">              - "--tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="126">              - "--client-ca-file=/etc/kubernetes/ssl/ca.pem"
</li><li class="line" prefix="127">              - "--service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem"
</li><li class="line" prefix="128">            ports:
</li><li class="line" prefix="129">              - containerPort: 443
</li><li class="line" prefix="130">                hostPort: 443
</li><li class="line" prefix="131">                name: https
</li><li class="line" prefix="132">              - containerPort: 8080
</li><li class="line" prefix="133">                hostPort: 8080
</li><li class="line" prefix="134">                name: local
</li><li class="line" prefix="135">            volumeMounts:
</li><li class="line" prefix="136">              - mountPath: /etc/kubernetes/ssl
</li><li class="line" prefix="137">                name: ssl-certs-kubernetes
</li><li class="line" prefix="138">                readOnly: true
</li><li class="line" prefix="139">              - mountPath: /etc/ssl/certs
</li><li class="line" prefix="140">                name: ssl-certs-host
</li><li class="line" prefix="141">                readOnly: true
</li><li class="line" prefix="142">          - name: "scheduler-elector"
</li><li class="line" prefix="143">            image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="144">            args:
</li><li class="line" prefix="145">              - "--whoami=$public_ipv4"
</li><li class="line" prefix="146">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="147">              - "--key=scheduler"
</li><li class="line" prefix="148">              - "--source-file=/src/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="149">              - "--dest-file=/dst/manifests/kube-scheduler.yaml"
</li><li class="line" prefix="150">            volumeMounts:
</li><li class="line" prefix="151">              - mountPath: /src/manifests
</li><li class="line" prefix="152">                name: manifest-src
</li><li class="line" prefix="153">                readOnly: true
</li><li class="line" prefix="154">              - mountPath: /dst/manifests
</li><li class="line" prefix="155">                name: manifest-dst
</li><li class="line" prefix="156">          - name: "controller-manager-elector"
</li><li class="line" prefix="157">            image: "gcr.io/google_containers/podmaster:1.1"
</li><li class="line" prefix="158">            args:
</li><li class="line" prefix="159">              - "--whoami=$public_ipv4"
</li><li class="line" prefix="160">              - "--etcd-servers=http://127.0.0.1:2379"
</li><li class="line" prefix="161">              - "--key=controller"
</li><li class="line" prefix="162">              - "--source-file=/src/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="163">              - "--dest-file=/dst/manifests/kube-controller-manager.yaml"
</li><li class="line" prefix="164">            volumeMounts:
</li><li class="line" prefix="165">              - mountPath: /src/manifests
</li><li class="line" prefix="166">                name: manifest-src
</li><li class="line" prefix="167">                readOnly: true
</li><li class="line" prefix="168">              - mountPath: /dst/manifests
</li><li class="line" prefix="169">                name: manifest-dst
</li><li class="line" prefix="170">coreos:
</li><li class="line" prefix="171">  etcd2:
</li><li class="line" prefix="172">    proxy: on 
</li><li class="line" prefix="173">    listen-client-urls: http://localhost:2379
</li><li class="line" prefix="174">    initial-cluster: "etcd-01=ETCD_PEER"
</li><li class="line" prefix="175">  units:
</li><li class="line" prefix="176">    - name: "etcd2.service"
</li><li class="line" prefix="177">      command: "start"
</li><li class="line" prefix="178">    - name: "flanneld.service"
</li><li class="line" prefix="179">      command: "start"
</li><li class="line" prefix="180">      drop-ins:
</li><li class="line" prefix="181">        - name: 50-network-config.conf
</li><li class="line" prefix="182">          content: |
</li><li class="line" prefix="183">            [Unit]
</li><li class="line" prefix="184">            Requires=etcd2.service
</li><li class="line" prefix="185">            [Service]
</li><li class="line" prefix="186">            ExecStartPre=/usr/bin/etcdctl set /coreos.com/network/config '{"Network":"10.2.0.0/16", "Backend": {"Type": "vxlan"}}'
</li><li class="line" prefix="187">    - name: "docker.service"
</li><li class="line" prefix="188">      command: "start"
</li><li class="line" prefix="189">      drop-ins:
</li><li class="line" prefix="190">        - name: 40-flannel.conf
</li><li class="line" prefix="191">          content: |
</li><li class="line" prefix="192">            [Unit]
</li><li class="line" prefix="193">            Requires=flanneld.service
</li><li class="line" prefix="194">            After=flanneld.service
</li><li class="line" prefix="195">    - name: "pull-kube-images.service"
</li><li class="line" prefix="196">      command: "start"
</li><li class="line" prefix="197">      content: |
</li><li class="line" prefix="198">        [Unit]
</li><li class="line" prefix="199">        Description=Pull and load all Docker wrapped Kubernetes binaries
</li><li class="line" prefix="200">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="201">        Requires=network-online.target docker.service
</li><li class="line" prefix="202">        After=network-online.target docker.service
</li><li class="line" prefix="203">        [Service]
</li><li class="line" prefix="204">        ExecStart=/opt/bin/pull-kube-images.sh
</li><li class="line" prefix="205">        RemainAfterExit=yes
</li><li class="line" prefix="206">        Type=oneshot
</li><li class="line" prefix="207">    - name: "kube-proxy.service"
</li><li class="line" prefix="208">      command: "start"
</li><li class="line" prefix="209">      content: |
</li><li class="line" prefix="210">        [Unit]
</li><li class="line" prefix="211">        Description=Kubernetes Proxy
</li><li class="line" prefix="212">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="213">        Requires=network-online.target
</li><li class="line" prefix="214">        After=network-online.target
</li><li class="line" prefix="215">        [Service]
</li><li class="line" prefix="216">        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
</li><li class="line" prefix="217">        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
</li><li class="line" prefix="218">        # wait for kube-apiserver to be up and ready
</li><li class="line" prefix="219">        ExecStartPre=/bin/bash -c "until /usr/bin/curl -s http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
</li><li class="line" prefix="220">        ExecStart=/opt/bin/kube-proxy \
</li><li class="line" prefix="221">        --master=http://127.0.0.1:8080 \
</li><li class="line" prefix="222">        --proxy-mode=iptables \
</li><li class="line" prefix="223">        --hostname-override=$public_ipv4
</li><li class="line" prefix="224">        TimeoutStartSec=10
</li><li class="line" prefix="225">        Restart=always
</li><li class="line" prefix="226">        RestartSec=10
</li><li class="line" prefix="227">    - name: "kube-kubelet.service"
</li><li class="line" prefix="228">      command: "start"
</li><li class="line" prefix="229">      content: |
</li><li class="line" prefix="230">        [Unit]
</li><li class="line" prefix="231">        Description=Kubernetes Kubelet
</li><li class="line" prefix="232">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="233">        Requires=docker.service
</li><li class="line" prefix="234">        After=docker.service
</li><li class="line" prefix="235">        [Service]
</li><li class="line" prefix="236">        ExecStartPre=-/bin/bash -c "mkdir -p /etc/kubernetes/{manifests,ssl}"
</li><li class="line" prefix="237">        ExecStart=/usr/bin/kubelet \
</li><li class="line" prefix="238">        --api-servers=http://127.0.0.1:8080 \
</li><li class="line" prefix="239">        --register-node=false \
</li><li class="line" prefix="240">        --allow-privileged=true \
</li><li class="line" prefix="241">        --config=/etc/kubernetes/manifests \
</li><li class="line" prefix="242">        --hostname-override=$public_ipv4 \
</li><li class="line" prefix="243">        --cluster-dns=10.3.0.10 \
</li><li class="line" prefix="244">        --cluster-domain=cluster.local
</li><li class="line" prefix="245">        Restart=always
</li><li class="line" prefix="246">        RestartSec=10
</li><li class="line" prefix="247">    - name: "tls-ready.service"
</li><li class="line" prefix="248">      command: "start"
</li><li class="line" prefix="249">      content: |
</li><li class="line" prefix="250">        [Unit]
</li><li class="line" prefix="251">        Description=Ensure TLS assets are ready
</li><li class="line" prefix="252">        Requires=kube-kubelet.service
</li><li class="line" prefix="253">        After=kube-kubelet.service
</li><li class="line" prefix="254">        [Service]
</li><li class="line" prefix="255">        Type=oneshot
</li><li class="line" prefix="256">        RemainAfterExit=yes
</li><li class="line" prefix="257">        ExecStart=/bin/bash -c "until [ `ls -1 /etc/kubernetes/ssl/{apiserver,apiserver-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \"waiting for TLS assets...\";sleep 5; done"
</li><li class="line" prefix="258">        ExecStart=/usr/bin/cp /srv/kubernetes/manifests/kube-controller.yaml /etc/kubernetes/manifests/
</li><li class="line" prefix="259">    - name: "create-kube-system-ns.service"
</li><li class="line" prefix="260">      command: "start"
</li><li class="line" prefix="261">      content: |
</li><li class="line" prefix="262">        [Unit]
</li><li class="line" prefix="263">        Description=Create the kube-system namespace
</li><li class="line" prefix="264">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="265">        Requires=kube-kubelet.service
</li><li class="line" prefix="266">        After=kube-kubelet.service
</li><li class="line" prefix="267">        [Service]
</li><li class="line" prefix="268">        ExecStartPre=/bin/bash -c "until /usr/bin/curl -s http://127.0.0.1:8080; do echo \"waiting for API server to come online...\"; sleep 3; done"
</li><li class="line" prefix="269">        ExecStart=/usr/bin/curl -XPOST -d'{"apiVersion":"v1","kind":"Namespace","metadata":{"name":"kube-system"}}' "http://127.0.0.1:8080/api/v1/namespaces"
</li><li class="line" prefix="270">        RemainAfterExit=yes
</li><li class="line" prefix="271">        Type=oneshot
</li><li class="line" prefix="272">  update:
</li><li class="line" prefix="273">    group: alpha
</li><li class="line" prefix="274">    reboot-strategy: off
</li></ul></code></pre>
<p><a href="https://coreos.com/validate" rel="nofollow">Validate</a> your cloud-config file, then create your <code>kube-controller-01</code> droplet with the following Doctl command. :</p>

<p>Ensure your <code>ETCD_PEER</code> environment variable is still set from the [Deploy the data storage back end](#) section of this tutorial:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">$ echo $ETCD_PEER
</li><li class="line" prefix="$">http://10.129.69.201:2380
</li></ul></code></pre>
<p>If not - set it to the <code>private_ip</code> of your single node ECTD cluster:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">export ETCD_PEER=`doctl -f json d f etcd-01.$region | jq -r '.networks.v4[] | select(.type == "private")  | "http://\(.ip_address):2380"'`
</li></ul></code></pre>
<p>Substitute the <code>ETCD_PEER</code> placeholder from above <code>cloud-config-controller.yaml</code> template file with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">sed -e "s|ETCD_PEER|${ETCD_PEER}|g;" cloud-config-controller.yaml &gt; kube-controller.yaml
</li></ul></code></pre>
<p>And send the command to create the droplet:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d c --wait-for-active \
</li><li class="line" prefix="$">    -i "CoreOS-alpha" \
</li><li class="line" prefix="$">    -s 512mb \
</li><li class="line" prefix="$">    -r "$region" \
</li><li class="line" prefix="$">    -p \
</li><li class="line" prefix="$">    -k k8s-key \
</li><li class="line" prefix="$">    -uf kube-controller.yaml kube-controller-01
</li></ul></code></pre>
<p>&lt;!-- TODO: Droplet size? --&gt;<br>
<strong>Note</strong>: running <code>free -m</code> on a <code>512mb</code> Droplet shows only 12mb free memory after all controller services have started, it may be better to use a <code>1024mb</code> droplet to fully test Kubernetes.</p>

<p>We are waiting for the droplet to be flagged as active before proceeding. Once the Doctl command completes, the Droplet configuration is returned. As it usually takes more time for the Droplet to return its public and private ip addresses, we need to re-query the Droplet configuration. We will cache the json string returned in the <code>$CONTROLLER_JSON</code> environment variable for subsequent commands:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">CONTROLLER_JSON=`doctl -f 'json' d f kube-controller-01.$region`
</li></ul></code></pre>
<p>We parse the private and public IPs out as explained in the [Working with doctl responses](#) section of this tutorial.</p>

<p>&lt;!--</p>

<p>Combined:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">read CONTROLLER_PUBLIC_IP2 CONTROLLER_PRIVATE_IP2 &lt;&lt;&lt;$(echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == "public"), select(.type == "private") | .ip_address')
</li></ul></code></pre>
<p>--&gt;</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">CONTROLLER_PUBLIC_IP=`echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == "public") | .ip_address'`
</li><li class="line" prefix="$">CONTROLLER_PRIVATE_IP=`echo $CONTROLLER_JSON | jq -r '.networks.v4[] | select(.type == "private") | .ip_address'`
</li></ul></code></pre>
<p>Confirm values were populated correctly:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
</li></ul></code></pre>
<p>You may monitor the initialization process driven by <code>cloud-config</code> by connecting to the Droplet:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@$CONTROLLER_PUBLIC_IP
</li></ul></code></pre>
<p>and follow the <code>oem-cloudinit</code> service running the <code>cloud-config</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">journalctl -u oem-cloudinit -f
</li></ul></code></pre>
<p>Once the <code>oem-cloudinit</code> service has reached the "tls-ready.service" it will wait for our actions. <code>CTRL+C</code> and confirm Etcd proxy is running:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">systemctl status etcd2
</li></ul></code></pre>
<p>Confirm Flannel service started</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">systemctl status flanneld
</li></ul></code></pre>
<p>If Flannel started, confirm it was able to retrieve its configuration from Etcd: </p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">cat /run/flannel/subnet.env
</li></ul></code></pre>
<p>The Docker daemon options for the overlay network generated by Flannel are stored under <code>/run/flannel_docker_opts.env</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">cat /run/flannel_docker_opts.env
</li></ul></code></pre>
<p>Confirm all services are running:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">systemctl status tls-ready
</li><li class="line" prefix="$">systemctl status docker
</li><li class="line" prefix="$">systemctl status pull-kube-images
</li></ul></code></pre>
<p>confirm the docker images loaded the kube images have all been loaded by running </p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">docker images | grep kube
</li></ul></code></pre>
<p>confirm all files have been written to disk:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ls -l /opt/bin/
</li><li class="line" prefix="$">ls -l /srv/kubernetes/manifests/
</li></ul></code></pre>
<p>Monitor when the kubelet will launch the containers (which will happen as soon as we copy the TLS assets)</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">watch -n 1 'docker ps --format="table {{.Image}}\t{{.ID}}\t{{.Status}}\t{{.Ports}}" -a' 
</li></ul></code></pre>
<p>If the <code>oem-cloudinit</code> failed, review the <code>cloud-config</code> stored by the Digital Ocean Metadata Service:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -sL 169.254.169.254/metadata/v1/user-data | less
</li></ul></code></pre>
<p>If you find a mistake in the <code>cloud-config</code>, your only option is to delete and re-create the Droplet.</p>

<h3 id="generating-and-transferring-the-kube-apiserver-tls-assets">Generating and Transferring the kube-apiserver TLS Assets</h3>

<p>The address of the controller node is required for the API Server certificate. In most cases this will be the publicly routable IP or hostname of the controller cluster. Worker nodes must be able to reach the controller node(s) via this address on port 443. Additionally, external clients (such as an administrator using kubectl) will also need access, since this will run the Kubernetes API endpoint.</p>

<p>If you will be running a highly-available control-plane consisting of multiple controller nodes, then the host name for the certificate will ideally be pointing at a network load balancer that sits in front of the controller nodes. Alternatively, a DNS name can be configured which will resolve to the controller node IPs. In either case, the certificate which is generated next, needs to have the correct CommonName and/or SubjectAlternateNames.</p>

<p>Ensure you have populated the <code>$CONTROLLER_PUBLIC_IP</code>, <code>$region</code> and <code>$CONTROLLER_PRIVATE_IP</code> variables:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo $region &amp;&amp; echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
</li></ul></code></pre>
<p>which should show output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>$ echo $region &amp;&amp; echo $CONTROLLER_PUBLIC_IP &amp;&amp; echo $CONTROLLER_PRIVATE_IP
ams2
188.166.252.4
10.130.158.66
</code></pre>
<p>The API Server will take the first IP in the Kubernetes Service IP range. In this tutorial we are using the <code>10.3.0.1/24</code> IP range for the cluster services (See [Running the Kubelet in standalone mode](#)). The IP used by the <code>apiserver</code> service within Kubernetes is thus <code>10.3.0.1</code> and needs to be included in the API server certificate. If you are using a different Service IP range, update the value in the configuration file below.</p>

<p>Now we are ready to prepare the openssl config file (see <a href="https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/openssl.md" rel="nofollow">CoreOS OpenSSL tutorial</a>).</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">cat &gt; openssl.cnf &lt;&lt;EOF
</li><li class="line" prefix="$">[req]
</li><li class="line" prefix="$">req_extensions = v3_req
</li><li class="line" prefix="$">distinguished_name = req_distinguished_name
</li><li class="line" prefix="$">[req_distinguished_name]
</li><li class="line" prefix="$">[ v3_req ]
</li><li class="line" prefix="$">basicConstraints = CA:FALSE
</li><li class="line" prefix="$">keyUsage = nonRepudiation, digitalSignature, keyEncipherment
</li><li class="line" prefix="$">subjectAltName = <a href="https://www.digitalocean.com/community/users/alt_names" class="username-tag">@alt_names</a>
</li><li class="line" prefix="$">[alt_names]
</li><li class="line" prefix="$">DNS.1 = kube-controller-01.$region
</li><li class="line" prefix="$">IP.1 = 10.3.0.1
</li><li class="line" prefix="$">IP.2 = $CONTROLLER_PUBLIC_IP
</li><li class="line" prefix="$">IP.3 = $CONTROLLER_PRIVATE_IP
</li><li class="line" prefix="$">EOF
</li></ul></code></pre>
<p>&lt;!-- TODO: change apiserver openssl.cnf</p>

<p>use</p>
<pre class="code-pre "><code langs="">$ENV::CONTROLLER_PUBLIC_IP 
</code></pre>
<p>instead of cat &lt;&lt;EOF...?</p>

<p>refer to <a href="https://github.com/coreos/coreos-kubernetes/blob/master/Documentation/openssl.md#openssl-config-1" rel="nofollow">worker config</a></p>

<p>--&gt;</p>

<p>Generate the API server private key (<code>apiserver-key.pem</code>) which is needed to create the signing request:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl genrsa -out ~/.kube/apiserver-key.pem 2048
</li></ul></code></pre>
<p>Generate the Certificate Signing Request (CSR):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl req -new -key ~/.kube/apiserver-key.pem -out apiserver.csr -subj "/CN=kube-apiserver" -config openssl.cnf
</li></ul></code></pre>
<p>And finally, use the Certificate Authority to generate the signed API Server certificate (<code>apiserver.pem</code>):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl x509 -req -in apiserver.csr \
</li><li class="line" prefix="$"> -CA "$HOME/.kube/ca.pem" \
</li><li class="line" prefix="$"> -CAkey "$HOME/.kube/ca-key.pem" \
</li><li class="line" prefix="$"> -CAcreateserial \
</li><li class="line" prefix="$"> -out "$HOME/.kube/apiserver.pem" \
</li><li class="line" prefix="$"> -days 365 \
</li><li class="line" prefix="$"> -extensions v3_req \
</li><li class="line" prefix="$"> -extfile openssl.cnf
</li></ul></code></pre>
<p>&lt;!-- </p>

<p>We no longer need the CSR, delete the <code>apiserver.csr</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">rm apiserver.csr
</li></ul></code></pre>
<p>--&gt;</p>

<p>[note]<br>
<strong>Note</strong>: the above command does not work on <code>git-for-windows</code> due to windows path conversions, it is recommended to copy the <code>apiserver.csr</code> and <code>openssl.cnf</code> to <code>~/.kube/</code> and just run the command from within the <code>~/.kube/</code> directory (without the <code>"$HOME/.kube/"</code> parts)<br>
</p>

<p>Copy the necessary certificates to the controller node. The <code>core</code> user does not have write permissions to <code>/etc/kubernetes/ssl</code> directly, thus we store the files in the home directory first.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">scp ~/.kube/apiserver-key.pem ~/.kube/apiserver.pem ~/.kube/ca.pem core@$CONTROLLER_PUBLIC_IP:~
</li></ul></code></pre>
<p>Move the certificates from the Home directory to the <code>/etc/kubernetes/ssl</code> path and fix the permissions by executing the following commands over ssh:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@$CONTROLLER_PUBLIC_IP &lt;&lt;EOF
</li><li class="line" prefix="$">sudo mkdir -p /etc/kubernetes/ssl/
</li><li class="line" prefix="$">sudo mv ~core/*.pem /etc/kubernetes/ssl/
</li><li class="line" prefix="$">sudo chown root:root /etc/kubernetes/ssl/*.pem
</li><li class="line" prefix="$">sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
</li><li class="line" prefix="$">EOF
</li></ul></code></pre>
<p><strong>Troubleshooting</strong>: Review the certificate contents with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl x509 -text -noout -in apiserver.pem
</li></ul></code></pre>
<p>As soon as the certificates are available it will take just a few minutes for all the Controller services to start running.</p>

<p>The <code>kube-proxy</code> service will start as soon as the <code>kube-apiserver</code> is available. As we specify the <code>iptables</code> mode, it will try to flush the <code>userpace</code> settings from <code>iptables</code> - which don't exist - this will show up in the log files, but can be ignored</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>$ journalctl -u kube-proxy -f
...
Error flushing userspace chain: error flushing chain "KUBE-NODEPORT-HOST": exit status 1: iptables: No chain/target/match by that name.
Error flushing userspace chain: error flushing chain "KUBE-NODEPORT-CONTAINER": exit status 1: iptables: No chain/target/match by that name.
</code></pre>
<p>If you started the <code>docker ps -a</code> <code>watch</code> on the controller, you should notice all containers being created by the kubelet.</p>

<p>We can confirm the <code>apiserver</code> authenticates itself with the certificate we provided and requires client authentication using <code>curl</code>. As the self-signed root CA used by the cluster is not trusted by our client, we need to pass it in:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces --cacert ~/.kube/ca.pem -v
</li></ul></code></pre>
<p>The <code>-v</code> flag allows us to see the verbose log of communication between our client and the apiserver. As we did not present our client certificate, the server responds with <code>unauthorized</code>.</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>...
* successfully set certificate verify locations:
*   CAfile: /home/demo/.kube/ca.pem
  CApath: none
* TLSv1.2 (OUT), TLS header, Certificate Status (22):
* TLSv1.2 (OUT), TLS handshake, Client hello (1):
* TLSv1.2 (IN), TLS handshake, Server hello (2):
...
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-SHA
* ALPN, server accepted to use http/1.1
* Server certificate:
*        subject: CN=kube-apiserver
*        start date: Dec 15 08:15:44 2015 GMT
*        expire date: Dec 14 08:15:44 2016 GMT
*        subjectAltName: 188.166.252.4 matched
*        issuer: CN=kube-ca
*        SSL certificate verify ok.
...
&lt; HTTP/1.1 401 Unauthorized
&lt; Content-Type: text/plain; charset=utf-8
&lt; Date: Wed, 16 Dec 2015 00:05:36 GMT
&lt; Content-Length: 13
&lt;
<span class="highlight">Unauthorized</span>
...
</code></pre>
<p>We will need to authenticate by presenting a client certificate signed by our Kubernetes root CA, we will generate an admin certificate in the Administrator set up section of this tutorial.</p>

<p>At this stage we can configure our client to communicate with our Kubernetes Controller, although we do not have any worker nodes and won't be able to start the workload yet, this will ensure our configuration is working so far.</p>

<p><strong>Note</strong>: We may re-use the same Controller cloud-config files to spin-up a cluster of Controller Droplets with a load balancer in front of it. In this case, our apiserver certificate should have included all necessary IP addresses (such as the Load Balancer IP) for proper TLS authentication.</p>

<h2 id="step-8-—-setting-up-the-kubernetes-cluster-administrator">Step 8 — Setting Up The Kubernetes Cluster Administrator</h2>

<h3 id="generate-the-cluster-administrator-keypair">Generate the Cluster Administrator Keypair</h3>

<p>Every administrator, needs to have a private key, which we generate using openssl as follows:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl genrsa -out ~/.kube/admin-key.pem 2048
</li></ul></code></pre>
<p>Using his private key, the administrator needs to create a Certificate Signing Request (CSR):</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl req -new -key ~/.kube/admin-key.pem -out admin.csr -subj "/CN=kube-admin"
</li></ul></code></pre>
<p>To be authorized to connect to the Kubernetes apiserver, this <code>admin.csr</code> needs to be sent to and processed by the Kubernetes Cluster root CA to generate  the signed <code>admin.pem</code> certificate:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl x509 -req -in admin.csr -CA ~/.kube/ca.pem -CAkey ~/.kube/ca-key.pem -CAcreateserial -out ~/.kube/admin.pem -days 365
</li></ul></code></pre>
<p>From now on, the administrator can use his <code>admin-key.pem</code> and signed certificate <code>admin.pem</code> to connect to the Kubernetes cluster.</p>

<p>&lt;!--</p>

<p>TODO: also remove the apiserver csr...</p>

<p>We no longer need the <code>admin.csr</code> file:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">rm admin.csr
</li></ul></code></pre>
<p>--&gt;</p>

<p>Test the freshly signed admin certificate by passing it in to the <code>curl</code> command we used earlier:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces --cacert ~/.kube/ca.pem --cert ~/.kube/admin.pem --key ~/.kube/admin-key.pem
</li></ul></code></pre>
<p>Now authenticated, this should return a <code>json</code> response containing all namespaces within our cluster. You can use Jq to simplify the output:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -s https://$CONTROLLER_PUBLIC_IP/api/v1/namespaces \
</li><li class="line" prefix="$"> --cacert ~/.kube/ca.pem --cert ~/.kube/admin.pem --key ~/.kube/admin-key.pem \
</li><li class="line" prefix="$"> | jq .items[].metadata.name
</li></ul></code></pre>
<p>Instead of talking to the API directly, we will download and configure the command line tool <code>kubectl</code>.</p>

<h3 id="download-kubectl">Download Kubectl</h3>

<p>As highlighted in the [Step 4 — Understanding Where To Get The Kubernetes Artifacts](#) section of this tutorial, the <code>kubectl</code> binary can be downloaded from the Google cloud storage bucket.</p>

<p>For 64bit Linux clients:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo curl -Lo /opt/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kubectl
</li><li class="line" prefix="$">sudo chmod +x /opt/bin/kubectl
</li></ul></code></pre>
<p>For 64bit OSX clients:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">sudo curl -Lo /usr/local/bin/kubectl https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/darwin/amd64/kubectl
</li><li class="line" prefix="$">sudo chmod +x /usr/local/bin/kubectl
</li></ul></code></pre>
<p>For 64bit Windows clients (for this tutorial, tested using <a href="https://git-for-windows.github.io" rel="nofollow">git-for-windows</a>) bash:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">curl -Lo /usr/bin/kubectl.exe https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/windows/amd64/kubectl.exe
</li></ul></code></pre>
<p>Before we can use <code>kubectl</code>, we need to understand how configuration is managed within Kubernetes. This section is also important for our worker node configuration as we will use these concepts to simplify the worker setup.</p>

<h3 id="introduction-to-kubeconfig-files">Introduction to Kubeconfig files</h3>

<p>&lt;!-- Environment variables to manage the environments, who would have thought... --&gt;<br>
Several tools, Docker for example, rely on command-line flags and environment variables to configure the environment:</p>
<div class="code-label " title="docker environment variables">docker environment variables</div><pre class="code-pre "><code langs="">DOCKER_HOST=tcp://192.168.99.101:2376
DOCKER_CERT_PATH=/home/demo/.docker/machines/.client
DOCKER_TLS_VERIFY=1
DOCKER_MACHINE_NAME=dev
</code></pre>
<p>When users have to work with multiple environments which require a different configuration however, managing several environment variables to define a single configuration becomes cumbersome, even more so when the combination of clusters and users allow for many different configurations as is the case with Kubernetes.</p>

<p>Docker opted to facilitate environment management by creating the <code>docker-machine env</code> command. This tool generates the necessary shell commands allowing users to easily switch the server their client talks to. The commands generated by <code>docker-machine</code> in turn need to support each shell (bash/fish/cmd/PowerShell/..) users may be using and ideally also auto-detect the shell in use.</p>

<p>For Kubernetes, kubeconfig files were created instead to store the environment definitions such as authentication and connection details as well as provide a mechanism to easily switch between multiple clusters and multiple user credentials. Kubernetes components were written to read the configuration from these config files including functionality for merging multiple configurations based on certain rules.</p>

<p>On one side, kubeconfig files store connection information for clusters in an associative array of <code>name-&gt;cluster</code> entries. A <code>cluster</code> entry consists of information such as the <code>server</code> to connect to, the <code>api-version</code> of the cluster and the <code>certificate-authority</code> for the cluster or a flag to skip verification of the authority which signed the server certificate (<code>insecure-skip-tls-verify</code>).</p>

<p>On the other side, kubeconfig files also store user credentials in a second associative array of <code>name-&gt;user</code> entries. A <code>user</code> entry defines <code>user</code> authentication mechanisms which may be:</p>

<ol>
<li>Authentication through a client certificate,</li>
<li>Basic authentication with username and password or</li>
<li>Authentication through a bearer token</li>
</ol>

<p>Decoupling users from clusters provides the ability to define cross cluster users only once. A <code>user</code> entry and a <code>cluster</code> entry combine to make up a <code>context</code>. Several such (<code>cluster</code>, <code>user</code>) pairs are then defined in a third associative array of <code>name-&gt;context</code> entries. Context entries also provide a <code>namespace</code> field to specify the Kubernetes <code>namespace</code> to be used for that context.  The <code>current-context</code> may be set to define the context in use.</p>

<p>&lt;!-- contexts are more than a pair (2-tuple), they are a 3-tuple as they consist of user, context and namespace --&gt;</p>

<p>To declare the above components, kubeconfig files are written in YAML and similar to Pod manifests start with a versioned schema definition:</p>
<pre class="code-pre "><code langs="">apiVersion: v1
kind: Config
...
</code></pre>
<p>In the next step we will manually write out a kubeconfig file to fully understand these concepts. We will also be using the <code>kubectl</code> tool to more easily manipulate kubeconfig files, with a series of <code>kubectl config</code> subcommands. Refer to <a href="http://kubernetes.io/v1.1/docs/user-guide/kubectl/kubectl_config.html" rel="nofollow">the official Kubernetes <code>kubectl config</code> documentation</a> for full details.</p>

<p>As mentioned, kubeconfig files also define a way multiple configurations may be merged together along with override options specified from the command line. See the <a href="http://kubernetes.io/v1.1/docs/user-guide/kubeconfig-file.html#loading-and-merging-rules" rel="nofollow">loading and merging rules</a> section of the Kubernetes documentation for a technical overview of these rules. We will only define a single kubeconfig file in this tutorial.</p>

<h3 id="configure-kubectl">Configure Kubectl</h3>

<p>As we have a theoretical understanding of what a kubeconfig if made up from, we will first manually write our default kubeconfig file (<code>~/.kube/config</code>).</p>

<p>Define the Digital Ocean cluster we just created as <code>do-cluster</code>:</p>
<div class="code-label " title="~/.kube/config - snippet">~/.kube/config - snippet</div><pre class="code-pre "><code langs="">
apiVersion: v1
kind: Config
clusters:
- name: do-cluster
  cluster:
    certificate-authority: <span class="highlight">ca.pem</span>
    server: https://$CONTROLLER_PUBLIC_IP
</code></pre>
<p>[note]<br>
<strong>Note</strong>: Relative paths are supported, in this tutorial we store our certificates in the same directory as our kubeconfig file (<code>~/.kube/</code>), modify these values to mirror your own configuration.<br>
</p>

<p>Next, we define the <code>admin</code> user and specify the associated TLS assets we generated for certification based authentication:</p>
<div class="code-label " title="~/.kube/config - snippet">~/.kube/config - snippet</div><pre class="code-pre "><code langs="">...
users:
- name: admin
  user:
    client-certificate: <span class="highlight">admin.pem</span>
    client-key: <span class="highlight">admin-key.pem</span>

</code></pre>
<p>Followed by the definition of the context combining these two, which we will name the <code>do-cluster-admin</code> context:</p>
<div class="code-label " title="~/.kube/config - snippet">~/.kube/config - snippet</div><pre class="code-pre "><code langs="">...
contexts:
- name: do-cluster-admin
  context:
    cluster: do-cluster
    user: admin
</code></pre>
<p>As we did not specify a <code>namespace</code> for our context, the <code>default</code> namespace will be used.</p>

<p>We may set this as our current context in our kubeconfig file by adding the <code>current-context: do-cluster-admin</code> setting at the end. </p>

<p>Using the <code>cat</code> command to combine all the above snippets with a here-string for variable substitution, we write out the file as follows:</p>
<pre class="code-pre "><code langs="">cat &gt; ~/.kube/config&lt;&lt;EOF
apiVersion: v1
kind: Config
clusters:
- name: do-cluster
  cluster:
    certificate-authority: ca.pem
    server: https://$CONTROLLER_PUBLIC_IP
users:
- name: admin
  user:
    client-certificate: admin.pem
    client-key: admin-key.pem
contexts:
- name: do-cluster-admin
  context:
    cluster: do-cluster
    user: admin
current-context: do-cluster-admin
EOF
</code></pre>
<p>If the kubeconfig file is not passed in to <code>kubectl</code> through the <code>--kubeconfig</code> flag, it will first look for a <code>kubeconfig</code> file in the current directory as well as the <code>$KUBECONFIG</code> environment variable. If none of these are set, <code>kubectl</code> will use the default <code>~/.kube/config</code> file we just created.</p>

<p>We may also generate the above file using kubectl with the following 4 commands:</p>

<p>Set the "do-cluster" entry:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl config set-cluster do-cluster --server=https://$CONTROLLER_PUBLIC_IP --certificate-authority=$HOME/.kube/ca.pem
</li></ul></code></pre>
<p>Set the "admin" user entry:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl config set-credentials admin --client-key=$HOME/.kube/admin-key.pem --client-certificate=$HOME/.kube/admin.pem
</li></ul></code></pre>
<p>Set the "do-cluster-admin" context:</p>
<pre class="code-pre "><code langs="">kubectl config set-context do-cluster-admin --cluster=do-cluster --user=admin
</code></pre>
<p>Set the <code>current-context</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl config use-context do-cluster-admin
</li></ul></code></pre>
<p>Confirm your configuration was successful with the following command:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl version
</li></ul></code></pre>
<p>If everything worked so far, this should return output similar to:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Client Version: version.Info{Major:"1", Minor:"1", GitVersion:"v1.1.2", GitCommit:"3085895b8a70a3d985e9320a098e74f545546171", GitTreeState:"clean"}
Server Version: version.Info{Major:"1", Minor:"1", GitVersion:"v1.1.2", GitCommit:"3085895b8a70a3d985e9320a098e74f545546171", GitTreeState:"clean"}
</code></pre>
<p>We may confirm the pods running in the kube-system namespace with the following command</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl get pods --namespace=kube-system
</li></ul></code></pre>
<p>Expected output looks like:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>NAME                                      READY     STATUS    RESTARTS   AGE
kube-controller-188.166.252.4             3/3       Running   0          3h
kube-controller-manager-188.166.252.4     1/1       Running   0          3h
kube-scheduler-188.166.252.4              1/1       Running   0          3h
</code></pre>
<p>Indicating all 3 containers (<code>kube-apiserver</code>, <code>scheduler-elector</code> &amp; <code>controller-manager-elector</code>) of the <code>kube-controller</code> pod as well as the <code>kube-controller-manager</code> and <code>kube-scheduler</code> pods are running.</p>

<p>We now have our Etcd data store and first controller droplet ready, but we do not have any worker nodes yet to schedule workloads on. </p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl get nodes
</li></ul></code></pre>
<p>Returns an empty collection of worker nodes. We will spin up our first worker node next.</p>

<h2 id="step-9-—-provisioning-the-kubernetes-worker-droplets">Step 9 — Provisioning The Kubernetes Worker Droplets</h2>

<p>Configuring the workers is significantly less complicated and we can reuse many of the controllers configuration.</p>

<p>Ensure your <code>DIGITALOCEAN_API_KEY</code>, <code>ETCD_PEER</code>, <code>CONTROLLER_PUBLIC_IP</code>, <code>CONTROLLER_PRIVATE_IP</code> and <code>region</code> environment variables are set for the next steps.</p>

<h3 id="the-etcd-flannel-amp-docker-services">The Etcd, Flannel &amp; Docker services</h3>

<p>As mentioned in the Etcd configuration section, we are running an Etcd daemon in proxy mode on each droplet for Flannel to access. As we are sharing our Etcd cluster between Flannel and Kubernetes we should note that exposing your Kubernetes data back end to each node is a bad practice. For production environments we should use a separate Etcd cluster to store the Flannel meta data configuration. If Flannel was not used to configure the overlay network, Etcd access would not be needed on the worker nodes at all.</p>

<p>Our cloud-config section for Etcd proxy daemon configuration as we saw before looks like this:</p>
<div class="code-label " title="cloud-config-worker.yaml - etcd proxy snippet">cloud-config-worker.yaml - etcd proxy snippet</div><pre class="code-pre "><code langs="">
coreos:
  etcd2:
    proxy: on 
    listen-client-urls: http://localhost:2379
    initial-cluster: "etcd-01=ETCD_PEER"
  units:
    - name: "etcd2.service"
      command: "start"
</code></pre>
<p>We need to ensure Flannel starts and add a Drop-in for Docker to depend on flannel. (Flannel will use localhost to get it's network configuration)</p>
<pre class="code-pre "><code langs="">[label cloud-config-worker.yaml - flannel snippet

coreos:
  units:
    - name: "flanneld.service"
      command: "start"
    - name: "docker.service"
      command: "start"
      drop-ins:
        - name: 40-flannel.conf
          content: |
            [Unit]
            Requires=flanneld.service
            After=flanneld.service
</code></pre>
<h3 id="the-kubelet-service">The kubelet service</h3>

<p>In order to facilitate secure communication between Kubernetes components, <code>kubeconfig</code> can also be used to define authentication settings for the kubelet. In this case, the kubelet and proxy are reading this configuration to communicate with the API. Refer back to the [Introduction to kubeconfig files](#) section of this tutorial for a detailed explanation of the kubeconfig specification.</p>

<p>Very similar to our previous kubeconfig file, we define a single cluster, in this case called "local" with a <code>certificate-authority</code> path. and a single user called "kubelet" with certificate based authentication through a worker private key and signed certificate. The combination of this <code>user</code> and <code>cluster</code> is defined as the "kubelet-context" and set as the <code>current-context</code>.</p>
<div class="code-label " title="kubelet-kubeconfig">kubelet-kubeconfig</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Config
</li><li class="line" prefix="4">clusters:
</li><li class="line" prefix="5">- name: local
</li><li class="line" prefix="6">  cluster:
</li><li class="line" prefix="7">    certificate-authority: /etc/kubernetes/ssl/ca.pem
</li><li class="line" prefix="8">users:
</li><li class="line" prefix="9">- name: kubelet
</li><li class="line" prefix="10">  user:
</li><li class="line" prefix="11">    client-certificate: /etc/kubernetes/ssl/worker.pem
</li><li class="line" prefix="12">    client-key: /etc/kubernetes/ssl/worker-key.pem
</li><li class="line" prefix="13">contexts:
</li><li class="line" prefix="14">- name: kubelet-context
</li><li class="line" prefix="15">  context:
</li><li class="line" prefix="16">    cluster: local
</li><li class="line" prefix="17">    user: kubelet
</li><li class="line" prefix="18">current-context: kubelet-context
</li></ul></code></pre>
<p>Our kubelet parameters are</p>
<pre class="code-pre "><code langs="">kubelet \
  --api-servers=https://CONTROLLER_PUBLIC_IP \
  --register-node=true \
  --allow-privileged=true \
  --config=/etc/kubernetes/manifests \
  --hostname-override=$public_ipv4 \
  --cluster-dns=10.3.0.10 \
  --cluster-domain=cluster.local \
  --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
</code></pre>
<p>The <code>--api-servers</code> flag points to the https protocol to use port 443 and we use a placeholder for our <code>CONTROLLER_PUBLIC_IP</code> to make a generic cloud-config file which we can re-use for creating multiple clusters. We pass in the kubeconfig we described above with the <code>--kubeconfig</code> flag. Our worker nodes are registered to receive work by specifying the <code>--register-node=true</code> flag. We still configure our Kubelet to monitor a local directory for Pod manifests, although we will not be using this at this point. The remaining parameters are identical to the controller Droplets configuration.</p>

<p>Similar to our controller setup, we define a Systemd unit to wait for the Worker TLS assets to be in place and require the <code>kube-kubelet</code> service to depend on this unit.</p>
<div class="code-label " title="cloud-config-worker.yaml - tls ready snippet">cloud-config-worker.yaml - tls ready snippet</div><pre class="code-pre "><code langs="">
coreos:
  units:
    - name: "tls-ready.service"
      command: "start"
      content: |
        [Unit]
        Description=Ensure TLS assets are ready
        Requires=kube-kubelet.service
        After=kube-kubelet.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStartPre=-/usr/bin/mkdir -p /etc/kubernetes/ssl
        ExecStart=/bin/bash -c "until [ `ls -1 /etc/kubernetes/ssl/{worker,worker-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \"waiting for TLS assets...\";sleep 5; done"
</code></pre>
<p>Add the <code>tls-ready.service</code> dependency to the <code>kube-kubelet</code> and <code>kube-proxy</code> services</p>
<div class="code-label " title="cloud-config-worker.yaml - kubelet snippet">cloud-config-worker.yaml - kubelet snippet</div><pre class="code-pre "><code langs="">
coreos:
  units:
    - name: "kube-kubelet.service"
      command: "start"
      content: |
        [Unit]
        Description=Kubernetes Kubelet
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=docker.service tls-ready.service
        After=docker.service tls-ready.service
        [Service]
        ExecStartPre=-/bin/bash -c "mkdir -p /etc/kubernetes/{manifests,ssl}"
        ExecStart=/usr/bin/kubelet \
        --api-servers=https://CONTROLLER_PUBLIC_IP \
        --register-node=true \
        --allow-privileged=true \
        --config=/etc/kubernetes/manifests \
        --hostname-override=$public_ipv4 \
        --cluster-dns=10.3.0.10 \
        --cluster-domain=cluster.local \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml 
        Restart=always
        RestartSec=10

</code></pre>
<h3 id="the-kube-proxy-service">The kube-proxy Service</h3>
<div class="code-label " title="cloud-config-worker.yaml - kube-proxy snippet">cloud-config-worker.yaml - kube-proxy snippet</div><pre class="code-pre "><code langs="">
coreos:
  units:
    - name: "kube-proxy.service"
      command: "start"
      content: |
        [Unit]
        Description=Kubernetes Proxy
        Documentation=https://github.com/kubernetes/kubernetes
        Requires=network-online.target tls-ready.service
        After=network-online.target tls-ready.service
        [Service]
        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
        ExecStart=/opt/bin/kube-proxy \
        --master=https://CONTROLLER_PUBLIC_IP \
        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
        --proxy-mode=iptables \
        --hostname-override=$public_ipv4
        Restart=always
        RestartSec=10
</code></pre>
<h3 id="the-final-worker-cloud-config-with-all-coreos-units">The Final Worker cloud-config with all CoreOS Units</h3>

<ol>
<li><code>etcd2.service</code> snippet to start a local Etcd proxy, notice the <code>ETCD_PEER</code> placeholder.</li>
<li><code>flanneld.service</code> snippet to start the overlay network daemon with a drop-in to configure the network subnet</li>
<li><code>docker.service</code> drop-in snippet to add flannel dependency</li>
<li><code>tls-ready.service</code> to block other units until the TLS assets for the worker have been put in place</li>
<li><code>kubelet.service</code> snippet running the kubelet to register with our controller nodes</li>
<li><code>kube-proxy.service</code> snippet running the <code>kube-proxy</code> service</li>
</ol>
<div class="code-label " title="cloud-config-worker.yaml">cloud-config-worker.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">#cloud-config
</li><li class="line" prefix="3">
</li><li class="line" prefix="4">write-files:
</li><li class="line" prefix="5">  - path: /etc/kubernetes/worker-kubeconfig.yaml
</li><li class="line" prefix="6">    permissions: '0644'
</li><li class="line" prefix="7">    content: |
</li><li class="line" prefix="8">      apiVersion: v1
</li><li class="line" prefix="9">      kind: Config
</li><li class="line" prefix="10">      clusters:
</li><li class="line" prefix="11">      - name: local
</li><li class="line" prefix="12">        cluster:
</li><li class="line" prefix="13">          certificate-authority: /etc/kubernetes/ssl/ca.pem
</li><li class="line" prefix="14">      users:
</li><li class="line" prefix="15">      - name: kubelet
</li><li class="line" prefix="16">        user:
</li><li class="line" prefix="17">          client-certificate: /etc/kubernetes/ssl/worker.pem
</li><li class="line" prefix="18">          client-key: /etc/kubernetes/ssl/worker-key.pem
</li><li class="line" prefix="19">      contexts:
</li><li class="line" prefix="20">      - name: kubelet-context
</li><li class="line" prefix="21">        context:
</li><li class="line" prefix="22">          cluster: local
</li><li class="line" prefix="23">          user: kubelet
</li><li class="line" prefix="24">      current-context: kubelet-context
</li><li class="line" prefix="25">coreos:
</li><li class="line" prefix="26">  etcd2:
</li><li class="line" prefix="27">    proxy: on 
</li><li class="line" prefix="28">    listen-client-urls: http://localhost:2379
</li><li class="line" prefix="29">    initial-cluster: "etcd-01=ETCD_PEER"
</li><li class="line" prefix="30">  units:
</li><li class="line" prefix="31">    - name: "etcd2.service"
</li><li class="line" prefix="32">      command: "start"
</li><li class="line" prefix="33">    - name: "flanneld.service"
</li><li class="line" prefix="34">      command: "start"
</li><li class="line" prefix="35">    - name: "docker.service"
</li><li class="line" prefix="36">      command: "start"
</li><li class="line" prefix="37">      drop-ins:
</li><li class="line" prefix="38">        - name: 40-flannel.conf
</li><li class="line" prefix="39">          content: |
</li><li class="line" prefix="40">            [Unit]
</li><li class="line" prefix="41">            Requires=flanneld.service
</li><li class="line" prefix="42">            After=flanneld.service
</li><li class="line" prefix="43">    - name: "tls-ready.service"
</li><li class="line" prefix="44">      command: "start"
</li><li class="line" prefix="45">      content: |
</li><li class="line" prefix="46">        [Unit]
</li><li class="line" prefix="47">        Description=Ensure TLS assets are ready
</li><li class="line" prefix="48">        Requires=docker.service
</li><li class="line" prefix="49">        After=docker.service
</li><li class="line" prefix="50">        [Service]
</li><li class="line" prefix="51">        Type=oneshot
</li><li class="line" prefix="52">        RemainAfterExit=yes
</li><li class="line" prefix="53">        ExecStartPre=-/usr/bin/mkdir -p /etc/kubernetes/ssl
</li><li class="line" prefix="54">        ExecStart=/bin/bash -c "until [ `ls -1 /etc/kubernetes/ssl/{worker,worker-key,ca}.pem 2&gt;/dev/null | wc -l` -eq 3 ]; do echo \"waiting for TLS assets...\";sleep 5; done"
</li><li class="line" prefix="55">    - name: "kube-proxy.service"
</li><li class="line" prefix="56">      command: "start"
</li><li class="line" prefix="57">      content: |
</li><li class="line" prefix="58">        [Unit]
</li><li class="line" prefix="59">        Description=Kubernetes Proxy
</li><li class="line" prefix="60">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="61">        Requires=network-online.target tls-ready.service
</li><li class="line" prefix="62">        After=network-online.target tls-ready.service
</li><li class="line" prefix="63">        [Service]
</li><li class="line" prefix="64">        ExecStartPre=-/usr/bin/mkdir -p /opt/bin
</li><li class="line" prefix="65">        ExecStartPre=/usr/bin/curl -sLo /opt/bin/kube-proxy -z /opt/bin/kube-proxy https://storage.googleapis.com/kubernetes-release/release/v1.1.2/bin/linux/amd64/kube-proxy
</li><li class="line" prefix="66">        ExecStartPre=/usr/bin/chmod +x /opt/bin/kube-proxy
</li><li class="line" prefix="67">        ExecStart=/opt/bin/kube-proxy \
</li><li class="line" prefix="68">        --master=https://CONTROLLER_PUBLIC_IP \
</li><li class="line" prefix="69">        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml \
</li><li class="line" prefix="70">        --proxy-mode=iptables \
</li><li class="line" prefix="71">        --hostname-override=$public_ipv4
</li><li class="line" prefix="72">        Restart=always
</li><li class="line" prefix="73">        RestartSec=10
</li><li class="line" prefix="74">    - name: "kube-kubelet.service"
</li><li class="line" prefix="75">      command: "start"
</li><li class="line" prefix="76">      content: |
</li><li class="line" prefix="77">        [Unit]
</li><li class="line" prefix="78">        Description=Kubernetes Kubelet
</li><li class="line" prefix="79">        Documentation=https://github.com/kubernetes/kubernetes
</li><li class="line" prefix="80">        Requires=docker.service tls-ready.service
</li><li class="line" prefix="81">        After=docker.service tls-ready.service
</li><li class="line" prefix="82">        [Service]
</li><li class="line" prefix="83">        ExecStart=/usr/bin/kubelet \
</li><li class="line" prefix="84">        --api-servers=https://CONTROLLER_PUBLIC_IP \
</li><li class="line" prefix="85">        --register-node=true \
</li><li class="line" prefix="86">        --hostname-override=$public_ipv4 \
</li><li class="line" prefix="87">        --cluster-dns=10.3.0.10 \
</li><li class="line" prefix="88">        --cluster-domain=cluster.local \
</li><li class="line" prefix="89">        --kubeconfig=/etc/kubernetes/worker-kubeconfig.yaml
</li><li class="line" prefix="90">        Restart=always
</li><li class="line" prefix="91">        RestartSec=10
</li></ul></code></pre>
<p>Use the above template to generate the worker node cloud config for this Digital Ocean cluster:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">sed -e "s|ETCD_PEER|${ETCD_PEER}|g;s|CONTROLLER_PUBLIC_IP|${CONTROLLER_PUBLIC_IP}|g;" cloud-config-worker.yaml &gt; kube-worker.yaml
</li></ul></code></pre>
<p>And send the command to create the Droplet</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d c --wait-for-active \
</li><li class="line" prefix="$">    -i "CoreOS-alpha" \
</li><li class="line" prefix="$">    -s 512mb \
</li><li class="line" prefix="$">    -r "$region" \
</li><li class="line" prefix="$">    -p \
</li><li class="line" prefix="$">    -k k8s-key \
</li><li class="line" prefix="$">    -uf kube-worker.yaml kube-worker-01
</li></ul></code></pre>
<p><strong>Note</strong>: running <code>free -m</code> after freshly starting all Kubernetes services on a <code>512mb</code> Worker Droplet shows 129mb free, consider using 1024mb Droplets</p>

<p>We refresh the Droplet configuration and cache the json string returned in the $WORKER_JSON environment variable for subsequent commands.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">WORKER_JSON=`doctl -f 'json' d f kube-worker-01.$region`
</li><li class="line" prefix="$">
</li></ul></code></pre>
<p>We parse the private and public IPs out as explained in the [Working with doctl responses](#) section of this tutorial.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">WORKER_PUBLIC_IP=`echo $WORKER_JSON | jq -r '.networks.v4[] | select(.type == "public") | .ip_address'`
</li><li class="line" prefix="$">WORKER_PRIVATE_IP=`echo $WORKER_JSON | jq -r '.networks.v4[] | select(.type == "private") | .ip_address'`
</li></ul></code></pre>
<p>Confirm</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">echo $WORKER_PUBLIC_IP &amp;&amp; echo $WORKER_PRIVATE_IP
</li></ul></code></pre>
<p><strong>Troubleshooting</strong>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@$WORKER_PUBLIC_IP
</li></ul></code></pre><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">journalctl -u oem-cloudinit -f
</li></ul></code></pre><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">watch -n 1 'docker ps --format="table {{.Image}}\t{{.ID}}\t{{.Status}}\t{{.Ports}}" -a' 
</li></ul></code></pre>
<h3 id="generating-and-transferring-the-worker-tls-assets">Generating and Transferring the Worker TLS assets</h3>

<p>As it is recommended to generate a unique certificate per worker, we will do so and transfer it to our worker droplet now.</p>

<p>The IP addresses and fully qualified hostnames of all worker nodes will be needed. The certificates generated for the worker nodes will need to reflect how requests will be routed to those nodes. In most cases this will be a routable IP and/or a routable hostname. These will be unique per worker; when you see them used below, consider it a loop and do that step for each worker.</p>

<p>This procedure generates a unique TLS certificate for every Kubernetes worker node in your cluster. While unique certificates are less convenient to generate and deploy, they do provide stronger security assurances and the most portable installation experience across multiple cloud-based and on-premises Kubernetes deployments.</p>

<p>We will use a common openssl configuration file for all workers. The certificate output will be customized per worker based on environment variables used in conjunction with the configuration file. Create the file worker-openssl.cnf on your local machine with the following contents.</p>
<div class="code-label " title="~/.kube/worker-openssl.cnf">~/.kube/worker-openssl.cnf</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">[req]
</li><li class="line" prefix="3">req_extensions = v3_req
</li><li class="line" prefix="4">distinguished_name = req_distinguished_name
</li><li class="line" prefix="5">[req_distinguished_name]
</li><li class="line" prefix="6">[ v3_req ]
</li><li class="line" prefix="7">basicConstraints = CA:FALSE
</li><li class="line" prefix="8">keyUsage = nonRepudiation, digitalSignature, keyEncipherment
</li><li class="line" prefix="9">subjectAltName = <a href="https://www.digitalocean.com/community/users/alt_names" class="username-tag">@alt_names</a>
</li><li class="line" prefix="10">[alt_names]
</li><li class="line" prefix="11">IP.1 = $ENV::WORKER_IP
</li></ul></code></pre>
<p>Generate the private key for our first Worker Droplet</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">openssl genrsa -out ~/.kube/worker-01-key.pem 2048
</li></ul></code></pre>
<p>Generate the Certificate Signing Request, substituting the WORKER_IP environment variable:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">WORKER_IP=${WORKER_PRIVATE_IP} openssl req -new -key ~/.kube/worker-01-key.pem -out worker-01.csr -subj "/CN=kube-worker-01" -config worker-openssl.cnf
</li></ul></code></pre>
<p>Generate the worker certificate</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">WORKER_IP=${WORKER_PRIVATE_IP} openssl x509 -req -in worker-01.csr \
</li><li class="line" prefix="$"> -CA "$HOME/.kube/ca.pem" \
</li><li class="line" prefix="$"> -CAkey "$HOME/.kube/ca-key.pem" \
</li><li class="line" prefix="$"> -CAcreateserial \
</li><li class="line" prefix="$"> -out "$HOME/.kube/worker-01.pem" \
</li><li class="line" prefix="$"> -days 365 \
</li><li class="line" prefix="$"> -extensions v3_req \
</li><li class="line" prefix="$"> -extfile worker-openssl.cnf
</li></ul></code></pre>
<p>&lt;!-- </p>

<p>We no longer need the csr</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">rm worker-01.csr
</li></ul></code></pre>
<p>--&gt;<br>
[note]<br>
<strong>Note</strong>: the above command does not work on <code>git-for-windows</code> due to windows path conversions, it is recommended to copy the <code>worker-01.csr</code> and <code>worker-openssl.cnf</code> to <code>~/.kube/</code> and just run the command from within the <code>~/.kube/</code> directory (without the <code>"$HOME/.kube/"</code> parts)<br>
</p>

<p>Copy the necessary certificates to the controller node. We store the files in the home directory first.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">scp ~/.kube/worker-01-key.pem ~/.kube/worker-01.pem ~/.kube/ca.pem core@$WORKER_PUBLIC_IP:~
</li></ul></code></pre>
<p>Move the certificates from the Home directory to the /etc/kubernetes/ssl path, fix the permissions and create links to match our generic kubeconfig (which expects <code>/etc/kubernetes/worker-key.pem</code> instead of <code>/etc/kubernetes/worker-01-key.pem</code>) by executing the following commands over ssh:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">ssh core@$WORKER_PUBLIC_IP &lt;&lt;EOF
</li><li class="line" prefix="$">sudo mkdir -p /etc/kubernetes/ssl/
</li><li class="line" prefix="$">sudo mv ~core/*.pem /etc/kubernetes/ssl/
</li><li class="line" prefix="$">sudo chown root:root /etc/kubernetes/ssl/*.pem
</li><li class="line" prefix="$">sudo chmod 600 /etc/kubernetes/ssl/*-key.pem
</li><li class="line" prefix="$">sudo ln -s /etc/kubernetes/ssl/worker-01.pem /etc/kubernetes/ssl/worker.pem
</li><li class="line" prefix="$">sudo ln -s /etc/kubernetes/ssl/worker-01-key.pem /etc/kubernetes/ssl/worker-key.pem
</li><li class="line" prefix="$">EOF
</li></ul></code></pre>
<p>As soon as the certificates are available it will take just a few minutes for the kubelet and kube-proxy to start running on the worker and register with the Controller.</p>

<p>We can verify by watching the <code>kubectl get nodes</code>:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl get nodes
</li></ul></code></pre>
<p>Which should show output as follows:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>NAME              LABELS                                   STATUS    AGE
128.199.203.205   kubernetes.io/hostname=128.199.203.205   Ready     9m
</code></pre>
<p>We may repeat the above steps to create additional Worker droplets with their own TLS assets. We now have a working Kubernetes cluster, ready to start running our containerized applications. To facilitate the application deployment however, we are recommended to run a few cluster services and will proceed to do so in the next step.</p>

<h2 id="step-10-—-running-kubernetes-cluster-services">Step 10 — Running Kubernetes Cluster Services</h2>

<p>Several cluster services are provided as cluster add-ons (UI/Dashboard, Image Registry, DNS, ...). Deploying these add-ons is optional, but availability of some of these services is often expected by Kubernetes users. A full listing of all supported add-ons can be found within the Kubernetes GitHub repository at <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons" rel="nofollow">kubernetes/cluster/addons/</a>. </p>

<p>Add-ons are built on the same Kubernetes components as user-submitted jobs — Pods, Replication Controllers and Services, however, cluster add-ons are expected to specify the label: <code>kubernetes.io/cluster-service: "true"</code>.</p>

<p>One such cluster add-on facilitates the discovery of services running within Kubernetes, we will first define the problem and the options Kubernetes provides to solve this problem.</p>

<p>When Pods depend on each other (for example: front end services may depend on back end services), mechanisms need to be in place to enable service discovery. Within Kubernetes, Pods are short lived objects and their IPs change over time due to crashes or scheduling changes. Because of this, addressing Pods directly has now become difficult, thus Kubernetes introduced the concept of Service objects to address this problem. Service objects are long lived objects which get a static Virtual IP within the cluster, usually referred to as their <code>clusterIP</code>, to address sets of Pods internally or externally to the cluster. This <code>clusterIP</code> is stable as long as the Service object exists. Kubernetes sets up a load balancer forwarding traffic through this <code>clusterIP</code> to the Service EndPoints, unless you explicitly disable the load balancer (by setting <code>clusterIP</code> to <code>none</code>) and expect to work with a list of the Service EndPoints directly. Such Services without a <code>clusterIP</code> are called Headless. Service objects may also be created for services running outside of the Kubernetes cluster (by omitting the Pod selector) as long as you manually create the EndPoint definitions for these external services. Full details on how to do this are available within the <a href="http://kubernetes.io/v1.1/docs/user-guide/services.html" rel="nofollow">official Kubernetes documentation</a>.</p>

<p>Once Service objects have been defined, Kubernetes provides 2 ways of finding them:</p>

<ol>
<li>Through Environment variables, or</li>
<li>Using DNS</li>
</ol>

<p>Upon Pod creation, the kubelet adds a set of environment variables for each active Service within the same namespace, similar to how Docker links worked. These environment variables enforce an ordering requirement as any Service that a Pod wants to access must be created before the Pod itself and may require applications to be modified before they can run within Kubernetes. If we use DNS to discover services, we do not have these restrictions, but we are required to deploy the DNS cluster add-on.</p>

<p>As part of this tutorial we ensure our Kubernetes cluster is integrated with DNS for Service discovery by deploying the DNS add-on with <code>kubectl</code>. </p>

<h3 id="dns-integration-with-kubernetes">DNS Integration with Kubernetes</h3>

<p>When enabled, the DNS add-on for Kubernetes will assign a DNS name for every Service object defined in the Kubernetes cluster.</p>

<p>At the time of writing, the DNS protocol implementation for the DNS add-on is provided by <strong>SkyDNS</strong>. SkyDNS is configured as a slave to the Kubernetes API Server with custom logic implemented in a bridge component called <strong>Kube2sky</strong>. SkyDNS itself is only a thin layer over <strong>Etcd</strong> to translate Etcd keys and values to the DNS protocol. In this way, SkyDNS can be as highly available and stable as the underlying Etcd cluster. We will have a closer look at how each of these 3 components work together and how we will deploy them as a single Pod.</p>

<p>&lt;!--<br>
Services announce their availability by sending a POST with a small JSON payload, Each service has a Time To Live that allows SkyDNS to expire the records for the services that haven't update their availability within the TTL window. Services can send a periodic POST to SkyDNS updating their TTL to keep them in the pool.</p>

<p>--&gt;</p>

<p>We will create a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_replicationcontroller" rel="nofollow">Replication Controller</a> to run the DNS Pod and a <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_service" rel="nofollow">Service</a> to expose its ports. </p>

<p>Our Replication Controller manifest starts, as we saw earlier, with the schema definition and a <code>metadata</code> section:</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v9
  namespace: kube-system
  ...
</code></pre>
<p>&lt;!-- use labels effectively: http://kubernetes.io/v1.1/docs/user-guide/managing-deployments.html#using-labels-effectively --&gt;</p>

<p>A Replication Controller can be thought of as a process supervisor, but which supervises multiple Pods across multiple nodes instead of individual processes on a single node. The Replication Controller creates Pods from a template and uses labels and selectors to monitor the actual running Pods. The selector finds Pods within the cluster by label, the labels we'll use for this Replication Controller are the <code>k8s-app</code> and <code>version</code> labels. We specify these labels together with the <code>kubernetes.io/cluster-service: "true"</code> label required for cluster add-ons in the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podtemplatespec" rel="nofollow">PodTemplateSpec</a> as well as attach them to our Replication Controller itself.</p>

<p>At the time of writing we are using version 9 and refer to the DNS add-on as the <code>kube-dns</code> app. By default Replication Controllers will run 1 replica, but we explicitly set the <code>replicas</code> field to 1 for clarity in our <code>spec</code>. This looks as follows in our manifest file:</p>

<p>&lt;!--<br>
<strong>note:</strong> <a href="https://github.com/kubernetes/kubernetes/commit/5abfce45e1fb0cb9bf3b643f0ee53b812e6f83b0" rel="nofollow">update to v10?</a><br>
--&gt;</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">apiVersion: v1
kind: ReplicationController
metadata:
  name: kube-dns-v9
  namespace: kube-system
  <span class="highlight">labels</span>:
    k8s-app: kube-dns
    version: v9
    kubernetes.io/cluster-service: "true"
spec:
  replicas: 1
  <span class="highlight">selector</span>:
    k8s-app: kube-dns
    version: v9
  template:
    metadata:
      <span class="highlight">labels</span>:
        k8s-app: kube-dns
        version: v9
        kubernetes.io/cluster-service: "true"
    spec:
      volumes:
  ...
</code></pre>
<p>We added the labels to the Replication Controller object in its <code>metadata</code> field. In the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_replicationcontrollerspec" rel="nofollow">ReplicationControllerSpec</a> we set the labels for the Pod <code>template.metadata</code> and also set the <code>replicas</code> and <code>selector</code> values. Let's look at the volumes and containers defined in the <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_podtemplatespec" rel="nofollow">PodTemplateSpec</a> next.</p>

<p>We will only define a volume for Etcd.  Giving Etcd a volume, outside of the union filesystem used by container runtimes such as Docker, will ensure optimal performance by reducing filesystem overhead. As the data is just a scratch space and it's fine to lose the data when the Pod is rescheduled on a different Node, it is sufficient to use an <a href="http://kubernetes.io/v1.1/docs/user-guide/volumes.html#emptydir" rel="nofollow">EmptyDir</a>-type volume:</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">...
      volumes:
      - name: etcd-storage
        emptyDir: {}
...
</code></pre>
<p>Let's look at the actual definitions of the containers in the Pod template. We see a container for each of the 3 components described earlier as well as an ExecHealthz sidecar container:</p>

<ol>
<li>Etcd - the storage for SkyDNS</li>
<li>Kube2sky - the glue between SkyDNS and Kubernetes</li>
<li>SkyDNS - the DNS server</li>
<li>ExecHealthz - sidecar container for health monitoring, see details below.</li>
</ol>

<p>The Etcd instance used by the DNS add-on is best ran separately from the Etcd cluster used by the Kubernetes API Services. For simplicity we run Etcd within the same Pod as our SkyDNS and Kube2sky components. This is sufficient considering the DNS add-on only requires a small subset of everything Etcd has to offer.</p>

<p>For the Etcd container we will use the busybox-based image available on the Google container registry, refer to the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/images/etcd" rel="nofollow">kubernetes/cluster/images/etcd</a> repository on GitHub to see the full details of how that image is made.</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">...
      - name: etcd
        image: <span class="highlight">gcr.io/google_containers/etcd:2.0.9</span>
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        command:
        - /usr/local/bin/etcd
        - -data-dir
        - /var/etcd/data
        - -listen-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -advertise-client-urls
        - http://127.0.0.1:2379,http://127.0.0.1:4001
        - -initial-cluster-token
        - skydns-etcd
        volumeMounts:
        - name: etcd-storage
          mountPath: /var/etcd/data
...
</code></pre>
<p>We run this container with the <code>etcd-storage</code> volume mounted and used as the Etcd <code>data-dir</code>. We configure Etcd to listen on localhost for connections on both the IANA-assigned <code>2379</code> port as well as the legacy <code>4001</code> port, this is required for Kube2sky and SkyDNS which still connect to port <code>4001</code> by default.</p>

<p>This spec also applies resource limits which define an upper bound on the maximum amount of resources that will be made available to this container. Resource limits are crucial to enable the scheduling components within Kubernetes to be effective. Without a definition of the required resources, schedulers can do little more than round robin assignments. The CPU resource is defined in Compute Units per second (KCU) and in this case the unit is milli-KCUs, where 1 KCU will roughly be equivalent to a single CPU hyperthreaded core for some recent x86 processor. The memory resource is defined in bytes. For a full overview of Resource management within Kubernetes, refer to <a href="http://kubernetes.io/v1.1/docs/design/resources.html#resource-specifications" rel="nofollow">the official Kubernetes resource guidelines</a>.</p>

<p>The Kube2sky container uses another Busybox based-image made available on the Google Container Registry, refer to the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns/kube2sky" rel="nofollow">kubernetes/cluster/addons/dns/kube2sky</a> repository on GitHub to see the source for that image.</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">...
      - name: kube2sky
        image: <span class="highlight">gcr.io/google_containers/kube2sky:1.11</span>
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -domain=<span class="highlight">cluster.local</span>
...
</code></pre>
<p>The Kube2sky Docker image has the <code>Entrypoint</code> set to <code>/kube2sky</code>, thus we only need to pass on the <code>-domain</code> under which we want all DNS names to be hosted through the <code>args</code> array. This should match our kubelet configuration which we set to <code>cluster.local</code> in this tutorial, modify this value to mirror your own configuration.</p>

<p>Kube2sky discovers and authenticates with the Kubernetes API Service through environment variables provisioned and secrets mounted by the kubelet into the container, we will have a closer look at these in [Step 11 — Deploying Kubernetes-ready applications](#). Once authenticated and connected, Kube2sky watches the Kubernetes API Service for changes in Service objects and publishes those changes to Etcd for SkyDNS. SkyDNS supports A and AAAA records to handle "legacy" services. With A/AAAA records the port number must be known by the client connection because that information is not in the returned records. Given we defined our cluster domain as <code>cluster.local</code>, the keys created by Kube2sky and served by SkyDNS will have the following DNS naming scheme:</p>
<div class="code-label " title="A Records naming scheme">A Records naming scheme</div><pre class="code-pre "><code langs="">&lt;service_name&gt;.&lt;namespace_name&gt;.svc.<span class="highlight">cluster.local</span>
</code></pre>
<p>For example: for a Service called "my-service" in the "default" namespace, an A record for <code>my-service.default.svc.cluster.local</code> is created. Other Pods within the same <code>default</code> namespace should be able to find the service simply by doing a name lookup for <code>my-service</code>, Pods which exist in other namespaces must use the fully qualified name.</p>

<p>For Service objects which define named ports, Kube2sky ensures SRV records are created with the following naming scheme:</p>
<div class="code-label " title="named ports SRV records naming scheme">named ports SRV records naming scheme</div><pre class="code-pre "><code langs="">_&lt;port_name&gt;._&lt;port_protocol&gt;.&lt;service_name&gt;.&lt;namespace_name&gt;.svc.<span class="highlight">cluster.local</span>
</code></pre>
<p>For example, If the Service called "my-service" in the "default" namespace has a port named "http" with a protocol of TCP, you can do a DNS SRV query for "<em>http.</em>tcp.my-service.default.svc.cluster.local" to discover the port number for "http". </p>

<p>We will confirm the above DNS records are served correctly after we have deployed the DNS add-on to our cluster.</p>

<p>The <a href="https://hub.docker.com/r/skynetservices/skydns/" rel="nofollow">skynetservices/skydns</a> image based on Alpine Linux is available on the Docker Hub at about <a href="https://imagelayers.io/?images=skynetservices%2Fskydns:2.5.3a" rel="nofollow">19MB</a> and comes with <code>dig</code>. The official <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns/skydns" rel="nofollow">kubernetes/cluster/addons/dns/skydns</a> add-on uses a busybox based image at about 41MB without <code>dig</code>. The discussion as to which image should be used in the long run can be followed on <a href="https://github.com/kubernetes/kubernetes/issues/10386" rel="nofollow">GitHub</a>. In this tutorial we opt to use the <code>skynetservices/skydns</code> image as the version tags are slightly more intuitive:</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">...
      - name: skydns
        image: skynetservices/skydns:2.5.3a
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -machines=http://localhost:4001
        - -addr=0.0.0.0:53
        - -domain=cluster.local.
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
...
</code></pre>
<p>The EntryPoint for both images is <code>/skydns/</code>, thus we only need to pass in 3 arguments. We point SkyDNS to the Etcd instance running within the Pod through the <code>-machines</code> flag. We define the address we want SkyDNS to bind to through the <code>-addr</code> flag and we specify the domain we want SkyDNS to serve records within through the <code>-domain</code> flag. We also expose the port SkyDNS is bound to on the Pod through named ports for both TCP and UDP protocols.</p>

<p>To monitor the health of the container with liveness probes, we run a health server as a sidecar container using the <a href="https://github.com/kubernetes/contrib/tree/be436560df6fa839fb92a2f88ae4c4b7da4e58e4/exec-healthz" rel="nofollow">ExecHealthz</a> utility. By running a sidecar container, we do not make these liveness probes dependent on the container runtime to execute commands directly in the SkyDNS container (which also requires those binaries to be available within the container image). Instead our sidecar container will provide the <code>/healthz</code> http endpoint, this usage of a sidecar container illustrates very well the concept of creating single purpose and re-usable components and the power of Pods to bundle them. This is one of the fundamental features of Kubernetes Pods and you may reuse these Kubernetes components for your own application setup. </p>

<p>The ExecHealthz image available on the Google Container Registry uses Busybox as a base image. We use the <code>nslookup</code> utility bundled with Busybox for liveness probes as <code>dig</code> is not available in this image. </p>

<p>Add the ExecHealthz container with the following container spec:</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">...
      - name: healthz
        image: gcr.io/google_containers/exechealthz:1.0
        resources:
          limits:
            cpu: 10m
            memory: 20Mi
        args:
        - -cmd=nslookup kubernetes.default.svc.cluster.local localhost &gt;/dev/null
        - -port=8080
        ports:
        - containerPort: 8080
          protocol: TCP
...
</code></pre>
<p>Our health check does a simple probe for the Kubernetes API service which, as discussed above, SkyDNS should serve under the <code>kubernetes.default.svc.<span class="highlight">cluster.local</span></code> DNS record.</p>

<p>We can now add the liveness and readiness probes via this sidecar health server to report on the health status of our SkyDNS container:</p>
<div class="code-label " title="DNS Add-on Manifest - snippet">DNS Add-on Manifest - snippet</div><pre class="code-pre "><code langs="">      - name: skydns
        image: skynetservices/skydns:2.5.3a
        resources:
          limits:
            cpu: 100m
            memory: 50Mi
        args:
        - -machines=http://localhost:4001
        - -addr=0.0.0.0:53
        - -domain=cluster.local.
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        &lt;^&gt;livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 30
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 1
          timeoutSeconds: 5&lt;^&gt;
</code></pre>
<p>The full manifest of the Replication Controller for the <code>kube-dns-v9</code> add-on will be listed next for your reference, we will look at the manifest for the Service right after.</p>
<div class="code-label " title="skydns-rc.yaml">skydns-rc.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: ReplicationController
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: kube-dns-v9
</li><li class="line" prefix="6">  namespace: kube-system
</li><li class="line" prefix="7">  labels:
</li><li class="line" prefix="8">    k8s-app: kube-dns
</li><li class="line" prefix="9">    version: v9
</li><li class="line" prefix="10">    kubernetes.io/cluster-service: "true"
</li><li class="line" prefix="11">spec:
</li><li class="line" prefix="12">  replicas: 1
</li><li class="line" prefix="13">  selector:
</li><li class="line" prefix="14">    k8s-app: kube-dns
</li><li class="line" prefix="15">    version: v9
</li><li class="line" prefix="16">  template:
</li><li class="line" prefix="17">    metadata:
</li><li class="line" prefix="18">      labels:
</li><li class="line" prefix="19">        k8s-app: kube-dns
</li><li class="line" prefix="20">        version: v9
</li><li class="line" prefix="21">        kubernetes.io/cluster-service: "true"
</li><li class="line" prefix="22">    spec:
</li><li class="line" prefix="23">      containers:
</li><li class="line" prefix="24">      - name: etcd
</li><li class="line" prefix="25">        image: gcr.io/google_containers/etcd:2.0.9
</li><li class="line" prefix="26">        resources:
</li><li class="line" prefix="27">          limits:
</li><li class="line" prefix="28">            cpu: 100m
</li><li class="line" prefix="29">            memory: 50Mi
</li><li class="line" prefix="30">        command:
</li><li class="line" prefix="31">        - /usr/local/bin/etcd
</li><li class="line" prefix="32">        - -data-dir
</li><li class="line" prefix="33">        - /var/etcd/data
</li><li class="line" prefix="34">        - -listen-client-urls
</li><li class="line" prefix="35">        - http://127.0.0.1:2379,http://127.0.0.1:4001
</li><li class="line" prefix="36">        - -advertise-client-urls
</li><li class="line" prefix="37">        - http://127.0.0.1:2379,http://127.0.0.1:4001
</li><li class="line" prefix="38">        - -initial-cluster-token
</li><li class="line" prefix="39">        - skydns-etcd
</li><li class="line" prefix="40">        volumeMounts:
</li><li class="line" prefix="41">        - name: etcd-storage
</li><li class="line" prefix="42">          mountPath: /var/etcd/data
</li><li class="line" prefix="43">      - name: kube2sky
</li><li class="line" prefix="44">        image: gcr.io/google_containers/kube2sky:1.11
</li><li class="line" prefix="45">        resources:
</li><li class="line" prefix="46">          limits:
</li><li class="line" prefix="47">            cpu: 100m
</li><li class="line" prefix="48">            memory: 50Mi
</li><li class="line" prefix="49">        args:
</li><li class="line" prefix="50">        - -domain=cluster.local
</li><li class="line" prefix="51">      - name: skydns
</li><li class="line" prefix="52">        image: skynetservices/skydns:2.5.3a
</li><li class="line" prefix="53">        resources:
</li><li class="line" prefix="54">          limits:
</li><li class="line" prefix="55">            cpu: 100m
</li><li class="line" prefix="56">            memory: 50Mi
</li><li class="line" prefix="57">        args:
</li><li class="line" prefix="58">        - -machines=http://localhost:4001
</li><li class="line" prefix="59">        - -addr=0.0.0.0:53
</li><li class="line" prefix="60">        - -domain=cluster.local.
</li><li class="line" prefix="61">        ports:
</li><li class="line" prefix="62">        - containerPort: 53
</li><li class="line" prefix="63">          name: dns
</li><li class="line" prefix="64">          protocol: UDP
</li><li class="line" prefix="65">        - containerPort: 53
</li><li class="line" prefix="66">          name: dns-tcp
</li><li class="line" prefix="67">          protocol: TCP
</li><li class="line" prefix="68">        livenessProbe:
</li><li class="line" prefix="69">          httpGet:
</li><li class="line" prefix="70">            path: /healthz
</li><li class="line" prefix="71">            port: 8080
</li><li class="line" prefix="72">            scheme: HTTP
</li><li class="line" prefix="73">          initialDelaySeconds: 30
</li><li class="line" prefix="74">          timeoutSeconds: 5
</li><li class="line" prefix="75">        readinessProbe:
</li><li class="line" prefix="76">          httpGet:
</li><li class="line" prefix="77">            path: /healthz
</li><li class="line" prefix="78">            port: 8080
</li><li class="line" prefix="79">            scheme: HTTP
</li><li class="line" prefix="80">          initialDelaySeconds: 1
</li><li class="line" prefix="81">          timeoutSeconds: 5
</li><li class="line" prefix="82">      - name: healthz
</li><li class="line" prefix="83">        image: gcr.io/google_containers/exechealthz:1.0
</li><li class="line" prefix="84">        resources:
</li><li class="line" prefix="85">          limits:
</li><li class="line" prefix="86">            cpu: 10m
</li><li class="line" prefix="87">            memory: 20Mi
</li><li class="line" prefix="88">        args:
</li><li class="line" prefix="89">        - -cmd=nslookup kubernetes.default.svc.cluster.local localhost &gt;/dev/null
</li><li class="line" prefix="90">        - -port=8080
</li><li class="line" prefix="91">        ports:
</li><li class="line" prefix="92">        - containerPort: 8080
</li><li class="line" prefix="93">          protocol: TCP
</li><li class="line" prefix="94">      volumes:
</li><li class="line" prefix="95">      - name: etcd-storage
</li><li class="line" prefix="96">        emptyDir: {}
</li><li class="line" prefix="97">      dnsPolicy: Default
</li></ul></code></pre>
<p>The kube-dns Service will expose the DNS Pod internally to the cluster on the fixed IP we assigned for our DNS server, this <code>clusterIP</code> has to match the value we passed to all our kubelets previously, which is <code>10.3.0.10</code> in this tutorial. Modify this value to mirror your own configuration. The full Service definition is listed below:</p>
<div class="code-label " title="skydns-svc.yaml ">skydns-svc.yaml </div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Service
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: kube-dns
</li><li class="line" prefix="6">  namespace: kube-system
</li><li class="line" prefix="7">  labels:
</li><li class="line" prefix="8">    k8s-app: kube-dns
</li><li class="line" prefix="9">    kubernetes.io/cluster-service: "true"
</li><li class="line" prefix="10">    kubernetes.io/name: "KubeDNS"
</li><li class="line" prefix="11">spec:
</li><li class="line" prefix="12">  selector:
</li><li class="line" prefix="13">    k8s-app: kube-dns
</li><li class="line" prefix="14">  clusterIP: <span class="highlight">10.3.0.10</span>
</li><li class="line" prefix="15">  ports:
</li><li class="line" prefix="16">  - name: dns
</li><li class="line" prefix="17">    port: 53
</li><li class="line" prefix="18">    protocol: UDP
</li><li class="line" prefix="19">  - name: dns-tcp
</li><li class="line" prefix="20">    port: 53
</li><li class="line" prefix="21">    protocol: TCP
</li><li class="line" prefix="22">
</li></ul></code></pre>
<p>In our Service object metadata we attach the same <code>k8s-app</code> label as our Pods and Replication Controller as well as the necessary labels for Kubernetes add-on services. In our <a href="http://kubernetes.io/v1.1/docs/api-reference/v1/definitions.html#_v1_servicespec" rel="nofollow">ServiceSpec</a> our selector, used to route traffic to Pods with matching labels, only specifies the <code>k8s-app</code> label. This does not specify the version, allowing us to do rolling updates of our DNS add-on in the future, see the <a href="http://kubernetes.io/v1.1/docs/user-guide/update-demo/README.html" rel="nofollow">Rolling Update Example</a> for more details. Finally, we also define named ports for the DNS service on both TCP and UDP protocols. We will later confirm SRV records exist for these named ports of the <code>kube-dns</code> service in the <code>kube-system</code> namespace itself.</p>

<p><br>
<strong>Note</strong>: Multiple yaml documents can be concatenated with the <code>---</code> separator. We may simplify management of multiple resources by grouping them together in the same file separated by <code>---</code>, we may just specify multiple resources through multiple <code>-f</code> arguments for the <code>kubectl create</code> command. See the official <a href="http://kubernetes.io/v1.1/docs/user-guide/managing-deployments.html#organizing-resource-configurations" rel="nofollow">Managing Deployments Guide</a><br></p>

<p>The resources will be created in the order they appear in the file. Therefore, it's best to specify the service first, since that will ensure the scheduler can spread the pods associated with the service as they are created by the replication controller(s).</p>

<p>use kubectl with multiple <code>-f</code> arguments:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl create -f ./skydns-svc.yaml -f ./skydns-rc.yaml
</li></ul></code></pre>
<p>And wait for the DNS add-on to start running</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl get pods --namespace=kube-system | grep kube-dns-v9
</li></ul></code></pre>
<p>Create a Busybox pod to test the DNS from within the cluster using the following Pod manifest:</p>
<div class="code-label " title="busybox.yaml">busybox.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Pod
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: busybox
</li><li class="line" prefix="6">  namespace: default
</li><li class="line" prefix="7">spec:
</li><li class="line" prefix="8">  containers:
</li><li class="line" prefix="9">  - image: busybox
</li><li class="line" prefix="10">    command:
</li><li class="line" prefix="11">      - sleep
</li><li class="line" prefix="12">      - "3600"
</li><li class="line" prefix="13">    imagePullPolicy: IfNotPresent
</li><li class="line" prefix="14">    name: busybox
</li><li class="line" prefix="15">  restartPolicy: Always
</li></ul></code></pre>
<p>This busybox will sleep for 1 hour before exiting and being restarted by the kubelet, we will use it to test <code>nslookup</code> commands from within the cluster. Create the Pod:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl create -f busybox.yaml
</li></ul></code></pre>
<p>Although it seems we are only creating a Pod, Kubernetes will create a Replication Controller to manage this Pod for us. After a few seconds, confirm the Pod is running:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl get pods busybox
</li></ul></code></pre>
<p>When the Pod is running, output will look as follows:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>NAME      READY     STATUS    RESTARTS   AGE
busybox   1/1       Running   0          14s
</code></pre>
<p>Do a DNS lookup from within the busybox Pod on your client's terminal with the following command:</p>
<pre class="code-pre "><code langs="">kubectl exec busybox -- nslookup kubernetes.default
</code></pre>
<p>The expected output should look as follows:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>Server:    10.3.0.10
Address 1: 10.3.0.10

Name:      kubernetes.default
Address 1: 10.3.0.1
</code></pre>
<p>If you are using the <code>skynetservices/skydns:5.2.3a</code> image, you may use the <code>dig</code> binary within to confirm the SRV records for the named ports are served as expected (the nslookup utility bundled in the busybox Pod does not support SRV queries).</p>

<p>To do this, get the name of the <code>kube-dns</code> Pod created by the <code>kube-dns</code> Replication Controller (Pod names are dynamic and change when they are restarted):</p>
<pre class="code-pre "><code langs="">dns_pod=`kubectl --namespace=kube-system get po | grep kube-dns | awk '{ print $1}'`
</code></pre>
<p>Open an interactive shell into the SkyDNS container of your kube-dns pod:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">kubectl --namespace=kube-system exec $dns_pod -c skydns -it sh
</li></ul></code></pre>
<p>We specify the container we want to execute commands in through the <code>-c</code> option of the <code>kubectl exec</code> command. </p>

<p>Use <code>dig</code> to query the SRV record for the port named dns using the UDP protocol and <code>sed</code> to only print the response from the <code>ANSWER SECTION</code> to the <code>Query Time</code> lines:</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">dig <a href="https://www.digitalocean.com/community/users/localhost" class="username-tag">@localhost</a> SRV _dns._udp.kube-dns.kube-system.svc.cluster.local | sed -n '/ANSWER SECTION:/,/Query time/ p'
</li></ul></code></pre>
<p>We are using <code>sed</code> with the <code>-n</code> option to suppress all output, we specify a range of regular expression patterns (<code>/ANSWER SECTION/,/Query time/</code>) and instruct <code>sed</code> to print only lines within this range with the <code>p</code> command</p>

<p>The expected output should look as follows:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>;; ANSWER SECTION:
_dns._udp.kube-dns.kube-system.svc.cluster.local. 30 IN SRV 10 100 <span class="highlight">53</span> kube-dns.kube-system.svc.cluster.local.

;; ADDITIONAL SECTION:
kube-dns.kube-system.svc.cluster.local. 30 IN A <span class="highlight">10.3.0.10</span>

;; Query time: 3 msec
</code></pre>
<p>As you can see, using the SRV records created by the kube-dns add-on, we are able to get the port as well as the IP.</p>

<p>Refer also to the <a href="http://kubernetes.io/v1.1/docs/admin/dns.html" rel="nofollow">Official DNS Integration documentation</a> and the <a href="https://github.com/kubernetes/kubernetes/tree/release-1.1/cluster/addons/dns" rel="nofollow">DNS Add-on repository</a>.</p>

<h2 id="step-11-—-deploying-kubernetes-ready-applications">Step 11 — Deploying Kubernetes-ready applications</h2>

<p>You should now have a Kubernetes cluster set up and be able to deploy Kubernetes-ready applications. </p>

<p>To better understand the inner workings of Kubernetes from a Pod perspective, we may use the <a href="https://github.com/kelseyhightower/inspector" rel="nofollow">Kubernetes Pod Inspector</a> application by Kelsey Hightower. With the below yaml file combining both the Replication Controller and Service, you can quickly deploy and expose this application on your cluster:</p>
<div class="code-label " title="kube-inspector.yaml">kube-inspector.yaml</div><pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="1">
</li><li class="line" prefix="2">apiVersion: v1
</li><li class="line" prefix="3">kind: Service
</li><li class="line" prefix="4">metadata:
</li><li class="line" prefix="5">  name: inspector
</li><li class="line" prefix="6">  labels:
</li><li class="line" prefix="7">    app: inspector
</li><li class="line" prefix="8">spec:
</li><li class="line" prefix="9">  type: NodePort
</li><li class="line" prefix="10">  selector:
</li><li class="line" prefix="11">    app: inspector
</li><li class="line" prefix="12">  ports:
</li><li class="line" prefix="13">  - name: http
</li><li class="line" prefix="14">    <span class="highlight">nodePort: 31000</span>
</li><li class="line" prefix="15">    port: 80
</li><li class="line" prefix="16">    protocol: TCP
</li><li class="line" prefix="17">
</li><li class="line" prefix="18">---
</li><li class="line" prefix="19">
</li><li class="line" prefix="20">apiVersion: v1
</li><li class="line" prefix="21">kind: ReplicationController
</li><li class="line" prefix="22">metadata:
</li><li class="line" prefix="23">  name: inspector-stable
</li><li class="line" prefix="24">  labels:
</li><li class="line" prefix="25">    app: inspector
</li><li class="line" prefix="26">    track: stable
</li><li class="line" prefix="27">spec:
</li><li class="line" prefix="28">  replicas: 1
</li><li class="line" prefix="29">  selector:
</li><li class="line" prefix="30">    app: inspector
</li><li class="line" prefix="31">    track: stable
</li><li class="line" prefix="32">  template:
</li><li class="line" prefix="33">    metadata:
</li><li class="line" prefix="34">      labels:
</li><li class="line" prefix="35">        app: inspector
</li><li class="line" prefix="36">        track: stable
</li><li class="line" prefix="37">    spec:
</li><li class="line" prefix="38">      containers:
</li><li class="line" prefix="39">      - name: inspector
</li><li class="line" prefix="40">        image: b.gcr.io/kuar/inspector:1.0.0
</li><li class="line" prefix="41">
</li></ul></code></pre>
<p>As seen previously, we provide our Replication Controller with the necessary labels and use the <code>b.gcr.io/kuar/inspector:1.0.0</code> image. Note that we are exposing the inspector application by telling Kubernetes to open port <code>31000</code> on every worker node (this will work if you ran the API service with <code>--service-node-port-range=30000-37000</code> as shown in Step 6 of this Tutorial).</p>

<p>Expected Output:</p>
<pre class="code-pre "><code langs=""><div class="secondary-code-label " title="Output">Output</div>You have exposed your service on an external port on all nodes in your
cluster.  If you want to expose this service to the external internet, you may
need to set up firewall rules for the service port(s) (tcp:31000) to serve traffic.

See http://releases.k8s.io/release-1.1/docs/user-guide/services-firewalls.md for more details.
&lt;^&gt;service "inspector" created
replicationcontroller "inspector-stable" created&lt;^&gt;
</code></pre>
<p>We can now point our web browser to <code>http://$WORKER_PUBLIC_IP:31000/env</code> on any worker node to access the Inspector Pod and view all environment variables published by the kubelet as well as access <code>http://$WORKER_PUBLIC_IP:31000/mnt?path=/var/run/secrets/kubernetes.io/serviceaccount</code> to see the secrets mounted into the Pods. To see how Kubernetes-ready applications can use these, refer to the <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/pkg/client/unversioned/helper.go#L324" rel="nofollow">InClusterConfig</a> function of the client Kubernetes client helper library and the <a href="https://github.com/kubernetes/kubernetes/blob/release-1.1/cluster/addons/dns/kube2sky/kube2sky.go#L491" rel="nofollow">KubeClient Setup</a>) section of Kube2Sky as an example implementation.</p>

<p>You may now proceed to set up a multi-tier web application (Guestbook) from the official Kubernetes documentation to visualize how the various Kubernetes components fit together.</p>

<p>See <a href="http://kubernetes.io/v1.1/examples/guestbook-go/README.html" rel="nofollow">the Guestbook Example app</a> from the Official Kubernetes documentation.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Following this Tutorial you have created a fully functional Kubernetes cluster. This gives you a great management and scheduling interface for working with services in logical groupings. As you have used many of the Kubernetes concepts to set up the cluster itself, you have a deep understanding of many of the core concepts and deployment workflow of Kubernetes. To review all the Kubernetes concepts, refer to the official <a href="http://kubernetes.io/v1.1/docs/user-guide/README.html#concept-guide" rel="nofollow">Kubernetes Concept Guide</a>.</p>

<p>You probably noticed that the steps above were still very manual, but the Cloud-config files created are flexible enough for you to automate the process.</p>

<h2 id="deleting-your-kubernetes-cluster">Deleting your Kubernetes Cluster</h2>

<p>If you decide you do no longer want to run this cluster (or want to start from scratch), below are the commands to do so:</p>

<p><strong>NOTE</strong> These commands destroy your cluster and all the data contained within without any backups, these commands are irreversible.</p>

<p>Repeat for every controller</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d d kube-controller-01.$region
</li></ul></code></pre>
<p>Repeat for every worker</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d d kube-worker-01.$region
</li></ul></code></pre>
<p>Delete all Kubernetes data</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">doctl d d etcd-01.$region
</li></ul></code></pre>
<p>Delete the apiserver &amp; worker certificates as they are tied to the IPs of the Droplets, but keep the Admin and CA certificates.</p>
<pre class="code-pre "><code langs=""><ul class="prefixed"><li class="line" prefix="$">rm ~/.kube/apiserver*.{pem,csr}
</li><li class="line" prefix="$">rm ~/.kube/worker*.{pem,csr}
</li><li class="line" prefix="$">rm *.srl
</li><li class="line" prefix="$">rm *openssl.cnf
</li></ul></code></pre>

<!-- END PREVIEW CONTENT -->
          </div>
        </div>
      </div>
    </div>
  </div>
  <script>
  var _gaq=[['_setAccount','UA-72129772-1'],['_trackPageview']];
  (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
  g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
  s.parentNode.insertBefore(g,s)}(document,'script'));
</script>
  <script data-no-instant="">document.write('<script src="/livereload.js?mindelay=10"></' + 'script>')</script>
  <script src="/livereload.js?mindelay=10"></script>
</body>